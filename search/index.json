[{"content":"自上次更新博客起已经有三年时间了，这期间经历了留学、毕业、工作等很多事情，个中滋味只有自己知道。博客重开，希望文笔和思路上能比以前进步一点点。同时因为学识有限，也希望各位大佬能不吝赐教。\n背景 上个月为了学习 rust 语言，参加了阿里云 ECS 团队举办的 CloudBuild 性能挑战赛，用 rust 实现一个聊天室服务器。因为 rust 不熟练初赛只提交了一次，居然也闯进复赛了，也挺有意思的。复赛题目要求服务要部署在三台机器上，同时提供完整的读写服务，且要求宕机一台不影响服务。看到题目后，我直观地想到了使用一致性协议让三台机器关于存储的数据达成一致就可。分布式系统中常用的一致性协议有 Paxos 和 Raft 等，其中 Raft 从原理上来说是一种特化的 Paxos，而且简单易懂。因此我最终决定采用 Raft + RocksDB 的方案进行实现。\n问题 Raft 是一个分布式一致性协议，而 RocksDB 是一个 K-V 数据库，在各自的领域应该都是闻名遐迩，在此不再多做介绍。在方案的实现过程中，碰到不少问题，其中的一个主要问题是：Raft 协议运行过程中需要维护一份日志，而 RocksDB 为了保障持久性，也会维护一份 WAL，这两份日志在数据内容上是高度重合的。这意味着在写入数据的过程中，需要再多写一份，这对于任何关注性能的系统来说都是不能接受的。\n事实上很多分布式数据库系统都已经合并了这两条日志，比如某些魔改的分布式 MySQL 系统，以及 tikv（？我没调研）。他们采用的方案或是将存储引擎的 WAL 扩展支持一致性协议的特殊事件，或是在一致性协议的日志中嵌入一条存储引擎的 WAL，或者对某些特化的系统比如分布式日志系统，干脆直接将 Raft 的日志改造成存储引擎。在 Google 解决方案的过程中，我发现了 eBay 以前的一篇文章[1]，讲的就是如何把日志存储的日志和 Raft 的日志相结合起来，虽然只是一篇 workshop 的文章，但图文讲解还是让我茅塞顿开。\n由于我采用了 RocksDB 作为存储引擎，显然我不希望去修改 RocksDB 的 WAL，这工程量太大了，不够回本的。因此我决定尝试定制 Raft 日志、关闭 RocksDB 的 WAL 的方案。在崩溃恢复的时候，使用 Raft 的日志去进行 RocksDB 的恢复。因为 RocksDB 是一个 LSMT 结构的系统，它在磁盘上持久化的 SST（Sorted String Table）都是只读的，即使崩溃也只是丢失内存中还没有刷盘的 memtable，这大大简化了崩溃恢复的工作量。\n共享日志 共享日志需要为功能和性能考虑以下两点问题：\n 崩溃恢复时，怎么寻找最早未写入 RocksDB 的日志？ 怎么减少小的磁盘 IO，尽量提高吞吐？  崩溃恢复 因为 RocksDB 只会丢失内存中的 memtable，而 KV 操作又都是幂等的，因此最简单的办法就是直接把日志从头重放到尾。显然这不是一个好方案，不说恢复时间很长，因为重复的 KV 操作导致的重复数据也带来了额外的磁盘压力，对 SSD 这种 GC 昂贵还有擦写次数限制的磁盘非常不友好。更好的做法是在 RocksDB 刷完 SST 的时候，记录下当前写下的 SST 的最大日志 LSN，同样写入磁盘。当重启恢复时查询到这个记下的 LSN，从这个点开始重放就好。\n这个方案有两种实现办法：\n 用 RocksDB 提供的事件回调异步地完成 LSN 获取和写入 每次 put(k, v) 时额外 put 一个特殊的 key，举个例子 _raft_log_lsn，值是当前日志的 lsn  第一种办法异步写入总是有可能会写入失败（崩溃时），因此我更倾向于第二种实现办法，它唯一需要保证的是特殊的 key 不在用户合法的 key 的范围内。当 memtable 被写入磁盘成为 SST 时，这个特殊的 key 同样也会写入。那么当 RocksDB 再次启动时，只要查询一下该 key 的值就可以获取当前最大已经写入的 LSN。显然这种方法更加的优雅。当然这种方法并不能保证 LSN 和 SST 中的数据一定是一致的，除非在写入时能锁定 memtable 不让刷。\n这里有一个注意点就是，RocksDB 默认的刷 memtable 策略是根据大小来的，因此可以先做 lsn key 的写入（不会改变 memtable 大小），再做 put(k, v)，这样即使触发刷盘，也能保证数据和 lsn 是一致的。但是需要保证不会因为其他的什么策略刷盘导致刷了 lsn key 而数据丢了，目前看来还是比较困难的（不改 RocksDB 的话）。\n合并提交 所有的数据库都有一个要求，那就是在用户请求返回前，数据一定以某种形式（e.g. WAL）已经在磁盘上了，从而保证持久性。但是当用户大量的请求都很小时，每个请求都单独落盘会导致大量的小 IO，而小 IO（小于 4KB）在任何类型的磁盘产品上都是慢且有害的。数据库系统中常用的一个优化手段是合并提交（group commit），将一组请求合并成一个请求同时提交到磁盘，然后再返回给用户。这样可以让每次的 IO 请求都尽可能的大，有效地提高了吞吐。\n共享的 WAL 同样也可以采用合并提交的方式进行写入，但是注意的这时需要对上述崩溃恢复的方案做一些小的修改，即当所有 K-V 都 put 后再将 LSN 写入。\n流程图 照猫画虎搞了个流程图：\n 流程图 \n总结 总之只是一个小比赛引发的一点思考，实际上比赛是只有插入和查询，没有删除、覆盖写的场景，甚至消息只有 append。RocksDB 并不是每个场景都能做到最优性能，但 Raft on 某种存储引擎的思路应该是通用的。\n参考文献 [1] Designing an Efficient Replicated Log Store with Consensus Protocol. Jung-Sang Ahn, et al. 11th USENIX Workshop on Hot Topics in Cloud Computing 2019.\n","date":"2021-08-17T00:00:00Z","image":"https://blog.crazyark.xyz/p/raft-and-share-log/img/raft_hu789963ccc359d12cda0ae24ab10587a7_341224_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.crazyark.xyz/p/raft-and-share-log/","title":"Raft on RocksDB -- 共享日志"},{"content":"Hash table \u0026ndash; Collision 在计算机科学中，哈希表 (hash table) 是一个非常重要的数据结构，它帮助我们快速的进行插入和查询。理论上来说，在表中查询一次的耗时应该是 O(1) 的。这里假设大家对于哈希 (hash) 都已经了解，如果你以前没有接触过这个概念，这篇文章或许是一个很好的开始。\n假设你有一个完美的哈希函数 (perfect hashing) 和无穷大的内存，那么哈希表中每一个哈希值都对应了一个原始的元素。显然此时要进行查找，我们只需要算出哈希值，然后找到表中对应的项就可以了。然而现实中不存在无穷大的内存，也不是很容易去找到一个“优秀”的完美哈希，比如最小完美哈希 (minimal perfect hashing)。\n所以实际中，我们不可避免的会碰到哈希冲突 (hash collision) ，也就是两个不同的元素被映射到同一个哈希值上。当一个哈希表碰到哈希冲突的时候，有几种办法去解决它，它们各有优劣，且容我一一道来。\nHash collision 哈希冲突的解决办法通常是两种，一种叫拉链法 (separate chaining) ，一种叫开地址法 (open addressing)。当然不仅限于这两种，其他的比如 multiple-choice hashing, cuckoo hashing, hopscotch hashing, robin hood hashing 等等，本文挑几种介绍一下。\nSeparate chaining 拉链法核心思想就是将哈希冲突的元素存入都存入到链表中，如下图所示，\n在每个哈希表的单元格中 (hash table cell, 或者我们也称为桶 bucket)，都存放了一个链表头。当哈希冲突发生时，只需要把冲突的元素放到链表中。当需要查找时，先通过哈希值找到对应的单元格，再遍历链表找到想要的元素，此时查找时间是 O(N) 的。这里 N 是链表长度，通常链表不会很长，所以可以认为整体查询时间还是 O(1) 的。大多数标准库中的哈希表时间都是采用了这种方式，因为足够通用，性能也还不错。\n当然我们也可以用一棵 BST 来代替链表，此时查询时间是 O(lgN) 的，但是插入和空间开销要比链表大。Java 的 HashMap 的实现中，当一个哈希单元格的链表长度超过 8 时，就会自动转成一颗红黑树来降低查询开销。\n拉链法有两个缺点，这在一些对性能要求比较高的地方可能不太能忍受:\n 链表的空间开销比较大 链表对 CPU cache 不友好  但同时它的通用性最好，因为采用拉链法不需要对存入的元素有任何了解，所以绝大部分标准库的实现都用的这种方式。\nOpen Addressing 开地址法是将元素直接存入到表中，但不一定是哈希值对应的那个单元格中，而找到元素对应存储的单元格的过程通常被称为探查 (probe)。开地址法需要有一个特殊的元素，来标识空的单元格。比较著名的开地址法有线性/二次探查 (linear/quadratic probing) 和双重哈希 (double hashing)。\nLinear/Quadratic probing 在插入/查找的过程中，线性/二次探查法会通过以下过程来找到插入/存放的位置:\n 找到初始的哈希值 h(e) 对应位置 s(0) 的单元格，查看是否是空/相同的键，如果符合要求则返回，否则继续下面的过程  通常 s(0) = h(e) % n，n 是表大小   如果是线性探查，则依次往后搜索，位置由 s(i +1) = (s(i) + 1) % n 推导; 如果是二次探查，也是依次往后搜索，不过位置推导为 s(i + 1) = (s(i) + i^2) % n 循环第二步直到找到或者确认找不到为止  线性探查的优势是对于 CPU cache 非常友好，相关的元素都聚集在一起；但是缺点同样也是因为聚集，这个问题称为 primary clustering: 任何添加到哈希表的元素，必须探查并跨过探查过程中的元素聚簇 (cluster)，并且最终会增大聚簇。Primary clustering 的问题在负载系数 (load factor) 比较大时候非常严重，感兴趣的同学可以去看下参考文献 [2] 中 CMU 的课程讲义，有一些证明和实验结果。\n由于这个原因，线性探查的性能和哈希函数的质量有很大的关系。当负载系数较小 (30% -70%) 并且哈希函数质量比较高时，线性探查的实际运行速度是相当快的。\n二次探查则很好的解决了 primary clustering 的问题，但同样无法避免聚集，这个聚集叫做 secondary clustering: 当两个哈希值相同时，它们的探查序列也是相同的，这就和刚才的一样，探查序列会越来越长。\n二次探查有一个重要的性质：当表大小是素数 p，并且表中还有至少一半的位置是空的，那么二次探查一定能找到空的位置，并且任何位置只会被探查一次。关于这个性质的证明同样可以参考 CMU 的教材。由于只能在负载系数小于 0.5 时才有这个保证，二次探查法的空间效率不怎么高。\nDouble hashing 双重哈希与线性/二次探查一样，都是一步步探查空/存放的位置，只不过使用两个哈希函数 h_1 和 h_2，探查的位置序列 s(i) = (h_1(e) +i * h_2(e)) % n。双重哈希有效地避免了 secondary clustering，因为对于每个探查起点，不同的元素对应的探查序列也是不同的。\n双重哈希相比于线性/二次探查，可以用更小的表空间存放更多的元素，但是探查的计算过程会较慢。\nRobin Hood hashing 罗宾汉哈希 (Robin Hood Hashing) 是一个开地址法的变种，我们通常所说的罗宾汉哈希是指 Robin Hood Linear Probing。在这篇博文 [14] 中，作者甚至推荐大家使用基于罗宾汉哈希实现的哈希表。那它到底有什么优势？\n To give you an idea how Robin Hood Hashing improves things, the probe length variance for a RH table at a load factor of 0.9 is 0.98, whereas for a normal open addressing scheme it’s 16.2. At a load factor of 0.99 it’s 1.87 and 194 respectively.\n 换句话说，罗宾汉哈希可以显著降低探查长度的方差。我们来看一下它是怎么做的：\n 对每个哈希表中的元素，记录插入时的探查长度 当插入一个新元素时，对于探查过程中的元素，如果它的探查长度小于当前插入元素的探查长度，那么交换这两个元素 (以及探测长度)，然后继续探查  也就是说，事实上大家的探查长度更加平均了，所以期望最长探查长度也会显著的下降。这也是罗宾汉 (英国传说中的侠盗) 哈希名字的来源，劫富济贫。虽然大部分元素的探查长度都更趋近于平均值，不是一次就能查到，但是由于这部分开销较 CPU 加载 cache line 开销可以忽略不计，所以整体上仍有显著的提高。\n论文 [16] 中针对各种负载系数和表规模的实验表明，罗宾汉哈希在负载系数为 0.9 时，最长探查长度的期望值也就是 6 左右 (表规模 20万)，而且随着表规模的增大，增长速度极为缓慢。\n但是，即使是罗宾汉哈希也不能解决 Primary clustering 的问题，该聚簇的还是会聚簇在一起，如果在聚簇块开始探查一个不存在的元素，那么探查过程会相当漫长。不必惊慌，这个问题很容易解决：我们记录一个全局的最长探查长度，如果当前探查长度大于全局最长探查长度了，那肯定是找不到了，我们就可以直接返回。而根据上面的实验，全局最长也不会太长，麻麻再也不怕我查询失败了！\n在各种测试中，罗宾汉哈希表的空间和时间性能表现都非常好，希望以后在工作过程中能有机会使用。\n2-choice hashing 2-choice hashing 同样是使用两个哈希函数 h_1 和 h_2，每次插入时，只考虑 h_1(e) 和 h_2(e) 这两个位置，并选取其中元素少的那个插入。2-choice hashing 有一个神奇的结论，即每个桶中的元素的期望值为 $\\theta(\\log(\\log(n)))$。\n关于期望值的证明，如果你也是南大的同学并且被尹一通老师的随机算法课虐过，那你应该听说过 The power of 2-choice，同样 CMU 的讲义中也有证明。\nCuckoo Hashing 布谷鸟哈希 (Cuckoo Hashing) 也是开地址法的一种，它最差情况的查询速度都是 O(1) 的。布谷鸟哈希的名字来源于一种布谷鸟，他们的幼崽会在刚孵化的时候，把其他未孵化的蛋一脚踢出鸟巢。\n布谷鸟哈希通常使用两个数组以及哈希函数，所以和 2-choice hashing 一样，每一个元素都对应两个位置。当一个元素插入的时候，如果两个位置没满，就直接放入空的位置；如果满了，那么踢掉其中一个，放入其中，然后把踢掉的这个放入它的第二个位置；如果踢掉的这个的第二个位置也被占了，那么继续踢掉其中的；循环上述过程直到最后放入空的位置。\n显然，当最后遇到无限循环的时候，上述插入过程有可能会失败。此时我们可以用新的哈希函数原地重建一个表：\n There is no need to allocate new tables for the rehashing: We may simply run through the tables to delete and perform the usual insertion procedure on all keys found not to be at their intended position in the table.\n— Pagh \u0026amp; Rodler, \u0026ldquo;Cuckoo Hashing\u0026rdquo;[17]\n 布谷鸟哈希相较于其他方法来说，空间利用率更高，所以有研究者用它设计了 Cuckoo Filter [18] [10]，比 Bloom Filter 更加的空间高效，并且支持动态删除。\n具体的其他细节这里就不多叙述了，大家可以去阅读维基百科 [5] 上的资料和参考文献。\nBenchmarks 这儿有一篇博客[11]，分析了 8 种哈希表主流实现在各种场景下的时间和空间性能，分析表明罗宾汉哈希表 (robin_map) 和二次线性探查 (dense_map) 的性能表现都相当优秀，详细数据可以前往原博文查看。\nReferences [1] https://courses.cs.washington.edu/courses/cse373/01sp/Lect13.pdf\n[2] http://www.cs.cmu.edu/afs/cs/academic/class/15210-f13/www/lectures/lecture24.pdf\n[3] https://en.wikipedia.org/wiki/Hash_table\n[4] https://github.com/efficient/libcuckoo\n[5] https://en.wikipedia.org/wiki/Cuckoo_hashing\n[6] https://en.wikipedia.org/wiki/2-choice_hashing\n[7] https://www.threadingbuildingblocks.org/\n[8] https://github.com/sparsehash/sparsehash\n[9] http://www.cs.cmu.edu/afs/cs/academic/class/15859-f04/www/scribes/lec10.pdf\n[10] https://brilliant.org/wiki/cuckoo-filter/\n[11] https://tessil.github.io/2016/08/29/benchmark-hopscotch-map.html\n[12] http://www.cs.princeton.edu/~mfreed/docs/cuckoo-eurosys14.pdf\n[13] http://tcs.nju.edu.cn/wiki/index.php/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95_(Fall_2017)\n[14] https://www.sebastiansylvan.com/post/robin-hood-hashing-should-be-your-default-hash-table-implementation/\n[15] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.130.6339\n[16] http://codecapsule.com/2013/11/17/robin-hood-hashing-backward-shift-deletion/ [17] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.4189\u0026rep=rep1\u0026type=pdf [18] https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf\n","date":"2018-08-16T14:55:23+08:00","permalink":"https://blog.crazyark.xyz/p/hash-table-collision/","title":"哈希表 -- 哈希冲突"},{"content":"从报名比赛开始到现在将近四个月了，终于告一段落，最终拿到了季军也很开心。这三个月中我学到了很多有用的工程知识，顺便把 C++ 又摸熟了。初赛代码实在写太丑就不放出来了，复赛的代码托管在 github 上：\nhttps://github.com/arkbriar/awrace2018_messagestore\n下面是本次大赛初赛和复赛部分的思考过程和最终方案。\n初赛部分 赛题背景分析及理解  实现一个高性能的 Service Mesh Agent 组件，并包含如下一些功能：1. 服务注册与发现, 2. 协议转换, 3. 负载均衡\n 本题要求我们能够尽可能的高性能，我们首先对场景和大致思路进行了一个重述：\n Consumer 将接受超过 500 个连接：想到使用 IO multiplex Http = TCP 连接：禁用 Nagle 算法 Dubbo provider 只有 200 个处理线程，超过 200 个并发请求会快速失败： 负载均衡尽量避免 provider 过载 线上网络性能 (pps) 较差：批量发送 request/response，使用 UDP 进行 Agent 间通信 Consumer 性能较差：将协议转换等放到 provider agent 上去做  核心思路 为了减少系统开销，我们在 agent 之间都只保持一个 udp 信道，在 PA (Provider Agent) 和 Provider 之间也只保持一个 tcp 信道。\n整体的架构图如下所示：\n \n其中，CA (Consumer Agent) 端使用一个 epoll 的 eventloop，loop 的大致流程如下图：\n  1. 首先接起所有的新的连接，在接到连接的时候，根据 provider 的权重直接分配一个 provider 处理后续该连接的所有请求\n2. 读所有的能读的 socket\n 如果读到完整的请求，那么将它放入到请求队列 如果读到完整的回复，直接通过阻塞写的方式写回  3. 所有事件处理完成以后，批量发送所有的请求队列 (此处使用 writev)\n在请求发送和获取回复的过程中，我们始终有一个 id 附在请求中，这个 id 是 consumer 端唯一的，这保证了我们只在 consumer 端有一个 map 来确定回复的归宿，在这里我们的 id 是 handler 的编号。\n关键代码 批量发送请求代码大致如下：\nepoll_event events[LOWER_LOAD_EVENT_SIZE]; while (_event_group.wait_and_handle(events, LOWER_LOAD_EVENT_SIZE, -1)) { // check pending write queue, and block write all  for (auto\u0026amp; entry : _pending_data) { // write all requests to provider agent  // ...  } } 其余代码在 https://code.aliyun.com/arkbriar/dubbo-agent-cxx 中。\n复赛部分 赛题背景分析及理解  实现 Queue Store 的接口 put/get，要求支持单机百万队列和百G数据。\n 在对题目稍微了解过后，我们得到了以下的几个信息：\n 消息条数大约为 2g，消息大小大约为 64 byte，但是可能会有长为 1024 byte 的消息 线上的机器是 4c8g 带 ssd，iops 1w 左右，顺序 4k 读写性能大约为 200m/s 左右 对同一个队列，put 是序列化的 对同一个队列，get 可能是并发的 不存在删除队列和消息，也不存在 put 与 get 同时进行  而评测程序的空跑 put TPS 大约在 600w 左右，性能也不是很高；而在不压缩的情况下，写打满写 100g 数据最少耗时 500s。因此本题要求选手能够尽可能的保证 put 阶段的无阻塞，以及能够设计出一个良好的数据结构，保证 get 阶段能够利用缓存和局部性。\n核心思路 为了实现高性能的接口，我们必须要解决以下三个问题：\n 存储设计问题，要求能够支持顺序写与随机读 IO/SSD 优化问题，能尽可能的使 IO 做到极限 内存规划问题，由于评测环境内存比较有限，需要紧凑的规划和使用  存储设计：页式存储 首先我们研究了一个比较类似的产品，Kafka：\n \n \nKafka 的大致存储模型如上图所示，这里并不展开叙述，有兴趣的同学可以到这个网站继续学习 https://thehoard.blog/how-kafkas-storage-internals-work-3a29b02e026\n但是对于我们来说，照搬它有很多问题：\n 100w 个队列，至少有 200w 个文件，这对文件系统形成了巨大的负担 Kafka 使用全量索引，而 2g 消息的全量索引根本放不进内存，至少做两级索引，这加重了读的负担 2g 消息的全量索引的磁盘 overhead 太大 队列的主要场景是顺序读，全量索引优势完全没有发挥 (随机读)  所以我们借鉴了一个常见的文件切分模式，也就是分页，并以页为最小单位进行操作和建立索引。这就是我们的存储模式：页式存储。\n通过这个存储方式，\n 可以有效的减少文件的数目 并且由于顺序写的性质，我们并不需要考虑脏页的问题 同时由于页是最小单位，同属一个队列的连续消息尽可能的在一个页中，保证了顺序读能够利用缓存和局部性 最后，我们对页建立索引，此时的索引已经完全可以放入内存了  为了兼顾内存，页大小设置为 4k。\nIO/SSD 优化：块读块写 对于现有的大多数存储设备来说，无论是 HDD 还是 SSD 也好，顺序读写永远都是最优的。\n但是，SSD 有区别的一点在于，由于 SSD 的特性 “内置并发”，对齐于 Clustered Block 的读写将完全不输于顺序读写。\n如果要理解内置并发这个事情，我们可能需要去了解一些 SSD 的原理，下面这个网站提供了更多的细节，在这里只叙述一部分的东西。\nhttp://codecapsule.com/2014/02/12/coding-for-ssds-part-3-pages-blocks-and-the-flash-translation-layer/\n \nSSD 的读写都是以页 (NAND-flash page) 为单位的，即使只读/写一个 bit，也会同时读/写一个页。相邻的页组成一个块，块大小通常在 256k 到 4m 之间。SSD 的还有些原理会导致一个写放大和 GC 的问题，这里不再展开，有兴趣的同学自行了解。\nSSD 的内置并发指的是，图示上面的不同 channel/package/chip/plane 的访问都是可以并发进行的，不同的 plane 上的 block 组成的访问单位叫做聚簇块 (Clustered Block)，如图示中黄色框中的部分。可以看到，读写聚簇块就能充分地并发访问，而我们接下来要讲的读写模式就是，对齐聚簇块随机读写，性能将完全不输顺序读写。这里额外提一句，通常聚簇块大小为 32/64m。\n这是一张随机写与顺序写的吞吐对比，在 32M/64M 下，随机写已经能够全线等同顺序写。  \n而我们也做了一个小测试，测试 4g 大小的文件读写，单位都是秒：\n \n其中 concurrent pseudo sequential write 是指多个线程从同一个 atomic int 上获取下一个写 offset。实验结果证明了这个模式的有效性。\n所以我们最后的读写模式为块读块写：\n 大块 (64m) 写，甚至可以不用顺序写 有效地提高了并发读写性能 顺序读时充分利用缓存和预读  内存规划：精打细算 在结合上述两个设计的基础上，我们对内存有个精确的规划：\n 每个队列有自己的 4k 页缓存，对应于当前正在写的页 每个 put 线程有自己的双 64m 写缓冲，缓冲永远对应某个文件的一个区间 总计加起来缓存共占用 4k * 1m + 64m * 2 ≈ 5.2g  整体架构 Queue 与文件的关系如下图：\n  页与消息的结构如下图：\n  Size 的计算采用变长编码：\n Size \u0026lt; 128，使用一个 byte 存储，第一个 bit 为 0 Size \u0026gt;= 128 并且 \u0026lt; 32768，使用两个 byte 存储，但是由于第一个 bit 也为 0，所以存储 size | 0x8000  针对每个页的索引大致如下：\n \n每个页存储页编号、文件编号、页内消息数和之前所有页消息数这四项，这样我们在 get 时二分查找就可以获取到对应消息所在的页。所有的索引总量大约为 12byte * 25m = 300m，完全可以放于内存。\nPut/Get 流程 这两个流程相对简单，首先是 put 流程：\n \n 检查内存中的页还有没有空间  有，直接写入 没有，将当前页写出，更新索引，然后写入消息    页写出时会提交到当前 put 线程的写缓冲，并反馈修改索引。当写缓冲满的时候，与后备的缓冲交换继续写入缓冲，并起新的线程刷盘，这样尽可能不阻塞 put 线程。\n在所有 get 发生前，我们会强制所有缓冲落盘，接下来的 get 流程也相对简单：\n 二分查找第一页和最后一页 顺序阻塞读对应的文件页 跳过页内无关的消息，收集所有相关消息 如果请求的最后一页剩余消息数小于某个阈值 (e.g. 15)，使用 readahead 预读下一页 返回请求的消息  我们在 get 的过程中保持两个理念：\n 全力使用 OS 的页缓存 尽可能预读，以期望减少顺序读的时间  全力使用 OS 的页缓存主要是考虑到\n 实现 Concurrent LRU cache 比较麻烦 如果实现队列的一个顺序读缓冲，类似于链表结构，一旦发生多线程同时顺序读，缓冲会一直失效；即使实现 thread local 缓冲，如果出现单线程不同偏移量顺序读同一个队列 (e.g. 完整读两次)，缓冲也会不断失效  所以不如直接使用页缓存。\n关键代码 队列映射我们使用了 tbb 的 concurrent hash map，最终证明这是最大的 cpu 瓶颈，而我们的 put 完全卡在了 cpu 上。\ntemplate \u0026lt;class K, class V, class C = tbb::tbb_hash_compare\u0026lt;K\u0026gt;, class A = tbb::cache_aligned_allocator\u0026lt;std::pair\u0026lt;const K, V\u0026gt;\u0026gt;\u0026gt; class ConcurrentHashMapProxy : public tbb::concurrent_hash_map\u0026lt;K, V, C, A\u0026gt; { public: ConcurrentHashMapProxy(size_t n) : tbb::concurrent_hash_map\u0026lt;K, V, C, A\u0026gt;(n) {} ~ConcurrentHashMapProxy() { this-\u0026gt;tbb::concurrent_hash_map\u0026lt;K, V, C, A\u0026gt;::~concurrent_hash_map\u0026lt;K, V, C, A\u0026gt;(); } typename tbb::concurrent_hash_map\u0026lt;K, V, C, A\u0026gt;::const_pointer fast_find(const K\u0026amp; key) const { return this-\u0026gt;internal_fast_find(key); } }; MessageQueue* QueueStore::find_or_create_queue(const String\u0026amp; queue_name) { auto q_ptr = queues_.fast_find(queue_name); if (q_ptr) return q_ptr-\u0026gt;second; decltype(queues_)::const_accessor ac; queues_.find(ac, queue_name); if (ac.empty()) { uint32_t queue_id = next_queue_id_.next(); auto queue_ptr = new MessageQueue(queue_id, this); queues_.insert(ac, std::make_pair(queue_name, queue_ptr)); DLOG(\u0026#34;Created a new queue, id: %d, name: %s\u0026#34;, q-\u0026gt;get_queue_id(), q-\u0026gt;get_queue_name().c_str()); } return ac-\u0026gt;second; } MessageQueue* QueueStore::find_queue(const String\u0026amp; queue_name) const { auto q_ptr = queues_.fast_find(queue_name); return q_ptr ? q_ptr-\u0026gt;second : nullptr; decltype(queues_)::const_accessor ac; queues_.find(ac, queue_name); return ac.empty() ? nullptr : ac-\u0026gt;second; } void QueueStore::put(const String\u0026amp; queue_name, const MemBlock\u0026amp; message) { if (!message.ptr) return; auto q_ptr = find_or_create_queue(queue_name); q_ptr-\u0026gt;put(message); } Vector\u0026lt;MemBlock\u0026gt; QueueStore::get(const String\u0026amp; queue_name, long offset, long size) { if (!flushed) flush_all_before_read(); auto q_ptr = find_queue(queue_name); if (q_ptr) return q_ptr-\u0026gt;get(offset, size); // return empty list when queue is not found  return Vector\u0026lt;MemBlock\u0026gt;(); } 然后是我们的主要数据结构 FilePage：\n// File page header struct __attribute__((__packed__)) FilePageHeader { uint64_t offset = NEGATIVE_OFFSET; }; #define FILE_PAGE_HEADER_SIZE sizeof(FilePageHeader) #define FILE_PAGE_AVAILABLE_SIZE (FILE_PAGE_SIZE - FILE_PAGE_HEADER_SIZE)  // File page struct __attribute__((__packed__)) FilePage { FilePageHeader header; char content[FILE_PAGE_AVAILABLE_SIZE]; }; 双写缓冲有个交换上的同步，我们使用 mutex 去进行这个同步：\n// buffer is full, switch to back { // spin wait until back buf is not in scheduled status  while (back_buf_status-\u0026gt;load() == BACK_BUF_FLUSH_SCHEDULED) ; // try swap active and back buffer  std::unique_lock\u0026lt;std::mutex\u0026gt; lock(*back_buf_mutex); std::swap(active_buf, back_buf); assert(back_buf_status-\u0026gt;load() == BACK_BUF_FREE); uint32_t exp = BACK_BUF_FREE; back_buf_status-\u0026gt;compare_exchange_strong(exp, BACK_BUF_FLUSH_SCHEDULED); } 为了使得 OS 充分的跑起来，get 阶段需要释放掉所有无用的内存，然而 tcmalloc 是不会主动释放的，所以需要强制释放：\n// release free memory back to os MallocExtension::instance()-\u0026gt;ReleaseFreeMemory(); 由于线上的写入太过于均匀，到了刷盘的时候就是所有页缓存一起刷盘，这会造成所有 put 线程都阻塞并等待的状况，为了缓解这个情况，我们让某些队列的第一页快速写出：\n// first page will hold at most (queue_id / DATA_FILE_SPLITS) % 64 + 1 messages, this make write // more average. This leads to 64 timepoints of first flush. I call it flush fast. bool flush_fast = paged_message_indices_.size() == 1 \u0026amp;\u0026amp; paged_message_indices_.back().msg_size \u0026gt;= ((queue_id_ / DATA_FILE_SPLITS) \u0026amp; 0x3f) + 1; 其余代码放在 https://code.aliyun.com/arkbriar/queue-race-2018-cpp 中。\n最终成绩 最快写入时间大约 690s，最终缓存落盘 20s，随机校验 120s，顺序校验 140s，总计 970s+。\n期间从 1.9 开始的提升全在压缩 cpu 开销，没有动过读写结构。\n工程价值与健壮性 在存储设计过程中，我们充分考虑到了长消息的存在；虽然在比赛代码的实现中，我们最长支持也就是 4k 多一点的长度，但是却可以很轻易的支持跨页消息来去掉这个限制。\n在 put 的设计过程中，我们充分利用了 SSD 的特性，使得即使是随机写也能达到最大的磁盘吞吐。遗憾的是，最终我们深陷 cpu 瓶颈从来没有跑出过最大吞吐。\n虽然考虑到场景下没有边读边写的情况，但是通过在队列映射上加读写锁，或者在更细粒度的内存页上加锁，并在读时考虑内存中未落盘的数据，则立刻可以支持边读边写。\n总结与感想 对各种知识的理解和运用是解决实际问题的关键，在与各位选手的交流与竞争中，我们学到了很多知识，相信大家通过这次比赛也能收获很多。\n","date":"2018-08-01T19:59:45+08:00","permalink":"https://blog.crazyark.xyz/p/awrace-2018/","title":"第四届天池中间件性能挑战赛感想"},{"content":"Epoll 是 Linux 平台上独有的一组编程接口，用于监听多个文件描述符上的 IO 事件。Epoll 相对于 select/poll 的优势在于即使监听了大量的文件描述符，性能也非常好。Epoll API 支持两种监听方式：edge-triggered (EPOLLET) 和 level_triggered (default)。\nEdge-triggered 模式下，只有当文件描述符上产生事件时，才会被 epoll_wait 返回。例如，监听一个 socket，假如第一次 epoll_wait 返回了该 sock，可读取为 2 字节，但是只读取了 1 字节。那么下一次 epoll_wait 将不会返回该文件描述符了。换句话说，缓冲区中还有数据可读不是一个事件。\nLevel-triggered 不同，只要该 sock 还是可读的，将持续返回。\n在使用 ET 模式时，必须使用非阻塞文件描述符，防止阻塞读/阻塞写将处理多个文件描述符的任务饿死。最好以以下模式调用 ET 模式的 epoll_wait 接口：\n 使用非阻塞的文件描述符 只有当 read/write 返回 EAGAIN 时挂起并等待；当 read/write 返回的数据长度小于请求的数据长度时，就可以确定缓冲中已经没有数据了，也就可以认为事件已经完成了。  Epoll API Linux 提供了以下几个函数，用于创建、管理和使用 epoll 实例：\n int epoll_create(int size); int epoll_create1(int flags); int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); int epoll_pwait(int epfd, struct epoll_event *events, int maxevents, int timeout, const sigset_t *sigmask);  epoll_create/epoll_create1 epoll_create 将创建一个 epoll 实例，并且返回一个代表该实例的文件描述符。在 epoll_create1 中，epoll 的大小限制被取消了。flags 可以为 EPOLL_CLOEXEC，即为新的文件描述符设置 close-on-exec (FD_CLOEXEC)，这个标志在文件描述符上表示当 execve 系统调用之后，新线程的文件描述符是否要被关闭。\nepoll_ctl epoll_ctl 用于控制 epoll 实例上的监听的文件描述符，其中 epfd 就是 epoll 文件描述符，op 是指可以做的操作 (operation)，一共有三种：\n EPOLL_CTL_ADD EPOLL_CTL_MOD EPOLL_CTL_DEL  顾名思义，添加、修改和删除。\n后面的就是对应的文件描述符和 fd，以及设置好的想要监听的事件集合，存放在 struct epoll_event 中：\ntypedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; struct epoll_event 中的 events 是个位数组，表明当前监听的时间，列举几个比较重要的：\n EPOLLIN/EPOLLOUT，文件可读/写 EPOLLRDHUP，关闭连接或者写入半连接 EPOLLERR，默认参数，文件描述符上发生错误 EPOLLHUP，默认参数，文件被挂断，在 socket/pipe 上代表本端关闭连接 EPOLLET，开启 edge-triggered，默认是 level-triggered EPOLLONESHOT，一次触发后自动移除监听 EPOLLWAKEUP，如果EPOLLONESHOT和EPOLLET清除了，并且进程拥有CAP_BLOCK_SUSPEND权限，那么这个标志能够保证事件在挂起或者处理的时候，系统不会挂起或休眠  epoll_wait/epoll_pwait epoll_wait 阻塞并等待文件描述上的事件，需要保证 events 数组的大小要比 maxevents 大。epoll_wait 将阻塞直到：\n 一个文件描述符产生事件 被信号打断 超时 (timeout）  并返回当前事件的数量。\nepoll_pwait 多设置一个 sigmask，代表不想被这些信号打断，其余的相当于 epoll_wait。\n性能测试 对 poll/selelct 和 epoll 在监听不同数量文件描述符时的系统调用消耗对比，参考自参考文献表中的第一个网站。\n# operations | poll | select | epoll 10 | 0.61 | 0.73 | 0.41 100 | 2.9 | 3.0 | 0.42 1000 | 35 | 35 | 0.53 10000 | 990 | 930 | 0.66 参考文献 [1] https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/\n[2] Linux Programmer\u0026rsquo;s Manual: man epoll/epoll_create/epoll_ctl/epoll_wait\n基于 epoll 的简易服务器 以下使用 C 语言实现了一个简单的服务器，支持同时最多 100 个连接，对每个新建的连接。将它加入 epoll 队列中。当 IO 事件到达时，处理对应的客户端的 IO 事件。\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;errno.h\u0026gt;#include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt;#include \u0026lt;sys/epoll.h\u0026gt; #define MAX_EVENTS 10 #define LISTEN_PORT 1234 #define BUF_LEN 512 #define MAX_CONN 100  struct epoll_event ev, events[MAX_EVENTS]; int listen_sock, conn_sock, nfds, epollfd; struct sockaddr_in server; #define log(...) printf(__VA_ARGS__)  void response_to_conn(int conn_sock) { char buf[BUF_LEN + 1]; int read_len = 0; while ((read_len = read(conn_sock, buf, BUF_LEN)) \u0026gt; 0) { buf[read_len] = \u0026#39;\\0\u0026#39;; int cursor = 0; while (cursor \u0026lt; read_len) { // writing to a pipe or socket whose reading end is closed  // will lead to a SIGPIPE  int len = write(conn_sock, buf + cursor, read_len - cursor); if (len \u0026lt; 0) { perror(\u0026#34;write\u0026#34;); return; } cursor += len; } // there are no data so we do not have to do another read  if (read_len \u0026lt; BUF_LEN) { break; } } // must make sure that the next read will block this non-blocking  // socket, then we think the event is fully consumed.  if (read_len \u0026lt; 0 \u0026amp;\u0026amp; errno == EAGAIN) { return; } // end of file  if (read_len == 0) { return; } } /* Code to set up listening socket, \u0026#39;listen_sock\u0026#39; */ void listen_and_bind() { if ((listen_sock = socket(AF_INET, SOCK_STREAM, 0)) == -1) { perror(\u0026#34;socket\u0026#34;); exit(EXIT_FAILURE); } int option = 1; setsockopt(listen_sock, SOL_SOCKET, SO_REUSEADDR, \u0026amp;option, sizeof(option)); server.sin_family = AF_INET; server.sin_addr.s_addr = INADDR_ANY; server.sin_port = htons(LISTEN_PORT); if (bind(listen_sock, (struct sockaddr *)\u0026amp;server, sizeof(server)) == -1) { perror(\u0026#34;bind\u0026#34;); exit(EXIT_FAILURE); } listen(listen_sock, MAX_CONN); } void create_epoll() { epollfd = epoll_create1(0); if (epollfd == -1) { perror(\u0026#34;epoll_create1\u0026#34;); exit(EXIT_FAILURE); } ev.events = EPOLLIN; ev.data.fd = listen_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: listen_sock\u0026#34;); exit(EXIT_FAILURE); } } void set_fd_nonblocking(int fd) { int flags = fcntl(fd, F_GETFL, 0); if (flags == -1) { perror(\u0026#34;getfl\u0026#34;); return; } if (fcntl(fd, F_SETFL, flags | O_NONBLOCK) \u0026lt; 0) { perror(\u0026#34;setfl\u0026#34;); return; } } void epoll_loop() { for (;;) { int nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1); if (nfds == -1) { perror(\u0026#34;epoll_wait\u0026#34;); exit(EXIT_FAILURE); } log(\u0026#34;get %d events from epoll_wait!\\n\u0026#34;, nfds); for (int n = 0; n \u0026lt; nfds; ++ n) { if (events[n].data.fd == listen_sock) { struct sockaddr_in local; socklen_t addrlen; conn_sock = accept(listen_sock, (struct sockaddr *) \u0026amp;local, \u0026amp;addrlen); if (conn_sock == -1) { perror(\u0026#34;accept\u0026#34;); exit(EXIT_FAILURE); } log(\u0026#34;accept a new connection!\\n\u0026#34;); // set non-blocking  set_fd_nonblocking(conn_sock); ev.events = EPOLLIN | EPOLLET | EPOLLRDHUP; ev.data.fd = conn_sock; if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, \u0026amp;ev) == -1) { perror(\u0026#34;epoll_ctl: conn_sock\u0026#34;); exit(EXIT_FAILURE); } } else { if (events[n].events \u0026amp; (EPOLLRDHUP | EPOLLERR)) { log(\u0026#34;detect a closed/broken connection!\\n\u0026#34;); epoll_ctl(epollfd, EPOLL_CTL_DEL, events[n].data.fd, NULL); close(events[n].data.fd); } else response_to_conn(events[n].data.fd); } } } } int main(int argc, char **argv) { log(\u0026#34;listenning on port 1234!\\n\u0026#34;); listen_and_bind(); log(\u0026#34;creating epoll!\\n\u0026#34;); create_epoll(); log(\u0026#34;starting loop on epoll!\\n\u0026#34;); epoll_loop(); return 0; } ","date":"2018-04-16T19:51:21+08:00","permalink":"https://blog.crazyark.xyz/p/epoll/","title":"Linux IO 多路复用 —— Epoll"},{"content":"布隆过滤器 (Bloom Filter) 是一个空间高效的概率数据结构，它能够用来测试是否一个元素在一个集合中。布隆过滤器存在着 false positive (返回存在，其实不存在），但是不存在 false negative (返回不存在，其实存在)。集合中的元素越多，false positive 的概率就越高。\n结构描述 布隆过滤器的结构非常简单，即一个长度为 m 的比特数组，初始化都为0。然后，布隆过滤器需要有 k 个不同的哈希函数，每一个都能将元素映射到 [0, m) 中的一个数字，也就是数组上的一个位置，并呈现均匀分布。\n通常，k 是一个远小于 m 的常数，与将要被加入的元素成比例。精确的 k 和 m 的取值取决于期望的 false positive 率。\n下图是一个布隆过滤器：\n \n那么介绍下布隆过滤器上的两个操作：\n 添加一个元素：对每一个哈希函数，都计算出对应的数组上的位置，并将其置为 1 查询一个元素：对每一个哈希函数，计算出对应的数组上的位置，如果都是 1，则返回元素存在，否则元素不存在  显然，如果一个元素曾经被添加过，在查询的时候一定存在，但是查询返回存在却不一定表明元素被添加过。\n在这种设计中，由于不允许 false negative，所以不可能进行元素删除。如果允许 false negative，可以通过计数数组来处理元素删除。如果是一次性的删除，可以通过新建一个记录被删除元素的布隆过滤器来实现，但是这个过滤器的 false positive 就会变成第一个过滤起的 false negative。\n空间和时间 布隆过滤器的空间效率非常高，比其他一些用于集合的数据结构高效的多，例如自平衡二叉树、字典树、哈希表等。期望 1% 以内误报率的布隆过滤器，在采用最优 k 的情况下，每个元素只需要 9.6 个比特存储，并且与元素本身无关。甚至，我们只需要为每个元素添加大约 4.8 个比特，就可以把误报率降低到 0.1%。但是，当集合中可能的元素非常少时，布隆过滤器显然要比比特数组差。\n在插入和查询时，布隆过滤器的时间复杂度都取决于 k 个哈希函数，通常哈希函数效率较高，我们认为时间复杂度就是 O(k)。\n误报率和最优选择 我们来推导一下误报的概率，当一个元素插入时，数组中一个位置还没被设置为 1 的概率显然为：$(1 - \\frac{1}{m})^{k}$，那么当已经插入了 n 个元素之后，这个位置仍然是 0 的概率为 $(1 - \\frac{1}{m})^{kn}$。\n所以当查询时，每个哈希函数的位置都为 1 的概率为: $(1 - \\left[1 - \\frac{1}{m}\\right]^{kn})^k \\approx (1 - e^{-kn/m})^k$。\n这个概率的推导并不完全正确，因为它假设每个比特被设置的事件是相互独立的。事实上当去除假设时，仍然可以分析出相同的近似结果，有兴趣的同学可以阅读。那么我们有\n 误报率随着 m 的增大而减小，并随着 n 的增大而增大  那么我们将有最优的哈希函数数为: $k = \\dfrac{m}{n}\\ln 2$ 推导过程如下所示:\n$p = (1 - e^{-kn/m})^k$\n$\\ln p = k\\ln(1 - e^{-kn/m}) = -\\dfrac{m}{n}\\ln(e^{-kn/m}) \\ln(1 - e^{-kn/m})$\n令 $g = e^{-kn/m}$，即有 $\\ln p = -\\dfrac{m}{n}\\ln(g) \\ln(1 - g)$，由对称法可知 $g = \\dfrac{1}{2}$ 时上式最小。\n所以有 $g = e^{-kn/m} = \\dfrac{1}{2}$, $k = \\dfrac{m}{n}\\ln 2$。\n估计集合大小 Swamidass \u0026amp; Baldi (2007) 给出了布隆过滤器中集合的大概大小：\n$$n^* = -\\dfrac{m}{k}\\ln\\left[1 - \\dfrac{X}{m}\\right]$$，\n其中 X 是比特数组中 1 的个数。\n布隆过滤器的应用  Google Bigtable、Apache HBase、Apache Cassandra 和 Postgresql 使用布隆过滤器来减少不必要的磁盘查询。 Google Chrome 使用布隆过滤器来检测危险的 URL。 \u0026hellip;  参考文献 [1] https://en.wikipedia.org/wiki/Bloom_filter#Approximating_the_number_of_items_in_a_Bloom_filter\n","date":"2018-04-16T13:02:27+08:00","permalink":"https://blog.crazyark.xyz/p/bloom-filter/","title":"布隆过滤器 (Bloom Filter)"},{"content":"Emm，这篇其实在 2017 年 9 月就打算写了，到现在才填上，再不填又要忘记了\u0026hellip;\n字符串界的明星回文串，总是有各种稀奇古怪难搞的题目，比如说这道最长回文子串 (Longest Palindromic Substring)，显而易见的算法复杂度是 $O(n^2)$，而这个 Manacher\u0026rsquo;s Algorithm 则可以在 $O(n)$ 的时间给出答案。\n我们用一个简单但是不错的例子来解释这个算法: \u0026ldquo;abababa\u0026rdquo;，它的最长回文子串就是自己。\n简单的算法 简单的算法就是枚举回文核，对于一个长度为 n 的字符串，一共有 2n + 1 个回文核，每个回文核计算回文长度的复杂度为 $O(n)$，所以总计为 $O(n^2)$。\nManacher\u0026rsquo;s Algorithm Manacher\u0026rsquo;s Algorithm 利用了之前已经计算过的最大回文长度，来加速后面的计算。\n首先依旧是标明所有回文核，如下图所示：\n \n对于每个回文核，我们标号从 0 - 2n，一共 2n + 1 个。偶数号的为偶数长度的回文的核，奇数号的为奇数长度的回文的核。我们使用一个数组 LPS 来记录已经知道的最长回文长度。\n毫无疑问，LPS[0] 一定是 0，LPS[1] 一定是 1。假设我们从左往右计算 LPS，当前为标红的 7 号，LPS[7] = 7。\n这里有一个很重要的事情，那就是 7 是 i \u0026lt;= 7 中，i + LPS[i] 最大的，也就是右边界最远的那个。i + LPS[i] 一定是偶数。\n为什么一定要它呢，我们之后会详细讲。\n我们现在要计算 LPS[8] 了，因为 8 关于 7 对称为 6，而 6 上的最大回文串为 0，所以其实在可见范围 [i - LPS[i], i + LPS[i]] = [0, 14] 内，6 上的回文串 [6 - LPS[6], 6 + LPS[6]] = [6, 6] 在 8 上的对称为 [8, 8]，它整个在 (0, 14) 内，不接触边界。\n由回文串对回文核的对称性，LPS[8] = 0。\n但是如果对称到的回文串接触边界或者超出边界？我们已知的长度只能是 min(i + LPS[i] - cur, LPS[2 * i - cur]), 也就是对称那边的 LPS 和右边剩余范围这两个的最小值。那么我们就邻近了一些未知的字符，此时就应该 向右扫描未知的字符，并逐个和左边匹配，来准确得到 LPS[cur]。此时右边界扩展了，我们可以更新 i 到 cur。\n上述文字可能有点绕，但是也有现成的例子，比如 i = 5, cur = 7 时，对称过去是 3，LPS[3] = 3 = i + LPS[i] - cur，也就是接着右边界了，所以就向右扫描，把右边界扩展到 14，同时更新 i = 7。\n重复上述过程，就可以获得所有 LPS[i]，返回其中最大的即可。\n复杂度 LPS 的计算，分为两种情况，一是直接计算，而是计算初始值后扩展右边界。\n右边界最多扩展到 2 * n，所以是 O(n) 的，而赋值一共 2 * n + 1 次，所以也是 O(n) 的。\n所以总计时间复杂度为 O(n)。\nCpp class Solution { public: string longestPalindrome(string s) { if (s.empty()) return \u0026#34;\u0026#34;; int n = s.size(); int res = 1, resi = 1; int N = n * 2 + 1; vector\u0026lt;int\u0026gt; lps(N); lps[1] = 1; int c = 1; for (int i = 2; i \u0026lt; N; ++i) { int li = 2 * c - i, r = c + lps[c]; int to_r = r - i; // set initial lps[i]  lps[i] = min(lps[li], to_r); /* if (lps[li] \u0026gt;= to_r) { */ // expand if we can  // i + lps[i] \u0026lt; N - 1 \u0026amp;\u0026amp; i - lps[i] \u0026gt; 0 so that there are spaces for expand  while ( i + lps[i] \u0026lt; N - 1 \u0026amp;\u0026amp; i - lps[i] \u0026gt; 0 \u0026amp;\u0026amp; ((i + lps[i] + 1) % 2 == 0 || s[(i - lps[i] - 1) / 2] == s[(i + lps[i] + 1) / 2])) { lps[i]++; } if (i + lps[i] \u0026gt; r) { c = i; } /* } */ // record result  if (lps[i] \u0026gt; res) { res = lps[i]; resi = i; } } return s.substr((resi - res) / 2, res); } }; Reference [1] http://articles.leetcode.com/longest-palindromic-substring-part-ii\n[2] https://www.felix021.com/blog/read.php?entryid=2040\n[3] https://www.geeksforgeeks.org/manachers-algorithm-linear-time-longest-palindromic-substring-part-1/\n","date":"2018-04-13T01:50:51+08:00","permalink":"https://blog.crazyark.xyz/p/manachers-algorithm/","title":"最长回文子串 Manacher 算法 (Longest Palindromic Substring -- Manacher's Algorithm)"},{"content":"上回说到接下来都是图算法\u0026hellip; 这个，我食个言 🙃\n做题目碰到了博弈论，对这方面我真是完全不了解，还是要学习一个。本篇文章主要介绍组合游戏中的 Impartial Combinatorial Games，结构和内容均参考自 CMU Thomas S. Ferguson 教授的博弈论讲稿，结合我自己的思考。\nTake-Away Games 组合游戏是指一种两个玩家的游戏，每个玩家都有着完全的信息，不存在随机动作，游戏的结果总是赢或者输。游戏的每一个步骤由一个移动构成，通常玩家会交替地进行移动，直到达到终止状态。终止状态是指从该状态不存在任何一个状态移动方式的状态。\n显然，游戏的结果从一开始就被决定了，结果由游戏的状态集合、游戏初始状态以及玩家的先后手完全确定。\n组合游戏的形式有两种，这里我们主要讨论其中的一种游戏形式 Impartial Combinatorial Games，以下简称 ICG。ICG 是指在游戏中，两个玩家所能进行的移动是完全相同的。对应的另一种形式 Partizan Combinatorial Games 就是指两个玩家分别有不同的移动，比如说我们熟知的象棋。\nA Simple Take-Away Game 我们来看一个简单的小游戏：\n 假设我们有两个玩家，标记为 I 和 II 假设桌子上有一堆碎片，总计21个 每一次移动对应于从碎片堆中取出1或2或3个碎片 从玩家 I 开始，两个玩家轮流移动 最后一个移走碎片的玩家获胜  这个游戏足够简单，很容易就得知玩家I总是能获胜。但是如果我们把碎片数目改成 n，我们是否也能确定最终是谁获胜呢？\n显然，如果n在1到3之间，玩家 I 总是能获胜。假设现在场上的碎片数是n，并且是玩家 I 的回合。那么假设玩家 I 取走了m片碎片，场上剩余 n - m 片碎片。\n显然，如果玩家 II 能够确定对所有合法的 m，他能够在所有 n - m 的情况下都必胜，那么玩家 I 必败，否则玩家 I 必胜。\n注意到这里玩家 II 和玩家 I 一样拥有完全的信息，其实也就是问先手条件下的玩家 I 能不能对所有 n - m 都必胜，那么显然一个 bottom-up 或者 top-down 的动态规划能很容易地回答上述问题。\nCombinatorial Game 现在我们来形式化地定义一个组合游戏：\n 两个玩家构成的游戏 游戏有一个状态集合，表示游戏过程中所有可能状态，通常是有穷的 游戏的规则描述了在某个状态下玩家移动到下一个状态的合法移动，如果规则对两个玩家是相同的，就是 impartial 的，否则是 partizan 的 玩家交替移动 当玩家移动到一个状态，下一个移动的玩家没有可行的移动时，游戏结束；  在通常的游戏规则下，最后一个移动的玩家获胜 另一种规则叫做 mis`ere play rule，最后一个移动的玩家失败；这个规则极大地提高的分析的难度   游戏无论怎么进行，始终能在有限步内结束  注意游戏假设两个玩家都是足够聪明的玩家，不允许任何随机移动的存在。\nP-positions，N-positions 在上述简单的 Take-Away 游戏里，游戏的状态就是桌子上剩余的碎片数。可以看到，桌子上剩余的碎片数决定了哪个玩家能够最终获胜。更普遍的，游戏的当前状态决定了玩家的获胜情况。\n对于一个游戏状态，我们定义它为\n P-position，在该状态，上一个移动的玩家(Previous player)能够获胜，也就是后手必胜 N-position，在该状态，下一个移动的玩家(Next player)能够获胜，也就是先手必胜  上述游戏中，1、2、3、5、6 \u0026hellip; 都是 N-position，0、4、8、12、16 \u0026hellip;都是 P-position。\n显然，由定义可知：\n 对于每一个 P-position，对于任何一个合法的移动下的下一个状态一定是一个 N-position 对于每一个 N-position，一定存在一个合法移动，使得下一个状态是 P-position  对于上述的游戏，状态对应的 N、P 如下：\n   x 0 1 2 3 4 5 6 7 8 9 \u0026hellip;     position P N N N P N N N P N \u0026hellip;    Subtraction Games 我们来看一个更难一点的问题。令 S 是一个正整数的集合，n 是一个比较大的正整数。假设桌上有一堆碎片，总计n个，两个玩家轮流从中取出x个，这里x必须是S中的某个数，最后取的玩家获胜。\n通过状态对应的 N、P 分析，我们可以轻松的得出所有状态下结果。\nThe Game of Nim Take-Away 游戏中最出名的一类游戏叫做 Nim 游戏，它的游戏规则如下：\n 假设现在有三堆碎片，分别为 x1, x2, x3 两个玩家轮流取碎片，每次必须从其中的一堆取至少一个碎片，最多不能超过选择的堆 最后一个取碎片的玩家获胜  Nim 游戏不一定是三堆的，可以使其他的数目，为了方便起见，我们用三堆的特例来分析。\n我们将状态表示为一个三元对 (a, b, c)，每个数表示对应的堆剩余的碎片数。\n显然，终止状态是 (0, 0, 0)，它是一个 P-pos。\n  对于只剩一个一堆的 Nim 游戏 (0, 0, x), x \u0026gt; 0，显然先手必胜，也就是一个 N-pos。\n  对于两堆的 Nim 游戏，所有 (0, x, x) 的点都是 N-pos，其余点都是 P-pos：假设初始状态为 (0, x, x)，也就是说两堆相等，先手的玩家必须取走一些碎片，使得场上碎片数不相等，后手的玩家通过取走另一堆里相同数目的碎片可以恢复场上相等的条件，最终后手玩家一定胜利。\n  对于三堆的 Nim 游戏，就很难做一个简单分析了，接下去我们将介绍一个优美的分析方法，它将轻易的分析任意 Nim 游戏。\n  Nim-Sum \u0026amp; Bouton\u0026rsquo;s Theorem 对于一个有 m 个堆的 Nim 游戏，假设场上碎片数为 x1, x2, x3 \u0026hellip; xm，那么 (x1, x2, x3 \u0026hellip; xm) 是一个 P-pos 当且仅当 x1 ^ x2 ^ x3 \u0026hellip; ^ xm = 0，这里 ^ 为异或操作，异或和在这里被称为是 Nim-Sum。\n上述定理被称为是 Bouton\u0026rsquo;s Theorem。\n这里先介绍几个异或操作的性质：\n 自反，x ^ x = 0 结合律，a ^ (b ^ c) = (a ^ b) ^ c 交换律，a ^ b = b ^ a  下面我们来证明上述定理。对该定理的证明，我们只需要验证上述 P、N -pos 的几个条件就可以了：\n 所有终止点都是 P-pos，因为我们只有终止点 (0, 0, \u0026hellip; 0)，所以显然。 对每个 N-pos，存在一个移动使得下一个状态时是P-pos。假设当前状态为 (x1, x2, x3 \u0026hellip;, xm), Nim-Sum 为 a = x1 ^ x2 ^ x3 \u0026hellip; ^ xm，且 a 不等于 0。假设 a 的二进制最高位为第 k 位，那么 x1, x2 \u0026hellip; xm 所有数中第 k 位为 1 的数的个数是奇数，也就是说其中一定存在一个数，第 k 位为 1。不失一般性，假设是 x1。那么令 x1' 为 x1 在 k 之后的高位、k位 为 0、a 在 k 之后的地位拼接而成的数，显然 x1' \u0026lt; x1，所以 (x1, x2, x3 \u0026hellip;, xm) 到 (x1', x2, x3 \u0026hellip;, xm) 是一个合法的移动。此时 x1' ^ x2 ^ x3 \u0026hellip; ^ xm = a ^ x1' ^ x1 = 0，(x1', x2, x3 \u0026hellip;, xm) 是一个 P-pos。 对每个 P-pos，每一个移动都是到 N-pos。这里反证就行。  证毕。\nMis`ere Nim 对于 Mis`ere 规则下的 Nim 游戏，Bouton 给出了玩法：\n 当前场上还有至少两堆大于1的碎片时，和正常规则一样移动 否则场上只剩一堆大于1的碎片，从这一堆里取，使得堆里剩余0或者1个碎片，从而保证场上还剩奇数个堆，每个堆只有一个1碎片，从而必胜。  Graph Games 终于介绍到这里了，这一节将介绍一种 Impartial Combinatorial Games 中的重要的分析手段，叫做 Sprague-Grundy function。\n首先我们先把游戏转换成一个有向图G(X, F)，其中 X 是游戏中所有状态的集合，F 表示状态之间的移动关系，令 x 表示一个状态，那么 F(x) 表示在 x 状态下的下一个合法状态，也就代表了移动的集合。\n如果对于节点x，F(x) 是空集，那么 x 就是一个终止节点。\n为了分析方便起见，我们假设这样的图游戏是渐进有界的：存在一个整数n，使得图上任何一条路径是的长度不超过n。\nSprague-Grundy Function 定义图 G(X,F) 上的 SG-函数为如下形式：\n$g(x) = \\min{{n \\ge 0: n \\ne g(y)\\ \\textrm{for}\\ y \\in F(x)}}$\n也就是说，g(x) 代表了在 x 的后继节点的 SG-函数中没有出现的第一个非负整数。我们定义一个操作叫做 minimal excludant, 简写为 mex，为给出不出现在一个非负整数集合中的第一个非负整数，那么 g(x) 可以写成如下形式：\n$g(x) = \\mathrm{mex}{{g(y): y \\in F(x)}}$\n注意到 g(x) 是递归定义的，对终止节点x，g(x) = 0。\nThe Use of the Sprague-Grundy Function 令 g 为图游戏 G(X, F) 上的 SG-函数，我们有如下性质：\n x是终止节点，那么 g(x) = 0 g(x) = 0，那么对于 x 每个后继节点 y，g(y) != 0 g(x) != 0，那么存在一个 x 的后继节点 y，g(y) = 0  根据 g 的定义，上述三条结论是显然的。\n现在我们可以说，对于任意 g(x) = 0 的节点，x 对应的状态为一个 P-pos，其余的节点都为 N-pos。\n事实上，SG-函数给出了获胜的路径：当前状态的 GF-函数交替变为 0。\n该理论也可以推广到渐进有穷的的图上：图的每一条路径都是有穷的。\nSums of Combinatorial Games 假设我们现在有几个组合游戏，我们打算把它合并成一个大游戏：\n 玩家的移动变为：选择其中一个未终止的游戏，进行一次合法移动 终止状态为：所有游戏都终止了  这样合并出来的游戏被称为游戏的 disjunctive sum。\n假设我们所合并的游戏分别表示为图 G1(X1, F1)，G2(X2, F2) \u0026hellip;，Gn(Xn, Fn)，我们可以把它们合并成一张新的图 G(X, F)，其中 $X = X_1 \\times X_2 \\times \\cdots \\times X_n$ 为笛卡尔乘积；令 x = (x1, \u0026hellip; xn) ，则 $F(x) = F(x_1, \u0026hellip;, x_n) = F_1(x_1) \\times {x_2} \\times \\cdots {x_n} \\cup {x_1} \\times F_2(x_2) \\times \\cdots{x_n} \\cup \\cdots {x_1} \\times {x_2} \\times \\cdots F_n(x_n)$\nThe Sprague-Grundy Theorem 合并后的图游戏中，我们可以通过以下方式给出图上的 SG-函数 g：假设原来游戏的 SG-函数分别为 g1, g2, \u0026hellip;, gn，那么对于 x = (x1, x2, \u0026hellip;, xn)，g(x) = g1(x1) ^ g2(x2) ^ \u0026hellip; gn(xn)，也就是子游戏当前状态的 SG-函数的 Nim-和。\n令 x = (x1, x2, \u0026hellip;, xn) 为图上任意一点，b = g1(x1) ^ g2(x2) ^ \u0026hellip; gn(xn) ，事实上我们只需要证明如下两点就可以得出上述结论：\n 对每个非负整数 a \u0026lt; b，存在一个 x 的后继 y，使得 g(y) = a 任意 x 的后继不为 b  此处证明与上一节 Nim 和的证明类似，就留作习题吧。\n事实上，Nim 游戏中，显然每堆的 SG-函数 g(n) = n，所以 Nim 游戏判断 N、P-pos 是 Sprague-Grundy 的一个特例。\n也可以说，任意ICG都等价于某个Nim游戏。\nGreen Hackenbush 下面介绍一个组合游戏，叫做 Hackenbush ，这个游戏的大意是从一个有根的图上砍下一些边，并移除不接地的部分。额，直译有点难，这里hack翻译为砍，bush是灌木，从 Hackenbush 这个游戏名的字面上理解就行了，就是砍灌木丛的游戏。\n这个游戏的 impartial 版本叫做 Green Hackenbush：图上的每条边都是绿色的，两个玩家都能砍任意边。Partizan 版本的游戏叫做 Blue-Red Hackenbush，其中图上的边有蓝色和红色之分，玩家 I 只能砍蓝色的，而玩家 II 只能砍红色的。更为一般的版本中，边有上述三种，绿色是都能砍的。\nBamboo Stalks 先看一个简单的 Green Hackenbush 的特例，这个特例里，每个有根的联通分支都是一条直线，就跟柱子一样，如下图所示：\n Bamboo Stalks \n每一次移动就是选取其中一条边，砍断它并移除所有不连到地面的部分。玩家交替进行移动，最后一个移动的玩家胜利。\n显然，有n根柱子的这个游戏和有n堆的nim游戏是等价的。\nGreen Hackenbush on Trees 顾名思义，每棵灌木长的和树一样：\n Green Hackenbush on Trees \n由上一节的定理，这个游戏等价于某个版本的 Bamboo Stalks。我们可以通过以下原则来进行转换，更为普遍地，叫做 Colon Principle：\n 当分支交汇于某个顶点时，可以用一条长度为分支长度 nim-和的杆来代替   Colon Principle \nColon Principle 的证明应该比较简单，这里留作思考。\nGreen Hackenbush on general rooted graphs 现在我们来考虑任意的图，这些图中可能存在环等，如下所示：\n Arbitrary Green Hackenbush \n这图看着复杂，其实同样对于每个联通分支都可以转换为 Nim 游戏中的一个堆。考虑如果能将每个分支转换成一棵树，那就可以用上面的方法转换成等价的堆了。\n这里介绍一种转换方式，叫做 Fusion Principle： 在不改变图上 SG-函数值的情况下，回路上的点可以进行合并。\n如下图所示，我们可以将门形状的回路转换为长度为1的杆：\n Fusion Principle Example \n首先，地上的两个点事实上可以合并成一个点；然后应用fusion principle，这个三角形等价于三个独立的自环；而每个自环又等价于一个大小为1的nim堆，最终合并为一个大小为1的堆。\n更一般地，奇数长度的回路都可以转换为一条边，偶数长度的回路转换为一个点。\n关于 fusion principle 的证明比较长，有兴趣的同学可以参考 《Winning Ways》 的第七章。\nA Problem 这里给出一个具体的算法问题 Bob\u0026rsquo;s Game，原题在 https://www.hackerrank.com/contests/university-codesprint-3/challenges/bobs-game/problem 。\n题目大意：\n Bob 发明了一个新游戏，这个游戏在一个 n x n 的棋盘上进行。游戏的规则如下：\n 一开始，棋盘上有一些国王。一个格子中可以存在多个国王。 国王只允许向上、左、左上移动 棋盘上有些格子损坏了，国王不能移动到损坏的格子上 国王不能移出棋盘 游戏由两个玩家进行，他们交替进行操作 玩家每次操作移动一个国王 最后一个移动的玩家获胜  Bob 和他的好朋友 Alice 玩这个游戏，Bob 先手。假设给定棋盘的初始状态，给出 Bob 一定能获胜的第一步移动的数目；如果 Bob 必败，输出 LOSE。\n 假设棋盘上只有一个国王，这个游戏通过 SG-函数很容易分析 Bob 是否必胜，以及必胜的第一步操作。\n现在场上存在多个国王，但是国王之间互相不干扰，所以这个游戏是由多个单国王的游戏合并而成的。应用 Sprague-Grundy 定理，我们可以推导出这个游戏的 SG-函数。\nReferences [1] http://www.cs.cmu.edu/afs/cs/academic/class/15859-f01/www/notes/comb.pdf\n","date":"2017-10-11T00:40:08+08:00","permalink":"https://blog.crazyark.xyz/p/game-theory/","title":"博弈论-公平组合游戏 (Game Theory -- Impartial Combinatorial Games)"},{"content":"上几篇博文主要是关于范围更新和范围查询的几个数据结构，接下去的主题是图论和图算法，希望能够学习和回忆起大部分图算法。\n今天遇到一个问题，可以转化成二部图(bipartite graph)上的完美匹配的存在性问题，我们先来看一下完美匹配的理论，Hall\u0026rsquo;s Marriage theorem；然后介绍两个算法，用于解决完美匹配的超集——最大匹配问题。\nHall\u0026rsquo;s Marriage Theorem 令 $G$ 表示一个二部图，左部和右部分别为 $X$ 和 $Y$。令 $W \\subset X$, $N_G(W)$ 为 $W$ 在 $Y$ 中的相邻点的集合。\n那么如果存在一个匹配方式覆盖整个 $X$ 当且仅当\n$\\forall W \\subset X, |W| \\le |N_G(W)|$，也就是说每个 $X$ 的子集都有足够的邻居做匹配。\nDeduction 1 [1] 加入一个二部图 $G(X + Y, E)$，$|X| = |Y|$，$G$ 是连通图，且每个 $X$ 中的点的度数都不相同，那么 $G$ 上一定存在完美匹配。\n证明：\n首先，因为 $G$ 是连通图，所以 $\\forall u \\in X, deg(u) \u0026gt;= 1$。 那么 $\\forall W \\subset X$， $\\max\\limits_{u \\in W}{deg(u)} \\ge |S|$，满足 Hall\u0026rsquo;s Marriage Theorem，得证。\nHungarian Algorithm 事实上，我对照着找了半天，并没有找到一个叫匈牙利算法 (Hungarian Algorithm) 的用于二部图最大匹配的算法，唯一找到的 Hungarian Algorithm 是用于任务分配问题 (Assignment Problem)，也就是带权二部图的最大匹配。\n由于最大匹配问题与最大流问题能够很容易的互相转换，所以同时我也找到了许多思想类似甚至一致的算法，如 Ford-Fulkerson Algorihtm，Edmonds-Karp Algorithm 等。\n至于匈牙利算法这个名字是哪本书上提出的，我已经记不太清了 \u0026hellip; 应该是某本学习过的图论书，在此主要讲一下它的具体思想和证明。\nAlternating Path \u0026amp; Augmenting Path 假设我们有一个二部图 $G(U + V, E)$，现在有一个匹配 $M \\subset E$，此时如果 $M$ 中的边上的点集中不存在一个点，我们就说该点是未匹配的，未匹配边的定义类似。\n交替路径 (alternating path)：从一个未匹配点出发，经过未匹配边、匹配边、未匹配边\u0026hellip;这样交替的路径叫做交替路径。 增广路径 (augmenting path)：从 $U$ 中一个未匹配点出发，到达 $V$ 中一个未匹配点的交替路叫做增广路径。\n显然，由增广路径的定义，可以知道增广路径上未匹配边比匹配边要多一条，并且将这条路径上所有未匹配边改为匹配边，匹配边改为未匹配边，则修改后的匹配 $M'$ 比原来大1。\nTheorem of Augmenting Path 一个匹配 $M$ 是最大匹配当且仅当那么在图 $G$ 上不存在增广路径。\n证明如下：\n假设存在一条增广路径，$M$ 显然不是最大的。所以我们只需要证明当不存在增广路径时，$M$ 是最大的。\n假设存在一个匹配 $M$，不存在增广路径并且 $M$ 不是最大匹配，我们令 $M^$ 为 $G$ 上的一个最大匹配，显然 $|M^| \u0026gt; |M|$。\n所以同时有 $|M^* - M| \u0026gt; |M - M^*|$。\n考察所有在 $M^$ 和 $M$ 对称差 ($M^ \\cup M - M^* \\cap M$) 中的边，令 $G'$ 是由点 $U + V$ 和上述边构成的图。\n因为 $G'$ 中的边是来自两个匹配，所以 $G'$ 上任意一个点最多与两条边相连。\n因此，对于 $G'$ 上的任意联通分支，只可能是一条路或者一个环，并且边的数目是偶数，并且路或者环上对于 $M^* - M$ 或者 $M - M^*$ 一定构成交替路。\n因为 $|M^* - M| \u0026gt; |M - M^*|$ 并且环都是偶数条边，所以一定有一条路，它的边，它的起点和终点都在 $M^* - M$ 中，且 $M^* - M$ 和 $M - M^*$ 交替构成，显然这条路对于 $M$ 构成增广路径，矛盾！\n得证。\nPseudocode 匈牙利算法有两种实现，分别基于 DFS 和 BFS，时间复杂度都是 $\\mathcal{O}(|V||E|)$。\n下面是 BFS 版本的伪代码：\nAlgorithm MaximumBigartiteMatching(G) initialize set M of edges // can be the empty set initialize queue Q with all the free vertices in V while not Empty(Q) do w ← Front(Q) if w ε V then for every vertex u adjacent to w do // u must be in U if u is free then // augment M ← M union (w, u) v ← w while v is labeled do // follow the augmenting path u ← label of v M ← M - (v, u) // (v, u) was in previous M v ← label of u M ← M union (v, u) // add the edge to the path // start over remove all vertex labels reinitialize Q with all the free vertices in V break // exit the for loop else // u is matched if (w, u) not in M and u is unlabeled then label u with w // represents an edge in E-M Enqueue(Q, u) // only way for a U vertex to enter the queue else // w ε U and therefore is matched with v v ← w's mate // (w, v) is in M label v with w // represents in M Enqueue(Q, v) // only way for a mated v to enter Q 相比于 BFS，DFS 版本的匈牙利算法更容易实现，它 C++ 代码可以参考附录。\nHopcroft-Karp Algorithm Hopcroft-Karp 算法是一个专用于解二部图最大匹配问题的算法，它最差情况的时间复杂度为 $\\mathcal{O}(|E|\\sqrt{|V|})$，最差情况下的空间开销为 $\\mathcal{O}(|V|)$。\nHopcroft-Karp 算法是在1973年由 Hohn Hopcroft 和 Richard Karp 两位计算机学者发现的。\n和匈牙利算法一样，Hopcroft-Karp 算法同样是不断地通过寻找增广路径，来增大部分匹配。不同的是，匈牙利算法每次只找到一条增广路径，而该算法则每次找增广路径的一个最大集合，从而我们只需要进行 $\\mathcal{O}(\\sqrt{|V|})$ 次迭代。\nHopcroft-Karp 算法循环以下两个阶段：\n 用 BFS 寻找下一个长度的增广路径，并且能遍历该长度下所有增广路径 (也就是上面所说的最大集合)。 如果存在更长的增广路径，对每个可能的起点 u，用 DFS 寻找并记录增广路径  每一次循环，BFS 所找到的最短增广路径的长度至少增加1，所以在 $\\sqrt{|V|}$ 次循环以后，能找到的最短增广路径长度至少为 $\\sqrt{|V|}$。假设当前的部分匹配集合为 $M$ (边集)，$M$ 和最大匹配的对称差组成了一组点不相交的增广路径和交替环。如果这个集合内所有的路径的长度都至少为 $\\sqrt{|V|}$，那么最多只有 $\\sqrt{|V|}$ 条路径，那么最大匹配的大小与 $|M|$ 最多为 $\\sqrt{|V|}$。而每次循环至少将匹配大小增加1，所以直到算法结束最多还有 $\\sqrt{|V|}$ 次循环。\n每次循环中，BFS 最多遍历图中每条边，DFS 也是最多遍历每条边，所以每一轮循环的时间复杂度为 $\\mathcal{O}({|E|})$，总时间复杂度为 $\\mathcal{O}({|E|\\sqrt{|V|}})$。\nPseudocode /* G = U ∪ V ∪ {NIL} where U and V are partition of graph and NIL is a special null vertex */ function BFS () for each u in U if Pair_U[u] == NIL Dist[u] = 0 Enqueue(Q,u) else Dist[u] = ∞ Dist[NIL] = ∞ while Empty(Q) == false u = Dequeue(Q) if Dist[u] \u0026lt; Dist[NIL] for each v in Adj[u] if Dist[ Pair_V[v] ] == ∞ Dist[ Pair_V[v] ] = Dist[u] + 1 Enqueue(Q,Pair_V[v]) return Dist[NIL] != ∞ function DFS (u) if u != NIL for each v in Adj[u] if Dist[ Pair_V[v] ] == Dist[u] + 1 if DFS(Pair_V[v]) == true Pair_V[v] = u Pair_U[u] = v return true Dist[u] = ∞ return false return true function Hopcroft-Karp for each u in U Pair_U[u] = NIL for each v in V Pair_V[v] = NIL matching = 0 while BFS() == true for each u in U if Pair_U[u] == NIL if DFS(u) == true matching = matching + 1 return matching A Problem 一个可以转换成做完美匹配的题目，题目大意：\n 二维平面上一共 n 个人和 n 个防空洞，现在你需要将 n 个人分配到防空洞中，使得每个防空洞仅容纳一个人，并且所有人进入防空洞的时间最短，即最晚进入防空洞的人的时间最短。一个人从 (X, Y) 移动到 (X1, Y1) 所需时间为 |X - X1| + |Y - Y1|。 1 \u0026lt;= n \u0026lt;= 100\n 这道题直接做我没有想到什么好办法，但是观察到可能解一定为某个人移动到某个防空洞的时间，所以我们将所有人移动到所有防空洞的时间全部计算出来并排序。\n对于某个人移动到某个防空洞，假设耗时为 T，那么所有耗时小于等于 T 的移动操作是可行的。我们建立一张二部图，左边是人的集合，右边是防空洞的集合，对于所有可行操作，我们在二部图上添加一条对应的边。那么如果此时存在一种分配方式满足上述条件，它在图上一定是一个完美匹配，而目标就是找到这样最小的一个T。\n对于我们的二部图，最差情况为完全二部图，对于匈牙利算法判定完美匹配的时间复杂度为 $\\mathcal{O}(n^3)$，Hopcroft-Karp 算法为 $\\mathcal{O}(n^2\\sqrt{n})$，所以判定复杂度还是比较高的。\n假如我们一条一条添加，也就是按照耗时顺序添加，那么最坏情况一共要判定 $n^2$ 次，这太高了，这里数据比较小还可以，但是万一n大到1000就难说了。\n还记得之前我们提过的减小判定次数的方式嘛？对，二分查找，一共判定 $2\\log n$ 次。\n在这里，我们同时也存在模拟复杂度，这里模拟为构造对应的二部图，每次构造的最坏时间复杂度为 $n^2$，所以总计时间复杂度为 $\\mathcal{O}(n^3\\log n)$ 或者 $\\mathcal{O}(n^2\\sqrt{n}\\log n)$。\n代码实现在附录中。\nReferences [1] https://en.wikipedia.org/wiki/Hall%27s_marriage_theorem\n[2] https://math.stackexchange.com/questions/1204270/bipartite-graph-has-perfect-matching\n[3] https://en.wikipedia.org/wiki/Matching_(graph_theory)\n[4] https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm\n[5] https://www.topcoder.com/community/data-science/data-science-tutorials/maximum-flow-augmenting-path-algorithms-comparison/\n[6] http://www.csl.mtu.edu/cs4321/www/Lectures/Lecture%2022%20-%20Maximum%20Matching%20in%20Bipartite%20Graph.htm\nAppendix Air Defense Exercise #include \u0026lt;cstdio\u0026gt;#include \u0026lt;cstdlib\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;string\u0026gt;#include \u0026lt;stack\u0026gt;#include \u0026lt;cmath\u0026gt;#include \u0026lt;deque\u0026gt;#include \u0026lt;queue\u0026gt;#include \u0026lt;map\u0026gt;#include \u0026lt;bitset\u0026gt;#include \u0026lt;set\u0026gt;#include \u0026lt;list\u0026gt;#include \u0026lt;unordered_map\u0026gt;#include \u0026lt;unordered_set\u0026gt;#include \u0026lt;sstream\u0026gt;#include \u0026lt;numeric\u0026gt;#include \u0026lt;climits\u0026gt;#include \u0026lt;utility\u0026gt;#include \u0026lt;iomanip\u0026gt;#include \u0026lt;cassert\u0026gt; using namespace std; using ll = long long; using ii = pair\u0026lt;int, int\u0026gt;; using iii = pair\u0026lt;int, ii\u0026gt;; template \u0026lt;class T\u0026gt; using vv = vector\u0026lt;vector\u0026lt;T\u0026gt;\u0026gt;; #define rep(i, b) for (int i = 0; i \u0026lt; int(b); ++i) #define reps(i, a, b) for (int i = int(a); i \u0026lt; int(b); ++i) #define rrep(i, b) for (int i = int(b) - 1; i \u0026gt;= 0; --i) #define rreps(i, a, b) for (int i = int(b) - 1; i \u0026gt;= a; --i) #define repe(i, b) for (int i = 0; i \u0026lt;= int(b); ++i) #define repse(i, a, b) for (int i = int(a); i \u0026lt;= int(b); ++i) #define rrepe(i, b) for (int i = int(b); i \u0026gt;= 0; --i) #define rrepse(i, a, b) for (int i = int(b); i \u0026gt;= int(a); --i)  #define all(a) a.begin(), a.end() #define rall(a) a.rbegin(), a.rend() #define sz(a) int(a.size()) #define mp(a, b) make_pair(a, b)  #define inf (INT_MAX / 2) #define infl (LONG_MAX / 2) #define infll (LLONG_MAX / 2)  #define X first #define Y second #define pb push_back #define eb emplace_back  // tools for pair\u0026lt;int, int\u0026gt; \u0026amp; graph template \u0026lt;class T, size_t M, size_t N\u0026gt; class graph_delegate_t { T (\u0026amp;f)[M][N]; public: graph_delegate_t(T (\u0026amp;f)[M][N]) : f(f) {} T\u0026amp; operator[](const ii\u0026amp; s) { return f[s.first][s.second]; } const T\u0026amp; operator[](const ii\u0026amp; s) const { return f[s.first][s.second]; } }; ii operator+(const ii\u0026amp; lhs, const ii\u0026amp; rhs) { return mp(lhs.first + rhs.first, lhs.second + rhs.second); } // clang-format off template \u0026lt;class S, class T\u0026gt; ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream\u0026amp; os, const pair\u0026lt;S, T\u0026gt;\u0026amp; t) { return os \u0026lt;\u0026lt; \u0026#34;(\u0026#34; \u0026lt;\u0026lt; t.first \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; t.second \u0026lt;\u0026lt; \u0026#34;)\u0026#34;; } template \u0026lt;class T\u0026gt; ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream\u0026amp; os, const vector\u0026lt;T\u0026gt;\u0026amp; t) { os \u0026lt;\u0026lt; \u0026#34;{\u0026#34;; rep(i, t.size() - 1) { os \u0026lt;\u0026lt; t[i] \u0026lt;\u0026lt; \u0026#34;,\u0026#34;; } if (!t.empty()) os \u0026lt;\u0026lt; t.back(); os \u0026lt;\u0026lt; \u0026#34;}\u0026#34;; return os; } vector\u0026lt;string\u0026gt; __macro_split(const string\u0026amp; s) { vector\u0026lt;string\u0026gt; v; int d = 0, f = 0; string t; for (char c : s) { if (!d \u0026amp;\u0026amp; c == \u0026#39;,\u0026#39;) v.pb(t), t = \u0026#34;\u0026#34;; else t += c; if (c == \u0026#39;\\\u0026#34;\u0026#39; || c == \u0026#39;\\\u0026#39;\u0026#39;) f ^= 1; if (!f \u0026amp;\u0026amp; c == \u0026#39;(\u0026#39;) ++d; if (!f \u0026amp;\u0026amp; c == \u0026#39;)\u0026#39;) --d; } v.pb(t); return v; } void __args_output(vector\u0026lt;string\u0026gt;::iterator, vector\u0026lt;string\u0026gt;::iterator) { cerr \u0026lt;\u0026lt; endl; } template \u0026lt;typename T, typename... Args\u0026gt; void __args_output(vector\u0026lt;string\u0026gt;::iterator it, vector\u0026lt;string\u0026gt;::iterator end, T a, Args... args) { cerr \u0026lt;\u0026lt; it-\u0026gt;substr((*it)[0] == \u0026#39; \u0026#39;, it-\u0026gt;length()) \u0026lt;\u0026lt; \u0026#34; = \u0026#34; \u0026lt;\u0026lt; a; if (++it != end) { cerr \u0026lt;\u0026lt; \u0026#34;, \u0026#34;; } __args_output(it, end, args...); } #define out(args...) { vector\u0026lt;string\u0026gt; __args = __macro_split(#args); __args_output(__args.begin(), __args.end(), args); } // clang-format on  const int MAX_N = 100; int n; ii p[MAX_N], h[MAX_N]; iii d[MAX_N * MAX_N]; vector\u0026lt;int\u0026gt; edges[MAX_N]; int match[MAX_N]; bool visited[MAX_N]; void link(int u, int v) { edges[u].push_back(v); } bool dfs(int u) { for (auto v : edges[u]) { if (visited[v]) continue; visited[v] = true; if (match[v] == -1 || dfs(match[v])) { match[v] = u; return true; } } return false; } bool hungarian() { int m = 0; fill_n(match, n, -1); rep(i, n) { fill_n(visited, n, false); if (dfs(i)) ++m; } return m == n; } int match_u[MAX_N], match_v[MAX_N]; int dist[MAX_N]; int NIL = MAX_N; bool bfs() { queue\u0026lt;int\u0026gt; q; rep(u, n) { if (match_u[u] == NIL) { dist[u] = 0; q.push(u); } else { dist[u] = inf; } } dist[NIL] = inf; while (!q.empty()) { auto u = q.front(); q.pop(); for (auto v : edges[u]) { if (dist[match_v[v]] == inf) { dist[match_v[v]] = dist[u] + 1; if (match_v[v] != NIL) q.push(match_v[v]); } } } return dist[NIL] != inf; } bool dfs_h(int u) { if (u == NIL) return true; for (auto v : edges[u]) { if (dist[match_v[v]] == dist[u] + 1 \u0026amp;\u0026amp; dfs_h(match_v[v])) { match_u[u] = v; match_v[v] = u; return true; } } dist[u] = inf; return false; } bool hopcraft_karp() { int m = 0; fill_n(match_u, n, NIL); fill_n(match_v, n, NIL); while (bfs()) { rep(u, n) { if (match_u[u] == NIL \u0026amp;\u0026amp; dfs_h(u)) { ++m; } } } return m == n; } bool pfmatch() { return hopcraft_karp(); } int main() { cin \u0026gt;\u0026gt; n; rep(i, n) { cin \u0026gt;\u0026gt; p[i].first \u0026gt;\u0026gt; p[i].second; } rep(i, n) { cin \u0026gt;\u0026gt; h[i].first \u0026gt;\u0026gt; h[i].second; } rep(i, n) { rep(j, n) { int idx = i * n + j; int dis = abs(p[i].X - h[j].X) + abs(p[i].Y - h[j].Y); d[idx] = {dis, {i, j}}; } } sort(d, d + n * n); int l = 0, r = n * n - 1; // binary search  // time complexity: O(n^3lgn) for hungarian,  // O(n^2√n * lgn) for hopcroft-karp  while (l \u0026lt; r \u0026amp;\u0026amp; d[l].first != d[r].first) { // replay  rep(i, n) { edges[i].clear(); } int mid = (l + r) / 2; repe(i, mid) { int pi = d[i].second.first, hj = d[i].second.second; link(hj, pi); } if (pfmatch()) { r = mid; } else { l = mid + 1; } } cout \u0026lt;\u0026lt; d[l].first \u0026lt;\u0026lt; endl; return 0; } ","date":"2017-09-26T22:26:08+08:00","permalink":"https://blog.crazyark.xyz/p/maximum-matching-in-bipartite-graph/","title":"二部图的最大匹配 (Maximum Matching in Bipartite Graph)"},{"content":"不刷题不知道自己菜，越刷题越发现自己🙄 —— 记 HourRank23 被虐。\nSparse Table 还记得上两篇线段树和BIT都讲到了区间查找的问题，我们来回忆一下。\n线段树空间支持各种函数(Associative，需要满足结合律)的区间更新和区间查询，空间复杂度是 O(nlgn)，更新和查询的时间复杂度都是 O(lgn)。\nBIT 支持任意群上运算的单点更新/区间查询、区间更新/单点查询，空间复杂度是 O(n)，更新和查询的时间复杂度也都是 O(lgn) (其实取决于逆元构造速度)。BIT 的区间更新/区间查询泛化需要更多性质，反正主要是用于整数域上的和运算。\n而这里所要讲的 Sparse Table 是另一种支持区间查询的数据结构，针对的是不变的（immutable）的数组，其空间复杂度为 O(nlgn)。\nSparse Table 同样支持各种函数，只要是满足结合律的函数一律都是支持的，对所有这样的函数，其时间复杂度为 O(nlgn)，而且思想和编码都非常简单易懂。\n更进一步地，如果函数是幂等 (Idemponent) 的，Sparse Table可以在O(1)内得到区间查询的结果。\n核心原理 假设有一个长度为 $N$ 的数组 ${a_0, \u0026hellip;, a_{N - 1}}$，并有一个二元函数 $f$，满足结合律 $f(a, f(b, c)) = f(f(a, b), c)$。\n我们简记区间 $[i, j]$ 上对函数 $f$ 的查询为 $f(a[i..j])$。\n那么 Sparse Table 将生成这样一个二维数组，这个二维数组的大小为 $N(\\lfloor\\log N\\rfloor + 1)$。数组的第 $(i, j)$ 项代表了区间结果 $f(a[i..i + 2^j - 1])$，记为 $b_{i,j}$。\n生成一个这样的二维数组是很简单的，因为 $f(a[i..i + 2^j - 1]) = f(f(a[i..i+2^{j-1} - 1]), f(a[i + 2^{j-1}..i + 2^j - 1]))$，而后面这两个分别是第 $(i, j - 1)$ 项和第 $(i + 2^{j - 1}, j - 1)$ `项，并且 $f([i..i]) = a_i$，所以我们一层层递推就行，过程如下\n// assuming Arr is indexed from 0 for i=0..N-1: Table[i][0] = Arr[i] // assuming N \u0026lt; 2^(k+1) for j=1..k: for i=0..N-2^j: Table[i][j] = F(Table[i][j - 1], Table[i + 2^(j - 1)][j - 1]) 那么我们如何进行查询呢？因为对于一个区间 $[i, j]$ 来说，区间长度 $L = j - i + 1 \\le N$ 恒成立，所以如果我们将 $L$ 表示成二进制形式，$L = 2^{q_k} + 2^{q_{k - 1}} + \u0026hellip; + 2^{q_0}$， 那么有\n$j = (\\cdots((i + 2^{q_k} - 1) + 2^{q_{k - 1}} - 1) + \u0026hellip; + 2^{q_0}) - 1$，这个表示形式是不是提醒你了呢？\n所以用以下过程我们可以在 O(lgN) 时间内得到准确结果:\nanswer = ZERO L’ = L for i=k..0: if L’ + 2^i - 1 \u0026lt;= R: // F is associative, so this operation is meaningful answer = F(answer, Table[L’][i]) L’ += 2^i 假设我们的函数 $f$ 同时是幂等的，也就是说 $f(x, x) = x$ 对所有定义域内的数都成立，那么我们马上就能得到\n$f(a[i..j]) = f(f(a[i..s],f(a[t..j])), i \\le t, s \\le j, t \\le s + 1$。\n这条性质允许我们不用精确地只覆盖该区域一次，这是加速到 O(1) 的关键。\n令 $t$ 是满足 $2^t \\le (j - i + 1)$ 的最大的 $t$，也就是 $2^{t + 1} \u0026gt; (j - i + 1)$。那么显然 $i + 2^t - 1 \\le j$，$j - 2^t + 1 \\ge i$，并且有 $j - 2^t + 1 \\le (i + 2^t - 1) + 1$ 恒成立。\n所以 $f(a[i..j]) = f(f(i..i + 2^t - 1), f(j - 2^t + 1..j))$，后面两项就是 $b_{i, t}$ 和 $b_{j - 2^t, t}$。\n至此原理介绍完毕，实现的代码在最后 Appendix 中。\nST \u0026amp; LCA Sparse Table 不仅可以用于计算各种区间查询，还可以用于计算树上两个节点的最近公共祖先。使用ST可以在 O(2lgH) 的时间内计算出任意两个几点的最近公共祖先，空间复杂度还是 O(NlgN)，这里 N 是树上节点数，H 是树高度。\n其主要思想是用数组存放节点 i 的第 2^j 个祖先，然后搜索，具体细节有兴趣的同学可以参考 topcoder 上关于RMQ和LCA的那篇文章，链接在引用中，这里不再赘述。\nParallel Binary Search Parallel Binary Search，中文叫整体二分，我们先用一个spoj上的题目来介绍这个方法，然后再来看一下hourrank上的另一个不太一样的题目。\n接下来的两个小节大量参考了codeforces上的博客，原文链接在文章的末尾，有兴趣的同学可以前去学习。\nMotivation Problem SPOJ上有这样一个题目：Meteors，在这里我简单描述一下：\n 有 N 个国家，有一块圆形的地方，等分成 M 个扇形区域，每个区域属于某一个国家。现在预报有 Q 阵流星雨，每一针降落在某个扇形区域 [L, R]，每个扇形都获得相同数量 K 的陨石。已知每个国家有一个陨石收集目标 reqd[i]，请问每个国家分别最少在多少阵流星雨后才能收集到目标数量的陨石。\n  1 \u0026lt;= N, M, Q\u0026lt;= 300000 1 \u0026lt;= K \u0026lt;= 1e9\n Solution 如果我们考虑不使用任何数据结构逐次模拟，然后检查目标是否达到了，更新代价是O(M)，检查的代价是O(N + M)，总代价 O((2M + N)Q) 是无法承受的。\n而看到区间更新，结合Blog开头所讲的几个数据结构，选择BIT用于模拟更新是最合适不过的了；这样我们更新代价降到了 O(lgM)，但是检查的代价变为了 O(MlgM)，如果还是逐次模拟，显然会比之前更差。\n到了这里，能想到了什么了嘛？对，二分查找！二分查找不关心序列中每个数长什么样子，只需要知道：\n 序列单调 可以通过某种方式获取到序列中指定位置的值  然后，通过二分查找，我们可以在 O(lgN) 的值获取/检查内，找到我们想要的位置。\n这里的序列指一个国家的收集总数，而这个收集总数由扇形区间所构成的序列隐式地反应，那么通过二分查找，对每个国家我们只需要进行 O(lgQ) 次检查，就能知道是在那个时间点满足条件。所以每个国家的总计时间复杂度为大约为 O(logQ * Q * logM)，这里每次检查的时间相比更新的时间微不足道所以略去了。最终复杂度为 O(N * log Q * Q * logM)。\n可以看到，上面的复杂度还是太高，比直接模拟还是高了很多，那怎么办呢？\n稍微想一想，每次模拟的代价虽然降低了，但是模拟的数量上升到了 O(NlgQ) 次，虽然检查的次数也降低了，但是跟模拟的耗时相比，低的都被略掉了 \u0026hellip; 所以当务之急是同时能降低模拟的开销。\n是不是感觉有点奢侈？每次我就检查那么一下，就要模拟一整轮，虽然我开销小，也架不住你烧CPU玩啊。\n其实每个国家的二分查找经历是很类似的，在整个模拟出来的序列里面找到那一片要的，然后检查一下我们就下一轮了。那么，既然一整轮模拟都做完了，能不能让每个国家都检查完，然后我们再开下一轮好不好？\n这就是 Parallel Binary Search 的核心思想：通过一轮模拟完成所有国家的这一步查找，从而将模拟总数减少到了 O(lgQ) 次，巨大的进步！\n具体方法是，开两个长度为N的数组，对每一个国家，记录它当前的 L 和 R；对于每一个要检查的 mid，开一个链表记录当前需要检查 mid 值的国家；其余过程用下列伪代码描述：\nfor all logQ steps: clear range tree and linked list check for all member states i: if L[i] != R[i]: mid = (L[i] + R[i]) / 2 insert i in check[mid] for all queries q: apply(q) for all member states m in check[q]: if m has requirements fulfilled: R[m] = q else: L[m] = q + 1 上述过程中，apply() 函数的作用是，进行模拟，然后检查需要检查的国家是否满足条件了。\n这样子，模拟的代价总计为 O(lgQ * Q * lgM)，检查的代价总计为 O(lgQ * (MlgM + N))，我们成功地同时将模拟和检查的代价都减小了！这样的时间复杂度总共为 O(lgQ * (Q + M) * lgM)，至于少的那个N，忽略不计~\n同原文一样，这里引一句大佬的话解释这个 Parallel Binary Search，\n \u0026ldquo;A cool way to visualize this is to think of a binary search tree. Suppose we are doing standard binary search, and we reject the right interval — this can be thought of as moving left in the tree. Similarly, if we reject the left interval, we are moving right in the tree. So what Parallel Binary Search does is move one step down in N binary search trees simultaneously in one \u0026ldquo;sweep\u0026rdquo;, taking O(N * X) time, where X is dependent on the problem and the data structures used in it. Since the height of each tree is LogN, the complexity is O(N * X * logN).\u0026rdquo; — rekt_n00b\n Hourrank23 \u0026ndash; Selective Additions 这是一道被诅咒的题目\u0026hellip;抱歉实在没忍住🤣\n这道题目是这样的：\n 有一个数组，长度为N，现在要进行 M 次区间更新，都是加一个正数。但是我有 K 个很喜欢的数字，一旦数组中某个元素成为我喜欢的数字，对它的更新就无效，它将一直保持那个数字。 问每轮更新后的数组和。\n1 \u0026lt;= N, M \u0026lt;= 1e5 1 \u0026lt;= k \u0026lt;= 5\n 这道题和上道题不太一样，检查的代价低、检查目标和原区间相同，而且目标变成了多个。\n目标是多个可以这样解决：对喜欢的数排序，因为永远是正的更新，所以先到达前一个后一个就不用检查了。\n采用上述的 PBS 的方法，这题的复杂度在 O(k(n + m)lognlogm)，是一个很好的复杂度了。\n但是，这里一定要有个但是，因为检查的代价很低，所以这题的二分查找不是必须的，我翻阅了大量大佬的代码和editorial，有用PBS的，有模拟时间序列的，有用Segment tree加trick的，各种各样，我来一个个介绍一下：\nPBS 这个不多说了，离线模拟 logm 遍。\nTime Series Simulation 代码: https://www.hackerrank.com/rest/contests/hourrank-23/challenges/selective-additions/hackers/nuipqiun/download_solution\n同样是离线处理，但是这个大佬的模拟方式很特别，他是在时间序列上模拟数组当前项的值，我还是第一次见。\n首先还记得 BIT 的区间更新方式么，在l处加delta，r + 1处减去delta。假设考虑区间更新都是 [l, n] 的，那么在时间序列上模拟 i-th 数组项 的值就很容易想到了：对于在 i 以及之前的更新(j, a, b, delta) (a \u0026lt;= i), 在时间序列对应数组的 j处加delta。这样j时间之后的数都被加了delta。\n想到这里，时间序列上的模拟就呼之欲出了，既然对应有加，肯定对应有减：对于 i 以及 i 之前的所有更新 (j, a, b, delta) (b \u0026lt; i)，在序列上抵消掉，也即在时间序列j处减delta，因为这样的更新不应该影响到数组项 i 的时间序列，而且一定在之前被更新上去了。\n所以就变成了这样一种方式：\nrep(i,n){ for(int j:ad[i]) add(j,ads[j]); for(int j:rm[i]) add(j,-ads[j]); // do you work } 这里，ad[i] 是 [i, r] 更新的索引号，rm[i] 是 [l, i - 1] 更新的索引号，add 操作是fenwick tree的加操作，于是我们可以用sum(j)计算出任一个时间节点上a[i]的值。\n后面就很简单了，对每个i二分查找，找到就记下来。总计时间复杂度应该为 O((kn + m)logm)。\n这里明显比 PBS 快了一点，这是因为检查区间和原区间一致。这件事情很重要，类比到 Meteors，虽然我们也能模拟出一个区间的时间序列，但是一个区间对于目标没有意义。\nSegment Tree 与上面两个不同的是，这是个在线算法。\n时间复杂度是 O(klgm(n + m))，不过用了线段树，空间复杂度为 O(kmlgm)。\n这里线段树中记录了三个值：lazy域add，区间最大值maxi，对应的数组索引id。\n一共k棵线段树，每颗线段树初始化为数组 {a_i - fav_j}，也就是说，对应的值如果为正数，才有检查的必要，如果为负数，那压根还没到检查的时候。\n正巧，利用线段树的结构，我们检查线段树的root节点就能知道这棵树中存不存在需要检查的项，这个操作是 O(1) 的。而当一个数组项被检查过后，它将被赋值为 -inf，这样它永远也不会被再次检查，线段树也顺利更新到下一个待检查状态。\n由于每个数组项对于每个favorite最多被检查一次，所以总计检查复杂度为 O(knlgm)。\n更新复杂度是 O(kmlgm)，所以总计复杂度为 O(k(m + n)lgm)。\n这里利用Segment Tree的trick一开始让我百思不得其解，为什么meteors不行而这里可以，想来想去也想不出第三种方式，把meteors的查询区间映射到方便修改的结构上，所以还是归结为之前所述的两个原因。\nConclusion 对于更新/查询的一系列问题，业界大佬们已经提供了各种各样有力的武器：\n 数据结构 Segment tree、Fenwick tree、Sparse table，还有最近才研究到的 Cartesian tree 等等 二分、整体二分、还没看过的 CDQ 分治等等  把这里整理进脑子里，大概下次刷题的时候底气也会足一些🍺\nReference [1] https://www.hackerearth.com/practice/notes/sparse-table/\n[2] https://www.topcoder.com/community/data-science/data-science-tutorials/range-minimum-query-and-lowest-common-ancestor/#Sparse_Table_(ST)_algorithm\n[3] http://codeforces.com/blog/entry/45578\n[4] https://ideone.com/tTO9bD\nAppendix Impl ST/C++ #include \u0026lt;vector\u0026gt;#include \u0026lt;cassert\u0026gt;#include \u0026lt;cstring\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;limits\u0026gt;#include \u0026lt;type_traits\u0026gt;#include \u0026lt;random\u0026gt;using namespace std; namespace st_impl { template \u0026lt;class T, class F\u0026gt; class SparseTable { public: typedef F func_type; typedef unsigned size_type; typedef T value_type; SparseTable(const vector\u0026lt;T\u0026gt;\u0026amp; init) : _size(init.size()), _idx_size(flsl(_size)) { table.resize(_size); for (auto\u0026amp; row : table) { row.resize(_idx_size, func_type::default_value); } // initialize sparse table  for (size_type i = 0; i \u0026lt; _size; ++i) { table[i][0] = init[i]; } for (size_type j = 1; j \u0026lt; _idx_size; ++j) { for (size_type i = 0; i \u0026lt;= _size - (1 \u0026lt;\u0026lt; j); ++i) { table[i][j] = f(table[i][j - 1], table[i + (1 \u0026lt;\u0026lt; (j - 1))][j - 1]); } } } SparseTable(const initializer_list\u0026lt;T\u0026gt;\u0026amp; init) : SparseTable(vector\u0026lt;T\u0026gt;(init)) {} SparseTable(const vector\u0026lt;T\u0026gt;\u0026amp; init, F f) : SparseTable(init) { this-\u0026gt;f = f; } SparseTable(const initializer_list\u0026lt;T\u0026gt;\u0026amp; init, F f) : SparseTable(vector\u0026lt;T\u0026gt;(init), f) {} T rangeQuery(size_type l, size_type r) const { if (!(l \u0026lt;= r \u0026amp;\u0026amp; r \u0026lt; _size)) { throw std::out_of_range(\u0026#34;Bad query!\u0026#34;); } // if the function is idempotent, which means f(x, x) = x holds for  // all x with definition, then we can deduce that  // f(range(l, s), range(t, r)) == f(range(l, r)) always  // holds for all l, s, t, r which satisfies l \u0026lt;= t \u0026amp;\u0026amp; s \u0026lt;= r \u0026amp;\u0026amp; t \u0026lt;= s + 1  // then rangeQuery will be executed in O(1).  // otherwise it should be finished in O(lgN).  if (func_type::idempotent) { size_type idx = flsl(r - l + 1) - 1; return f(table[l][idx], table[r - (1 \u0026lt;\u0026lt; idx) + 1][idx]); } else { T res = func_type::default_value; for (size_type i = 0; i \u0026lt; _idx_size; ++i) { size_type idx = _idx_size - 1 - i; if (l + (1 \u0026lt;\u0026lt; idx) - 1 \u0026lt;= r) { res = f(res, table[l][idx]); l += 1 \u0026lt;\u0026lt; idx; } } return res; } } private: func_type f; size_type _size; size_type _idx_size; vector\u0026lt;vector\u0026lt;T\u0026gt;\u0026gt; table; }; } // namespace st_impl  template \u0026lt;class T, T v = T{}\u0026gt; struct sum_f { static constexpr T default_value = v; static constexpr bool idempotent = false; T operator()(const T\u0026amp; a, const T\u0026amp; b) const { return a + b; } }; template \u0026lt;class T, T v\u0026gt; constexpr const T sum_f\u0026lt;T, v\u0026gt;::default_value; template \u0026lt;class T, T v = numeric_limits\u0026lt;T\u0026gt;::min(), typename = typename enable_if\u0026lt;numeric_limits\u0026lt;T\u0026gt;::is_specialized\u0026gt;::type\u0026gt; struct max_f { static constexpr T default_value = v; static constexpr bool idempotent = true; T operator()(const T\u0026amp; a, const T\u0026amp; b) const { return max(a, b); } }; template \u0026lt;class T, T v, typename R\u0026gt; constexpr const T max_f\u0026lt;T, v, R\u0026gt;::default_value; template \u0026lt;class T, T v = numeric_limits\u0026lt;T\u0026gt;::max(), typename = typename enable_if\u0026lt;numeric_limits\u0026lt;T\u0026gt;::is_specialized\u0026gt;::type\u0026gt; struct min_f { static constexpr T default_value = v; static constexpr bool idempotent = true; T operator()(const T\u0026amp; a, const T\u0026amp; b) const { return min(a, b); } }; template \u0026lt;class T, T v, typename R\u0026gt; constexpr const T min_f\u0026lt;T, v, R\u0026gt;::default_value; uint64_t gcd(uint64_t a, uint64_t b) { if (a \u0026lt; b) swap(a, b); while (b != 0) { auto t = b; b = a % b; a = t; } return a; } template \u0026lt;class T, T v = T{}, typename = typename enable_if\u0026lt;numeric_limits\u0026lt;T\u0026gt;::is_integer\u0026gt;::type\u0026gt; struct gcd_f { static constexpr T default_value = v; static constexpr bool idempotent = true; T operator()(const T\u0026amp; a, const T\u0026amp; b) const { return gcd(a, b); } }; template \u0026lt;class T, T v, typename R\u0026gt; constexpr const T gcd_f\u0026lt;T, v, R\u0026gt;::default_value; template \u0026lt;class T, class F = max_f\u0026lt;T\u0026gt;\u0026gt; using SparseTable = st_impl::SparseTable\u0026lt;T, F\u0026gt;; template \u0026lt;class F\u0026gt; void random_test(string target_func) { int n = 400; vector\u0026lt;int\u0026gt; test(n); // generate random numbers  random_device r; default_random_engine eng(r()); uniform_int_distribution\u0026lt;int\u0026gt; uniform_dist(0, 2000); for (int i = 0; i \u0026lt; n; ++i) { test[i] = uniform_dist(eng); } // query and verify  F f; SparseTable\u0026lt;int, F\u0026gt; st_test(test, f); cout \u0026lt;\u0026lt; \u0026#34;Begin random test on \u0026#34; \u0026lt;\u0026lt; target_func \u0026lt;\u0026lt; \u0026#34;!\u0026#34; \u0026lt;\u0026lt; endl; int t = 10; for (int i = 0; i \u0026lt; t; ++i) { int l = uniform_dist(eng) % n, r = l + ((uniform_dist(eng) % (n - l)) \u0026gt;\u0026gt; (i / 2)); auto to_verify = st_test.rangeQuery(l, r); auto expected = decltype(f)::default_value; for (int j = l; j \u0026lt;= r; ++j) { expected = f(expected, test[j]); } assert(to_verify == expected); cout \u0026lt;\u0026lt; \u0026#34; + query range(\u0026#34; \u0026lt;\u0026lt; l \u0026lt;\u0026lt; \u0026#34;,\u0026#34; \u0026lt;\u0026lt; r \u0026lt;\u0026lt; \u0026#34;)\\t= \u0026#34; \u0026lt;\u0026lt; to_verify \u0026lt;\u0026lt; endl; } cout \u0026lt;\u0026lt; \u0026#34;Test passed!\u0026#34; \u0026lt;\u0026lt; endl; } void regular_test() { SparseTable\u0026lt;int\u0026gt; st_max({3, 1, 2, 5, 2, 10, 8}); assert(st_max.rangeQuery(0, 2) == 3); assert(st_max.rangeQuery(3, 6) == 10); assert(st_max.rangeQuery(0, 6) == 10); assert(st_max.rangeQuery(2, 4) == 5); SparseTable\u0026lt;int, min_f\u0026lt;int\u0026gt;\u0026gt; st_min({3, 1, 2, 5, 2, 10, 8}); assert(st_min.rangeQuery(0, 2) == 1); assert(st_min.rangeQuery(3, 6) == 2); assert(st_min.rangeQuery(0, 6) == 1); assert(st_min.rangeQuery(2, 4) == 2); SparseTable\u0026lt;int, sum_f\u0026lt;int\u0026gt;\u0026gt; st_sum({3, 1, 2, 5, 2, 10, 8}); assert(st_sum.rangeQuery(0, 2) == 6); assert(st_sum.rangeQuery(3, 6) == 25); assert(st_sum.rangeQuery(0, 6) == 31); assert(st_sum.rangeQuery(2, 4) == 9); } int main() { regular_test(); random_test\u0026lt;max_f\u0026lt;int\u0026gt;\u0026gt;(\u0026#34;max\u0026#34;); random_test\u0026lt;min_f\u0026lt;int\u0026gt;\u0026gt;(\u0026#34;min\u0026#34;); random_test\u0026lt;sum_f\u0026lt;int\u0026gt;\u0026gt;(\u0026#34;sum\u0026#34;); random_test\u0026lt;gcd_f\u0026lt;int\u0026gt;\u0026gt;(\u0026#34;gcd\u0026#34;); return 0; } Selective Additions / PBS C++ #include \u0026lt;iostream\u0026gt;#include \u0026lt;cstdio\u0026gt;#include \u0026lt;cstdlib\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;vector\u0026gt;#include \u0026lt;climits\u0026gt;#include \u0026lt;utility\u0026gt;#include \u0026lt;queue\u0026gt;#include \u0026lt;map\u0026gt; using namespace std; #define defv(alias, type) using v##alias = vector\u0026lt;type\u0026gt; #define defvv(alias, type) using vv##alias = vector\u0026lt;vector\u0026lt;type\u0026gt;\u0026gt;  using ii = pair\u0026lt;int, int\u0026gt;; using iii = pair\u0026lt;int, ii\u0026gt;; defv(i, int); defvv(i, int); defv(ii, ii); defvv(ii, ii); #define forall(i, a, b) for (int i = int(a); i \u0026lt; int(b); ++i) #define all(a) a.begin(), a.end() #define inf (IMAX_NT_MAX / 2) #define sz(a) int(a.size()) #define mp(a, b) make_pair(a, b)  const int MAX_N = 1e5 + 5; long a[MAX_N], d[MAX_N]; int l[MAX_N], r[MAX_N]; long f[5]; int n, m, k; long fen[MAX_N]; int lowbit(int x) { return x \u0026amp; -x; } void fen_update(long *fen, int idx, int delta) { for (int i = idx; i \u0026lt; n; i += lowbit(i + 1)) { fen[i] += delta; } } long __fen_get(long *fen, int r) { long res = 0; while (r \u0026gt;= 0) { res += fen[r]; r -= lowbit(r + 1); } return res; } long fen_get(long *fen, int l, int r) { return __fen_get(fen, r) - __fen_get(fen, l - 1); } void fen_range_update(long *fen, int l, int r, int delta) { fen_update(fen, l, delta); fen_update(fen, r + 1, -delta); } long fen_point_get(long *fen, int i) { return __fen_get(fen, i); } void fen_reset(long *fen) { forall(i, 0, n) fen[i] = 0; } inline int fls(int x) { int r = 32; if (!x) return 0; if (!(x \u0026amp; 0xffff0000u)) { x \u0026lt;\u0026lt;= 16; r -= 16; } if (!(x \u0026amp; 0xff000000u)) { x \u0026lt;\u0026lt;= 8; r -= 8; } if (!(x \u0026amp; 0xf0000000u)) { x \u0026lt;\u0026lt;= 4; r -= 4; } if (!(x \u0026amp; 0xc0000000u)) { x \u0026lt;\u0026lt;= 2; r -= 2; } if (!(x \u0026amp; 0x80000000u)) { x \u0026lt;\u0026lt;= 1; r -= 1; } return r; } int t[MAX_N], lb[MAX_N], rb[MAX_N]; vector\u0026lt;int\u0026gt; to_check[MAX_N + 1]; void solve() { sort(f, f + k); fill_n(t, n, -1); forall(i, 0, n) forall(s, 0, k) { if (t[i] \u0026lt; 0 \u0026amp;\u0026amp; a[i] == f[s]) t[i] = 0; } // Parallel Binary Search  for (int s = 0; s \u0026lt; k; ++s) { forall(i, 0, n) if (t[i] \u0026lt; 0) { lb[i] = 1, rb[i] = m; } forall(rnd, 0, fls(m)) { fen_reset(fen); forall(i, 0, m + 1) to_check[i].clear(); forall(i, 0, n) { if (t[i] \u0026lt; 0) to_check[(lb[i] + rb[i]) \u0026gt;\u0026gt; 1].push_back(i); } forall(i, 0, m) { fen_range_update(fen, l[i], r[i], d[i]); for (int idx : to_check[i + 1]) { auto now = fen_point_get(fen, idx); if (now + a[idx] \u0026gt; f[s]) rb[idx] = i; else if (now + a[idx] \u0026lt; f[s]) lb[idx] = i + 2; else t[idx] = i + 1; } } } } // reuse fen for counting invalid updates  long sum = 0; fen_reset(fen); forall(i, 0, m + 1) to_check[i].clear(); forall(i, 0, n) { if (t[i] == 0) fen_update(fen, i, 1); else if (t[i] \u0026gt; 0) to_check[t[i]].push_back(i); sum += a[i]; } forall(i, 0, m) { sum += d[i] * (r[i] - l[i] + 1 - fen_get(fen, l[i], r[i])); cout \u0026lt;\u0026lt; sum \u0026lt;\u0026lt; endl; for (auto idx : to_check[i + 1]) { fen_update(fen, idx, 1); } } } int main() { cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; m \u0026gt;\u0026gt; k; forall(i, 0, n) { cin \u0026gt;\u0026gt; a[i]; } forall(i, 0, k) { cin \u0026gt;\u0026gt; f[i]; } forall(i, 0, m) { cin \u0026gt;\u0026gt; l[i] \u0026gt;\u0026gt; r[i] \u0026gt;\u0026gt; d[i]; l[i]--, r[i]--; } solve(); return 0; } ","date":"2017-09-10T01:06:11+08:00","permalink":"https://blog.crazyark.xyz/p/hourrank23/","title":"稀疏表和并行二分查找 (Sparse Table \u0026 Parallel Binary Search)"},{"content":"Binary Indexed Tree/Fenwick tree 的树构成方式我一直很疑惑，总是似懂非懂。现在终于弄清楚了它的节点的父子关系，记录下来防止忘记。\nBinary Indexed Tree BIT 通常用于存储前缀和，或者存储数组本身(用前缀和trick)。BIT 用一个数组来表示树，空间复杂度为 O(n)。BIT 支持两个操作，update和query，用于单点更新和前缀和查询，它们的时间复杂度都为严格的 O(lgn)。\n前缀和BIT数组里的第i项（1-based）记录了原数组区间 [i - (i \u0026amp; -i) + 1, i] 的和，其中(i \u0026amp; -i) 是 i 的最低位1表示的数。\nParent 假设我们有节点i，节点i的父节点为 i - (i \u0026amp; -i)，也就是抹除最后一个1。显然父节点唯一，且所有节点都有公共祖先0。\nSiblings 通过以下过程可以获得所有兄弟节点:\nfor (int j = 1; j \u0026lt; (i \u0026amp; -i); j \u0026lt;\u0026lt;= 1) { // z is a sibling of i  int z = i + j; } Point Get 假设我们有节点i，那么可以通过以下过程获取数组i的值：\nint res = bit[i]; int z = i - (i \u0026amp; -i); int j = i - 1; while (j != z) { res -= bit[j]; j = j - (j \u0026amp; -j); } 上述过程即把 i - 1 的末尾的连续的1一步步移除，也就是求了 sum(i - (i \u0026amp; -i), i - 1)，然后用i节点的和减去就得到了数组项i的值。\nPrefix Sum 通过以下过程不断找到父节点，并加和：\nlong sum = 0; for (int j = i; j \u0026gt; 0; j -= (j \u0026amp; -j)) { sum += bit[j]; } 显然各个区间不交，最终区间为 [1, i]。\nIntervals Cover [i, i] 通过以下过程，获得所有覆盖区间[i, i]的节点，同时它们是节点i在补码中的父节点，稍后证明：\nfor (int j = i; j \u0026lt; n; j += (j \u0026amp; -j)) { // node j covers [i, i] } 首先显然，节点i是第一个覆盖i的节点，然后 j = i + (i \u0026amp; -i) 是i之后的第一个满足 j - (j \u0026amp; -j) \u0026lt; i 的节点，证明也很容易。\n且有 j = i + (i \u0026amp; -i)，j - (j \u0026amp; -j) \u0026lt;= i - (i \u0026amp; -i) 恒成立，也就是节点j所代表的区间能够覆盖i的节点，而i到j之间的节点与节点i均不交。\n所以很容易就能推导出，所有与节点i所代表区间相交的节点为上述过程中的节点。\nParent in 2\u0026rsquo;s Complement 不妨设这里是32位的环境，大家还记得负数 -i 的二进制表示么？对，就是 2^32 - i，所以我们有 i \u0026amp; -i = i \u0026amp; (2^32 - i)，这样我们可以从有符号数转换到无符号数的操作。\n还记得上述获取下一个覆盖i的节点的操作，i + (i \u0026amp; -i)，我们用 j = 2^32 - i 来重新组织一下这个式子：\ni + (i \u0026amp; -i) = 2^32 - (j - (j \u0026amp; (2 ^32 - j))),\n我们把它也映射成补码，2^32 - i - (i \u0026amp; -i) = j - (j \u0026amp; (2 ^32 - j))\n十分熟悉了吧？就是j去掉最后一位1的操作，我们也可以把它看做一个找父节点的操作，公共祖先也是0。现在就可以清楚地看到了，其实 i + (i \u0026amp; -i) 的操作在补码中是一个对称的求父节点操作，原来的求父节点操作 i - (i \u0026amp; -i) 同理为对称操作。\nSuffix BIT 现在可以引出一个后缀和的BIT：\nconst int MAX_N = 1e5; int bit[MAX_N]; void update(int i, int delta) { while (i \u0026gt; 0) { bit[i] += delta; i -= (i \u0026amp; -i); } } // get the suffix sum of [i, n] int get(int i, int n) { int res = 0; while (i \u0026lt;= n) { res += bit[i]; i += (i \u0026amp; -i); } return res; } 本来百思不得其解的操作在补码中得到了解释，这里节点i代表的区间和为[i, i + (i \u0026amp; -i) - 1]。\n但是这里直接添加一个数字用于记录全局sum，然后 sum - prefix(1, i - 1) 也是可以的。\nAppendix Impl \u0026amp; 3 Usages #include \u0026lt;vector\u0026gt;#include \u0026lt;iostream\u0026gt;#include \u0026lt;cassert\u0026gt;using namespace std; class BinaryIndexedTree { protected: vector\u0026lt;long\u0026gt; bit; static int lowbit(int x) { return x \u0026amp; -x; } BinaryIndexedTree(BinaryIndexedTree\u0026amp;\u0026amp; other) { bit = std::move(other.bit); } public: BinaryIndexedTree(int n) { bit = vector\u0026lt;long\u0026gt;(n); } BinaryIndexedTree(const vector\u0026lt;long\u0026gt;\u0026amp; nums) { int n = nums.size(); bit = vector\u0026lt;long\u0026gt;(n); for (int i = 0; i \u0026lt; n; ++i) { bit[i] = nums[i]; for (int j = i - 1; j \u0026gt; i - lowbit(i + 1); --j) { bit[i] += nums[j]; } } } void add(int i, long delta) { for (int j = i; j \u0026lt; int(bit.size()); j += lowbit(j + 1)) { bit[j] += delta; } } long sum(int k) { long res = 0; for (int i = k; i \u0026gt;= 0; i -= lowbit(i + 1)) { res += bit[i]; } return res; } }; class PointUpdateRangeQueryExectuor { private: int n; BinaryIndexedTree tree; long prefixSum(int r) { if (r \u0026lt; 0) return 0; return tree.sum(r); } public: PointUpdateRangeQueryExectuor(int n) : n(n), tree(n) {} PointUpdateRangeQueryExectuor(const vector\u0026lt;long\u0026gt;\u0026amp; nums) : n(nums.size()), tree(nums) {} void update(int i, long delta) { assert(i \u0026gt;= 0 \u0026amp;\u0026amp; i \u0026lt; n); tree.add(i, delta); } long rangeSum(int l, int r) { assert(l \u0026lt;= r \u0026amp;\u0026amp; l \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; n); return prefixSum(r) - prefixSum(l - 1); } }; class RangeUpdatePointQueryExecutor { private: int n; BinaryIndexedTree tree; // Tear array into pieces  static vector\u0026lt;long\u0026gt; rangePieces(const vector\u0026lt;long\u0026gt;\u0026amp; nums) { int n = nums.size(); vector\u0026lt;long\u0026gt; res(n); // make sure that prefix_sum(res, i) = nums[i]  if (n != 0) res[0] = nums[0]; for (int i = 1; i \u0026lt; n; ++i) { res[i] = nums[i] - nums[i - 1]; } return res; } friend class RangeUpdateRangeQueryExecutor; public: RangeUpdatePointQueryExecutor(int n) : n(n), tree(n) {} RangeUpdatePointQueryExecutor(const vector\u0026lt;long\u0026gt;\u0026amp; nums) : n(nums.size()), tree(rangePieces(nums)) {} void update(int l, int r, long delta) { assert(l \u0026lt;= r \u0026amp;\u0026amp; l \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; n); tree.add(l, delta); if (r + 1 \u0026lt; n) tree.add(r + 1, -delta); } long get(int i) { assert(i \u0026gt;= 0 \u0026amp;\u0026amp; i \u0026lt; n); return tree.sum(i); } }; class RangeUpdateRangeQueryExecutor { private: long n; BinaryIndexedTree tree; BinaryIndexedTree tree2; static vector\u0026lt;long\u0026gt; prefixPieces(const vector\u0026lt;long\u0026gt;\u0026amp; nums) { int n = nums.size(); vector\u0026lt;long\u0026gt; res(n); // make sure that nums[i] * i - res[i] = prefix_sum(nums, i),  // so that the following prefixSum works.  // Then run rangePieces, so that we get res[i] = (nums[i] - nums[i - 1]) * (i - 1);  if (n != 0) res[0] = -nums[0]; for (long i = 0; i \u0026lt; n; ++i) { res[i] = (nums[i] - nums[i - 1]) * (i - 1); } return res; } long prefixSum(long r) { if (r \u0026lt; 0) return 0; return tree.sum(r) * r - tree2.sum(r); } static constexpr auto rangePieces = RangeUpdatePointQueryExecutor::rangePieces; public: RangeUpdateRangeQueryExecutor(long n) : n(n), tree(n), tree2(n) {} RangeUpdateRangeQueryExecutor(const vector\u0026lt;long\u0026gt;\u0026amp; nums) : n(nums.size()), tree(rangePieces(nums)), tree2(prefixPieces(nums)) {} void update(long l, long r, long delta) { assert(l \u0026lt;= r \u0026amp;\u0026amp; l \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; n); tree.add(l, delta); if (r + 1 \u0026lt; n) tree.add(r + 1, -delta); tree2.add(l, delta * (l - 1)); if (r + 1 \u0026lt; n) tree2.add(r + 1, -delta * r); } long rangeSum(long l, long r) { assert(l \u0026lt;= r \u0026amp;\u0026amp; l \u0026gt;= 0 \u0026amp;\u0026amp; r \u0026lt; n); return prefixSum(r) - prefixSum(l - 1); } }; int main() { // point update range query  PointUpdateRangeQueryExectuor purq(5); purq.update(0, 2); purq.update(3, 3); purq.update(4, 5); cout \u0026lt;\u0026lt; purq.rangeSum(0, 1) \u0026lt;\u0026lt; endl; // 2  cout \u0026lt;\u0026lt; purq.rangeSum(2, 3) \u0026lt;\u0026lt; endl; // 3  cout \u0026lt;\u0026lt; purq.rangeSum(3, 4) \u0026lt;\u0026lt; endl; // 8  PointUpdateRangeQueryExectuor purq2({2, 1, 2, 3, 5}); cout \u0026lt;\u0026lt; purq2.rangeSum(0, 1) \u0026lt;\u0026lt; endl; // 3  cout \u0026lt;\u0026lt; purq2.rangeSum(2, 3) \u0026lt;\u0026lt; endl; // 5  cout \u0026lt;\u0026lt; purq2.rangeSum(3, 4) \u0026lt;\u0026lt; endl; // 8  // range update point query  RangeUpdatePointQueryExecutor rupq(5); rupq.update(0, 4, 2); rupq.update(3, 4, 3); cout \u0026lt;\u0026lt; rupq.get(0) \u0026lt;\u0026lt; endl; // 2  cout \u0026lt;\u0026lt; rupq.get(3) \u0026lt;\u0026lt; endl; // 5  RangeUpdatePointQueryExecutor rupq2({11, 3, 2, 6, 5}); cout \u0026lt;\u0026lt; rupq2.get(0) \u0026lt;\u0026lt; endl; // 11  cout \u0026lt;\u0026lt; rupq2.get(3) \u0026lt;\u0026lt; endl; // 6  // range update range query  RangeUpdateRangeQueryExecutor rurq(5); rurq.update(0, 4, 2); rurq.update(3, 4, 3); cout \u0026lt;\u0026lt; rurq.rangeSum(2, 4) \u0026lt;\u0026lt; endl; // 12  RangeUpdateRangeQueryExecutor rurq2({2, 2, 3, 6, 5}); cout \u0026lt;\u0026lt; rurq2.rangeSum(2, 4) \u0026lt;\u0026lt; endl; // 14  return 0; } ","date":"2017-09-08T22:23:50+08:00","permalink":"https://blog.crazyark.xyz/p/binary-indexed-tree/","title":"二叉索引树/树状数组 (Binary Indexed Tree)"},{"content":"本篇为WCIPEG上关于SegmentTree的翻译稿，除了删去了几个小节，其余行文结构将完全一致。\n线段树是一种非常灵活的数据结构，它可以帮助我们高效地完成对底层数组区间查询或是修改。顾名思义，线段树可以被想象成底层数组区间构成的一棵树，它的基本思想是分治。\nMotivation 线段树的一个最常见的应用是用于解决范围最小值查询问题：给定一个数组，然后不断地查询某个给定索引范围的最小值。举个例子，给定数组 [9, 2, 6, 3, 1, 5, 0, 7]，查询第3个到第6个数之间的最小值，答案是 min(6, 3, 1, 5) = 1。接着查询第1个到第3个，答案是2\u0026hellip; 等等一系列查询操作。关于这个问题有很多文章进行了探讨，提出了诸多不同的解法。其中线段树往往是最合适的选择，特别是查询和修改穿插着进行的时候。为了简便起见，在接下来的几个小节中，我们将关注于用于回答范围最小值查询的特定线段树，不会再另行声明，而其他类型线段树将在本文后面进行讨论。\nThe divide-and-conquer solution 分而治之:\n 如果这个范围仅包含一个元素，那么这个元素自己显然就是最小值 否则，将范围二分成两个差不多大的小范围，然后找到它们相应的最小值，那么原来的最小值等于两个值之中小的那个。  故而，令 $a_i$ 表示数组内第i个元素，最小值查询可以表示为以下递归函数：\n$f(x,y) = \\begin{cases} a_x \u0026amp; \\textrm{if}\\ x = y\\\\min(f(x, \\lfloor\\frac{x+y}{2}\\rfloor), f(\\lfloor\\frac{x+y}{2}\\rfloor) + 1, y) \u0026amp; \\textrm{otherwise}\\end{cases}, x \\le y$\n因此，举个例子，上一章的第一个查询$f(3,6)$将被递归地表示为$\\min(f(3,4),f(5,6))$。\nStructure 假设我们用上面定义的函数来计算 $f(1,N)$，这里 $N$ 是数组内元素的数目。当 $N$ 很大时，这个递归计算有两个子计算，一个是 $f(1, \\lfloor\\frac{1+N}{2}\\rfloor)$，另一个是 $f(\\lfloor\\frac{1+N}{2}\\rfloor) + 1, N)$。每个子计算又有两个子计算，等等，直到遇到base case。如果我们将这个递归计算的过程表示成一棵树，$f(1, N)$就是根节点，它有两个自己点，每个子节点可能也有两个子节点，等等。Base case 就是这棵树的叶节点。现在我们可以来描述线段树的结构：\n 一棵二叉树，用于表示底层数组 每个节点代表了数组的某个区间，并且包含了区间上的某些函数值 根节点代表整个数组 (也就是区间 [1, N]) 每个叶节点表示数组中某个特定元素 每个非叶节点有两个子节点，它们的区间不交，并且它们的区间的并等于父节点的区间 每个子节点的区间大约是父节点区间的一半大小 每个非叶节点存储的值不仅是它代表的区间的函数值，而且是它的子节点的存储值的函数值（拗口\u0026hellip;就是那个递归过程）  也就是说，线段树的结构和递归计算 $f(1,N)$ 的过程完全相同。\n因此，举个例子，数组 [9,2,6,3,1,5,0,7]的根节点包含数字0 —— 整个数组的最小值。它的左子节点包含[9,2,6,3]中的最小，2；右子节点包含[1,5,0,7]中的最小，0。每个元素对应于一个叶节点，仅仅包含它自己。\n \nOperations 线段树一共有三种基础操作：构建、更新、查询。\nConstruction 为了进行查询和更新，首先我们需要构建一棵线段树来表示某个数组。我们可以自底向上或是自顶向下地构建。自顶向下的构建是一个递归的过程：尝试填充节点，如果是叶子节点，直接用对应的值填充，否则先填充该节点的两个子节点，然后用子节点中小的值填充该节点。自底向上的构建留作练习。这两种方式在运行速度上的差距几乎可以忽略不计。\nUpdate 更新线段树即更新底层数组的某个元素。我们首先更新对应的叶子节点 —— 因为叶子节点只对应数组的一个元素。被更新的节点的父节点也将被影响，因为它对应的区间包含了被修改的元素，对祖父节点同样，直到根节点，但是不会影响其他节点。如果要进行自顶向下的更新，首先进行根节点的更新，这将导致两个子节点中的相关的那个递归地更新。对子节点的更新也是同样的，边界为对叶节点的更新。当递归过程完成以后，非叶节点的值更新为两个子节点的较小值。自底向上的更新同样留作练习。\n 0 changed to 8 \nQuery 在线段树上进行查询即确定底层数组某个区间的函数值，在这里就是区间内的最小元素值。查询操作的执行过程比更新操作复杂的多，我们用一个例子来阐释。假设我们想要知道第1个到第6个元素内的最小值，我们将这个查询表示为 $f(1, 6)$。线段树上每个节点包含了某个区间内的最小值：举例来说，根节点包含了 $f(1, 8)$，它的左子节点 $f(1, 4)$，右子节点 $f(5, 8)$，等等；每个叶节点包含了 $f(x,x)$。没有一个节点是 $f(1,6)$，但是注意到 $f(1, 6) = \\min(f(1,4),f(5,6))$，并且存在两个节点包含这两个值（在下图中以黄色标识）。\n因此在线段树上查询时，我们选择所有节点的一个子集，使得它们所表示的区间的并集与我们要查询的区间完全相同。为了找到这些节点，我们首先从根节点开始，递归查询那些与区间至少有一个交集的节点。在我们的例子中，$f(1, 6)$，我们注意到左子树和右子树对应的区间都有交集，因此他们都被递归执行。左子节点是一个base case（以黄色标识），因为它的区间被查询区间整个包含。在右子节点中，我们注意到它的左子节点和查询区间相交，但是右子节点没有，所以我们只递归执行它的左子节点。而它也是一个base case，同样以黄色标识。递归终止，而最终的最小值就是这些被选中的节点的最小值。\n Query (1, 6) \nAnalysis 线段树的一些重要的性能指标如下：\nSpace 很容易证明，一个深度为d的节点对应的区间大小不超过 $\\lceil \\frac{N}{2^d} \\rceil$。我们可以看到所有深度为 $\\lceil \\lg N\\rceil$ 的节点对应于不超过一个节点，也就说它们是叶节点。因此，线段树是一棵完全平衡二叉树，它的高度是理论最小值。因此，我们可以将树存储为一个数组，节点顺序按照宽度优先遍历排序，为了方便起见，子节点按照先左后右顺序。那么，对于上述[9,2,6,3,1,5,8,7]的线段树，将被存储为[1,2,1,2,3,1,7,9,2,6,3,1,5,8,7]。假设根节点的索引是1。那么对于一个索引是i的非叶节点，它的两个子节点的索引是2i和2i+1。注意在树的叶节点层有一些空间可能是被无用的，但是通常这都是可以接受的。一棵高度为 $\\lceil \\lg N\\rceil$ 的二叉树最多有 $2^{\\lfloor\\lg N\\rfloor + 1} - 1$ 个节点，所以通过简单的数学分析我们可以知道一棵存储了N个元素的线段树需要一个不超过 $4N$ 大小的数组进行存储。一棵线段树使用了 $\\mathcal{O} (N)$ 的空间\nTime Construction 对每个节点，构建只需要进行几个固定操作。因为一棵线段树的节点数是 $\\mathcal{O} (n)$，所以构建也是线性的时间。\nUpdate 更新操作一共更新从根节点到被影响的叶节点的路径上所有的节点，对每个节点的更新操作数是固定的。节点的数量以树的高度为上界，因此同上述结论，更新的时间复杂度为 $\\mathcal{O}(\\lg N)$。\nQuery 考虑所有被选中的节点 (上节图中黄色的节点)。查询 $f(1, N - 1)$ 的情况下，有 $\\lg N$ 那么多。那么是否会有更多呢？答案是否定的。一个可能最简单的证明是，将选中节点的算法展开，将在 $\\mathcal{O}(\\lg N)$ 步内终止，也就是之前暗示过的非递归的方式。所以一个查询操作耗时 $\\mathcal{O}(\\lg N)$ 。对递归版本的证明留作练习。\nImplementation object rmq_segtree private function build_rec(node,begin,end,a[]) if begin = end A[node] = a[begin]; else let mid = floor((begin+end)/2) build_rec(2*node,begin,mid,a[]) build_rec(2*node+1,mid+1,end,a[]) A[node] = min(A[2*node],A[2*node+1]) private function update_rec(node,begin,end,pos,val) if begin = end A[node] = val else let mid=floor((begin+end)/2) if pos\u0026lt;=mid update_rec(2*node,begin,mid,pos,val) else update_rec(2*node+1,mid+1,end,pos,val) A[node] = min(A[2*node],A[2*node+1]) private function query_rec(node,t_begin,t_end,a_begin,a_end) if t_begin\u0026gt;=a_begin AND t_end\u0026lt;=a_end return A[node] else let mid = floor((t_begin+t_end)/2) let res = ∞ if mid\u0026gt;=a_begin AND t_begin\u0026lt;=a_end res = min(res,query_rec(2*node,t_begin,mid,a_begin,a_end)) if t_end\u0026gt;=a_begin AND mid+1\u0026lt;=a_end res = min(res,query_rec(2*node+1,mid+1,t_end,a_begin,a_end)) return res function construct(size,a[1..size]) let N = size let A be an array that can hold at least 4N elements build_rec(1,1,N,a) function update(pos,val) update_rec(1,1,N,pos,val) function query(begin,end) return query_rec(1,1,N,begin,end) Variations 线段树不仅适用于区间最小值查询，它还可以适用于许多不同的函数查询。这里有一些比较难的竞赛题中的例子。\nMaximum 和最小类似：所有min操作替换为max。\nSum or product 每个非叶节点的值为它的子节点的和，也就是表示了它的区间内的数组和。举例来说，在上述伪代码中，所有min操作被替换成加操作。但是，在一些加和的情况中，线段树被二叉索引树(Binary Indexed Tree)取代，因为BIT空间占用更小，运行更快，并且容易编码。乘可以以同样的方式实现，只不过是用乘取代加。\nMaximum/minimum prefix suffix sum 一个区间前缀由区间内前k个元素组成 (k可以为0)；类似的，后缀由后k个元素组成。区间最大前缀和表示区间内所有前缀的和中最大的数(空区间的和为0)。这个最大和被称为最大前缀和（最小前缀和可以类似的定义）。我们希望能够高效地查询某个区间的最大前缀和。举个例子，[1,-2,3,-4]的前缀和是2，对应的前缀是[1,-2,3]。\n为了使用线段树解决这个问题，我们用每个节点存储对应区间的两个函数值：最大前缀和和区间和。一个非叶节点的区间和是它的两个子节点区间和的和。为了找到一个非叶节点的最大前缀和，我们注意到最大前缀和对应的前缀的最后一个元素要么在左子节点对应的区间内，要么在右子节点对应的区间内。前一种情况，我们可以直接从左子节点获取最大前缀和，而后一种情况，我们通过左子节点的区间和和右子节点的最大前缀和相加获得。这两个数中的较大这就是我们的最大前缀和。查询也是类似的，但是可能会有超过两个相邻的区间（这种情况下最大前缀和的最后一个元素可能在它们中的任何一个）。\nMaximum/minimum subvector sum 这个问题是查询给定区间内所有子区间的最大和。和上一节最大前缀和的问题类似，但是子区间的第一个元素并不一定在区间的开始。([1,-2,3,-4]最大子区间和是3，对应的子区间是[3])。每个节点需要存储4份信息：最大前缀和，最大后缀和，最大子区间和，区间和。详细设计留作习题。\n\u0026ldquo;Stabbing query\u0026rdquo; For more details, please visit the original page. 🍺\nExtension to two or more dimensions 线段树并不只限于解决一维数组的问题。原则上来说，它可以被用于任意纬度的数组，区间将被箱子答题。因此，在一个二维数组中，我们可以查询一个箱子内的最小元素，或者两个区间的笛卡尔乘积。\nFor more details, please visit the original page. 🍺\nLazy propagation 某些类型的线段树支持区间更新。举个例子，考虑一个区间最小值问题的变种：我们需要能够更新一个区间内所有的元素到某个特定的值。这被称为区间更新。懒传播是一种可以使得区间更新和单个元素更新一样能够在 $\\mathcal{O}(\\lg N)$ 时间内完成的技术。\n它的工作原理如下：每个节点额外拥有一个lazy域，用于临时存储。当这个域不被使用是，令它的值为 $+\\infty$。当更新一个区间是，我们选中和查询一样的那些区间。如果区间不是叶节点，则更新它们的lazy域到新的最小值(如果新的最小值比原来小的话)，否则直接将值更新到节点上。当查询或是更新操作需要访问一个节点的后代，并且这个节点的lazy域有值，我们将lazy域内的值更新到它的两个子节点上：用父节点lazy域内的值更新到两个子节点的lazy域上，然后将父节点的lazy域重新设置为 $+\\infty$。但是，如果我们只是想要该节点的值，并不访问它的任何子节点，并且lazy域有值，我们可以直接返回lazy内的值当做是区间内最小值。\nReference [1] https://wcipeg.com/wiki/Segment_tree\nApendix C++ 最大查询、区间更新、懒传播 class SegmentTree { private: int n; vector\u0026lt;int\u0026gt; max_val, to_add; void push(int i, int tl, int tr) { max_val[i] += to_add[i]; if (tl != tr - 1) { to_add[2 * i + 1] += to_add[i]; to_add[2 * i + 2] += to_add[i]; } to_add[i] = 0; } void add(int i, int tl, int tr, int l, int r, int delta) { push(i, tl, tr); if (tl \u0026gt;= r || tr \u0026lt;= l) { return; } if (l \u0026lt;= tl \u0026amp;\u0026amp; tr \u0026lt;= r) { to_add[i] += delta; push(i, tl, tr); return; } int tm = (tl + tr) / 2; add(2 * i + 1, tl, tm, l, r, delta); add(2 * i + 2, tm, tr, l, r, delta); max_val[i] = max(max_val[2 * i + 1], max_val[2 * i + 2]); } int get(int i, int tl, int tr, int l, int r) { push(i, tl, tr); if (tl \u0026gt;= r || tr \u0026lt;= l) { return 0; } if (l \u0026lt;= tl \u0026amp;\u0026amp; tr \u0026lt;= r) { return max_val[i]; } int tm = (tl + tr) / 2; return max(get(2 * i + 1, tl, tm, l, r), get(2 * i + 2, tm, tr, l, r)); } public: SegmentTree(int k) { n = 1; while (n \u0026lt; k) { n *= 2; } max_val = vector\u0026lt;int\u0026gt;(2 * n, 0); to_add = vector\u0026lt;int\u0026gt;(2 * n, 0); } void add(int l, int r, int delta) { add(0, 0, n, l, r, delta); } int get(int l, int r) { return get(0, 0, n, l, r); } }; ","date":"2017-09-08T12:58:11+08:00","permalink":"https://blog.crazyark.xyz/p/segment-tree/","title":"线段树 (Segment Tree)"},{"content":"原题目在 AtCoder Grand Contest 019，F - Yes or No。\n把它改成数学题，题目大意如下：\n 假设你有M+N个问题要回答，每个问题的回答不是Yes就是No。你知道其中有N个Yes，M个No，但是并不知道顺序。你将按顺序一个一个回答问题，并且答完一道题后立刻就能知道这道题的正确答案。 假设你每次回答问题都采取最大化期望正确题目数的方式，请问期望正确题目数是多少？\n 正确答案 令 $E(M, N)$ 表示已知(M, N)x(Yes, No)的期望正确题目数。\n则\n$E(M, N) = E(N, M)$，且\n$E(M, N) = \\dfrac{\\sum\\limits_{k = 1}^{k = N}\\binom{2k}{k}\\binom{M + N - 2k}{N - k}}{2\\binom{M + N}{N}} + M, M \\ge N$\n归纳证明 显然，所谓最大化期望的方式就是选当前答案多的那个，所以有\n  $E(M, N) = \\dfrac{M}{M + N}\\left(E(M - 1, N) + 1\\right) + \\dfrac{N}{M + N}E(M, N - 1), M \\ge N$\n  $E(M, 0) = M$\n  由两条及对称性 $E(M, N) = E(N, M)$ 可以推出任何 $E(M, N)$。\n令 $F(M, N) = E(M, N) - M, M \\ge N$，以及 $F(M, N) = F(N, M)$\n我们有\n $F(M, N) = \\dfrac{M}{M + N}F(M - 1, N) + \\dfrac{N}{M + N}F(M, N - 1), M \\gt N$ $F(N, N) = F(N, N - 1) + \\dfrac{1}{2}$  令 $G(M, N) = \\binom{M + N}{N}F(M, N), M \\ge N$，同样有 $G(M, N) = G(N, M)$\n同时下两式成立\n $G(M, N) = G(M - 1, N) + G(M, N - 1), M \\gt N$ $G(N, N) = \\dfrac{1}{2}\\binom{2N}{N} + 2G(N, N-1)$  所以我们只需要验证 $G(M, N) = \\dfrac{\\sum\\limits_{k = 1}^{k = N}\\binom{2k}{k}\\binom{M + N - 2k}{N - k}}{2} = \\sum\\limits_{k = 1}^{k = N}\\binom{2k - 1}{k}\\binom{M + N - 2k}{N - k}, M\\ge N$ 即可。\n首先显然 $G(M, 0) = 0$ 成立。\n令 $M = N = 1$，有$G(1, 1) = 1 + 2G(1, 0) = 1$，且 $G(M, 1) = G(M - 1, 1) + G(M, 0), M \\gt 1$，所以 $G(M, 1) = 1, M \\ge 1$ 成立。\n假设对 $\\forall n.n \\le N - 1, \\forall m.m\\ge n$, 都有 $G(m, n) = \\sum\\limits_{k = 1}^{k = n}\\binom{2k - 1}{k}\\binom{m + n - 2k}{n - k}$, 则有\n$G(N, N) = \\binom{2N - 1}{N - 1} + 2\\sum\\limits_{k = 1}^{k = N - 1}\\binom{2k - 1}{k}\\binom{2N - 1 - 2k}{N - k}$ $ = \\binom{2N - 1}{N - 1} + \\sum\\limits_{k = 1}^{k = N - 1}\\binom{2k - 1}{k}\\binom{2N - 2k}{N - 1 - k}$ $=\\sum\\limits_{k = 1}^{k = N}\\binom{2k - 1}{k}\\binom{2N - 2k}{N - k}$\n假设 $\\forall m. m \\le M, m \\ge n$, 上式同样成立，则有\n$G(M + 1, N) = G(M, N) + G(M + 1, N - 1)$ $= \\sum\\limits_{k = 1}^{k = N}\\binom{2k - 1}{k}\\binom{M + N - 2k}{N - k} + \\sum\\limits_{k = 1}^{k = N - 1}\\binom{2k - 1}{k}\\binom{M + N - 2k}{N - 1- k}$ $ = \\sum\\limits_{k = 1}^{k = N - 1}\\binom{2k - 1}{k}\\left(\\binom{M + N - 2k}{N - k} + \\binom{M + N- 2k}{N - 1- k}\\right) + \\binom{2N-1}{N}$ $ = \\sum\\limits_{k = 1}^{k = N - 1}\\binom{2k - 1}{k}\\binom{M + N + 1 - 2k}{N - k}+ \\binom{2N-1}{N} = \\sum\\limits_{k = 1}^{k = N}\\binom{2k - 1}{k}\\binom{M + N + 1 - 2k}{N - k}$\n所以 $\\forall n.n\\le N, \\forall m.m\\ge n$，上式成立。\n所以 $\\forall m, \\forall n.m \\ge n$，上式成立。\n证毕。\n头疼的问题 看这个解的形式，感觉上去像是可以有构造解，但是我绞尽脑汁也想不出来。而且更关键的是，在不知道解形式的情况下，根本不能完成上述归纳证明\u0026hellip;\n我想我得求助下老朋友\u0026hellip;或者发个邮件问下解题的大佬了\u0026hellip;\n构造解 结果刚洗完澡就研究出来了\u0026hellip;难道是传说中的洗澡解题法？？？\n问题映射 假设我们有一个MxN的矩形格子，我们把格子放置在坐标系第一象限，格子左下角在(0, 0)，右上角在(M, N)。\n那么，一共存在 $\\binom{M + N}{N}$ 条不同的路径能够连通左下角和右上角，(从左下到右上为M次向右和N次向上的排列)。\n下述所有路径构造过程均从右上角(M,N)至左下角(0,0)，且 M \u0026gt;= N，提前声明。\n下面定义一条路径到答题方式的映射：\n 当处于格点 (x, y) 时，表明当前还剩 x + y 题，其中 x 题为Yes，y题为No 当处于格点 (x, y) 时  如果 x \u0026gt;= y，那么路径向左(x - 1, y)表明(答Yes，正确)，向下(x, y - 1)表明(答Yes，错误) 如果 x \u0026lt; y, 则向左(x - 1, y)表明(答No，错误)，向下表明(答No，正确)    显然，通过上述表示映射，一条路径唯一地确定出题和答题方式，反之亦然。\n那么原问题中正确答案的数目怎么计算呢？可以看到，路径经过格点(x, y)时，正确或是错误是由路径走向唯一确定的，也就是由边唯一确定的\n 所有 x \u0026gt;= y, (x, y)-(x - 1, y) 的边计数为1，(x, y)-(x, y - 1)的边计数为0 所有 x \u0026lt; y, (x, y)-(x - 1, y) 的边计数为0，(x, y)-(x, y - 1)为1  我们将计数标到格子的所有边上，那么我们正确答案的计数就等于所有路径上所经过的边权和。\n问题求解 这里我不画图，大家还是在稿纸上画一下。\n令 $w(x, y, 1)$ 代表(x,y)-(x-1,y)边上的权，$w(x, y, 0)$代表(x, y-1)边上的权，那么\n $x \\ne y, w(x, y, 1) = w(y, x, 0)$ $x = y, w(x, y, 1) = 1, w(x, y, 0) = 0$  这个脑补出来就是除了(x, x)格点出来的边，其余边都关于$y = x$对称。\n假设任意一条路径S，其在格子内与y=x对称得到的路径为S' (可以对称的部分对称)。\n令$w(S)$表示S上所有边的加权和，可以证明：\n$w(S) + w(S')= 2M + |S \\cap {(x, y) | y = x}|$\n证明如下：\n显然，假设 S 与 $y = x$ 第一次相交于 (X, X)，那么到达 (X, X)之前权和总共为 (M - X)；\n从 (X, X) 开始到 (0, 0)，两条路径 S + S' 的加权和为 $2X + |S \\cap {(x, y) | y = x}|$：\n因为假设 $w(x,x,1)=w(x,x,0)=0$，那么边关于$y=x$就完全对称，我们将S所有路径全部翻到$y=x$之上不影响路径权和，则向下永远为1，向左永远为0，权和为X。而 S + S' 中 $w(x,x,1)$和$w(x,x,0)$ 都只能获得一次，所以上式成立。\n所以对于所有路径，边权和为 $W = \\sum\\limits_{S}w(S) = \\sum\\limits_{S}\\dfrac{2M + |S \\cap {(x, y) | y = x}|}{2}$，也就是\n$W = M\\binom{M + N}{N} + \\frac{\\sum\\limits_{S}{|S \\cap {(x, y) | y = x}|}}{2}$，我们只需要再计算每个(x,x)点被被多少条路径经过，再加和就行。\n对于一个点(x,y)，经过的路径数为$\\binom{x + y}{x}\\binom{M + N - x - y}{N - y}$，超级显然\u0026hellip;\n所以，$W = M\\binom{M + N}{N} + \\frac{\\sum\\limits_{k = 1}^{N}\\binom{2k}{k}\\binom{M + N - 2k}{N - k}}{2}$。\n所以期望为 $\\dfrac{W}{\\binom{M + N}{N}} = M + \\frac{\\sum\\limits_{k = 1}^{N}\\binom{2k}{k}\\binom{M + N - 2k}{N - k}}{2\\binom{M + N}{N}}$，终于证毕。\n在证明过程中可以发现一个很有意思的事实，在回答问题的过程中，如果当前剩余的M=N，直接放弃答题并查看答案，那么答对的题目数永远是M。\n总结 脑子不好使的时候，我选择洗澡🛀（误\n这题的解法构造十分巧妙，没图解释/理解起来会很累，大家看的时候还是自己画一画图比较好。\n我花了三天时间才想出来，期间还看了editorial，提示了我格子构造法\u0026hellip;如果答题没什么时间想，我大概就直接O(MN)的dp了\u0026hellip;\n附录 小结论 这次研究题目还有一个小结论，假设 $p$ 是质数，$a, b, c, d$ 与 $p$ 均互质，且存在 $x, y \\lt p$，使得 $a \\equiv bx \\ (\\textrm{mod} \\ p)$，$c \\equiv dy \\ (\\textrm{mod} \\ p)$。\n那么 $ad + bc \\equiv (x + y)bd \\ (\\textrm{mod} \\ p)$，证明如下\n$ad + bc \\equiv xbd^pb^{p - 1} + ydb^pd^{p - 1} \\ (\\textrm{mod} \\ p) \\equiv (bd)^p(x + y) \\ (\\textrm{mod} \\ p) \\equiv bd(x + y) \\ (\\textrm{mod} \\ p)$，证毕。\n用于分数 $\\frac{a}{b}$ 对于 $p$ 的模数 $ab^{-1} \\ (\\textrm{mod} \\ p)$ 的加和。\n","date":"2017-09-06T21:16:06+08:00","permalink":"https://blog.crazyark.xyz/p/agc019-mysterious-combinators/","title":"神奇的组合数 (AGC019-F Mysterious Combinators)"},{"content":"最长递增子序列算法，原本以为已经记住了最快的算法，看来是记性太差，今天碰到一道题目又忘记了怎么做 🙄\n三种做法：DP，Sort + LCS，DP + BS，我只记得第一种DP了 \u0026hellip;\n然后咱们顺便把某道题目做了 🤣\nAlgorithms for LIS DP 很显然，以第i个元素为结尾的最长递增子序列的长度是 max{dp[j] + 1}, j \u0026lt; i \u0026amp;\u0026amp; arr[j] \u0026lt; arr[i]，所以DP空间复杂度 O(n)，时间复杂度 O(n^2)。\nint LIS(vector\u0026lt;int\u0026gt; nums) { int n = nums.size(); int dp[n]; int res = 0; for (int i = 0; i \u0026lt; n; ++ i) { dp[n] = 1; for (int j = 0; j \u0026lt; i; ++ j) { if (nums[j] \u0026lt; nums[i]) dp[i] = max(dp[i], dp[j] + 1); } res = max(res, dp[i]); } return res; } Sort + LCS 原数组的递增子序列一定是排序后数组和原来数组的公共子序列，所以排完序找最长公共子序列就行了。\n空间复杂度 O(n^2)，时间复杂度 O(n^2)。\nint LCS(vector\u0026lt;int\u0026gt; a, vector\u0026lt;int\u0026gt; b) { assert(a.size() == b.size()); int n = a.size(); int dp[n + 1][n + 1]; for (int i = 0; i \u0026lt; n; ++ i) { for (int j = 0; j \u0026lt; n; ++ j) { if (i == 0 || j == 0) dp[i][j] = 0; else if (a[i - 1] == b[j - 1]) dp[i][j] = dp[i - 1][j - 1] + 1; else dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]); } } return dp[n][n]; } int LIS(vector\u0026lt;int\u0026gt; nums) { auto sorted = nums; sort(sorted.begin(), sorted.end()); return LCS(nums, sorted); } DP + BS 这是最快的一个算法，也是我到现在都还没记住的算法 🙄，关键思想为维护一个数组，数组中第l项(1-based) 为长度为l的递增子序列的最大(最后)元素的最小值，然后不断维护这个数组直到结束，最终数组的有效长度即为LIS的结果。\n由于第l项为长度为l的递增子序列的最大元素的最小值，所以该数组一定为一个递增数组。当下一个数n需要能够成为某个递增子序列的结尾时，我们在该数组中查找第一个不比n小的数，假设位置为l，那么我们就知道我们能以n结尾构建一个最长为l的递增子序列 (想想为什么？很简单)，此时我们就将l位置的数组值更新为n。有一个特殊情况即数组中不存在位置l，那么就在数组最后添加n。\n这里查找可以使用二分查找的方式，从而空间复杂度为 O(n)，时间复杂度为 O(nlgn)。\nint LIS(vector\u0026lt;int\u0026gt; nums) { vector\u0026lt;int\u0026gt; dp; for (auto n : nums) { auto it = lower_bound(dp.begin(), dp.end(), n); if (it == dp.end()) dp.push_back(n); else *it = n; } return dp.size(); } 其实我发现\u0026hellip;，我都不用手写BS，STL还是解决了很多问题的，除了没有Fibonacci Heap之外感觉好像也没啥缺点了 (笑\nAtCoder Grand Contest 019C 题目：\n In the city of Nevermore, there are 10e8 streets and 10e8 avenues, both numbered from 0 to 10e8 − 1. All streets run straight from west to east, and all avenues run straight from south to north. The distance between neighboring streets and between neighboring avenues is exactly 100 meters. Every street intersects every avenue. Every intersection can be described by pair (x,y), where x is avenue ID and y is street ID.\n  There are N fountains in the city, situated at intersections (Xi,Yi). Unlike normal intersections, there\u0026rsquo;s a circle with radius 10 meters centered at the intersection, and there are no road parts inside this circle. The picture below shows an example of how a part of the city with roads and fountains may look like.\n  city \n约束:\n  0≤x1,y1,x2,y2\u0026lt;10e8    1≤N≤200,000 0≤Xi,Yi\u0026lt;10e8 Xi≠Xj for i≠j Yi≠Yj for i≠j Intersections (x1,y1) and (x2,y2) are different and don\u0026rsquo;t contain fountains. All input values are integers.  输入:\n x1 y1 x2 y2 N X1 Y1 X2 Y2 : XN YN\n 输出:\n Print the shortest possible distance one needs to cover in order to get from intersection (x1,y1) to intersection (x2,y2), in meters. Your answer will be considered correct if its absolute or relative error doesn\u0026rsquo;t exceed 10e−11.\n Samples:\n I: 1 1 6 5 3 3 2 5 3 2 4\n  O: 891.415926535897938\n  I: 3 5 6 4 3 3 2 5 3 2 4\n  O: 400.000000000000000\n  I: 4 2 2 2 3 3 2 5 3 2 4\n  O: 211.415926535897938\n Ok，一个网格给定两个点，那么不考虑喷泉的走法是固定的，长度为 abs(x1 - x2) + abs(y1 - y2)。\n由对称性，我们不妨设 x1 \u0026lt;= x2, y1 \u0026lt;= y2，考虑喷泉的情况下，从喷泉处转角能够少走一些路，而直走经过喷泉会多走一些路。\n假设我们的走法是绕远路贪转角少走，或是避免直走，即走出{(x1, y1), (x2, y2)}所构成的矩形的情况，由于每行每列仅有一个喷泉，所以多远离矩形一格会导致多走200m，而最多只能少走 (2 * r - pi * r / 2) 大约为4.5m的距离，或者少走(pi * r - 2 * r) 大约为 11.4m 的距离，所以显然是不可能的。\n所以走法一定是在{(x1, y1), (x2, y2)}矩形内部，贪最多的转角。\n显然，在 (x1, y1) 到 (x2, y2) 的路程中，最多只有 c = min(x2 - x1, y2 - y1) 次转角，最多有 c + 1 个喷泉。\n假设喷泉是按照x的大小进行排列的，那么我们能过的最多的温泉等于温泉y值数组的最长递增子序列，假设长度为k。\n那么有两种情况，\n k \u0026lt;= c, 那么就一定有一种走法，使得我们经过这个子序列中每个温泉的时候都是转角 k == c + 1，那么一定存在一个喷泉需要经过，所以走法是走c个角，并经过剩余的一个。  所以关键即求最长递增子序列，放代码\n#include \u0026lt;bits/stdc++.h\u0026gt; using namespace std; pair\u0026lt;long long, long long\u0026gt; p[200000]; const double PI = 3.14159265358979323846; const double R = 10.0; const double quarter_circle = PI * R / 2; const double edge = 100.0; using PLL = pair\u0026lt;long long, long long\u0026gt;; int dp[200000]; int main() { long long x1, y1, x2, y2; scanf(\u0026#34;%lld %lld %lld %lld\u0026#34;, \u0026amp;x1, \u0026amp;y1, \u0026amp;x2, \u0026amp;y2); if (x1 \u0026gt; x2) { swap(x1, x2); swap(y1, y2); } long long y_min = min(y1, y2), y_max = max(y1, y2); // drop those not used  int n; scanf(\u0026#34;%d\u0026#34;, \u0026amp;n); for (int i = 0; i \u0026lt; n;) { scanf(\u0026#34;%lld %lld\u0026#34;, \u0026amp;p[i].first, \u0026amp;p[i].second); if (p[i].first \u0026lt; x1 || p[i].first \u0026gt; x2 || p[i].second \u0026lt; y_min || p[i].second \u0026gt; y_max) { n--; } else { ++i; } } // x1 \u0026lt;= x2  double res = edge * (x2 - x1 + y_max - y_min); sort(p, p + n, [\u0026amp;](const PLL\u0026amp; a, const PLL\u0026amp; b) { return a.first \u0026lt; b.first; }); if (y1 \u0026gt; y2) { for (int i = 0; i \u0026lt; n; ++i) { p[i].second = -p[i].second; } } int len = 0; for (int i = 0; i \u0026lt; n; ++i) { auto it = lower_bound(dp, dp + len, p[i].second); *it = p[i].second; if (it == dp + len) { len ++; } } int max_corner = min(x2 - x1, y_max - y_min); if (len \u0026lt; max_corner + 1) res += len * (quarter_circle - 2 * R); else res += max_corner * (quarter_circle - 2 * R) + (2 * quarter_circle - 2 * R); printf(\u0026#34;%.15f\\n\u0026#34;, res); return 0; } 总结 一天不刷题就会忘记算法，还有 atcoder 的题还真是难\u0026hellip;\n","date":"2017-09-01T15:41:22+08:00","permalink":"https://blog.crazyark.xyz/p/longest-increasing-subsequence/","title":"最长递增子序列 (Longest Increasing Subsequence)"},{"content":"第一次遇到了除0以外的SIGFPE，记录一下。\n症状 在使用以下函数的时候，假定 a = 1e18, b = 1e18, m = 1e9 + 7 就会触发 SIGFPE。\nlong mulmod(long a, long b, long m) { long res; asm(\u0026#34;mulq %2; divq %3\u0026#34; : \u0026#34;=d\u0026#34;(res), \u0026#34;+a\u0026#34;(a) : \u0026#34;S\u0026#34;(b), \u0026#34;c\u0026#34;(m)); return res; } 原因 主要是由于 divq S 的执行逻辑是用 128bit 的 %rdx:%rax 除以 S，将商存入 %rax, 余数存入 %rdx，而上面的情况 a * b / m 太大，超过了 64bit，所以 %rax 存不下就触发了 SIGFPE。\n解决方案 先模再模乘。\n","date":"2017-08-27T17:38:37+08:00","permalink":"https://blog.crazyark.xyz/p/cpp-sigfpe/","title":"SIGFPE When Doing DivQ"},{"content":"接上次的博文，我们来解决大整数分解问题，并最终解决 Project Euler #188。\n回忆一下，问题要求解的是 $a\\uparrow\\uparrow b \\ (\\textrm{mod} \\ m)$，其中 $1 \\le a, b, m \\le 10^{18}$。\n其实这里的整数在整数分解领域并不算太大，之前并没有学习过这类的算法，正好也算是补上了。在这里我使用了 Pollard Rho 算法，其他的算法还有 Fermat Rho 和 Quadratic Sieve 算法。\nPollard Rho 算法伪代码 x ← 2; y ← 2; d ← 1 while d = 1: x ← g(x) y ← g(g(y)) d ← gcd(|x - y|, n) if d = n: return failure else: return d 核心思想 假设要分解的整数 $n = pq$，其中 $p, q$ 都是质数，不妨设 $p \\le q$。我们使用 $g(x) = (x^2 + 1) \\ (\\textrm{mod} \\ n)$ 来生成一个伪随机数序列。\n不妨设初始的x为2，那么我们有一个序列为 ${x_1 = g(2), x_2 = g(g(2)), \u0026hellip; x_k = g^k(2),\u0026hellip;}$，因为序列中的值一定是有穷的，并且序列的每一个数只依赖于前一个数，所以该序列一定会循环。\n定义 ${y_k = x_k\\ (\\textrm{mod} \\ p), k = 1, 2, \u0026hellip;}$ 序列，那么这个序列也一定会循环，并且由于 $p$ 比 $n$ 小得多，所以序列循环也一定会比 ${x_k}$ 序列循环得早许多。\n那么我们只需要检测到这个环，假设环长为r，他一定有$y_{r+k} - y_{k} \\equiv 0 \\ (\\textrm{mod} \\ p)$，所以使用 Floyd Cycle Detection ，并使用最小公约数来检测环的出现 (最小公约数不为1)，此时如果最小公约数为不等于n，那么它一定是p或者q。\n唯一需要注意的是环出现的时候可能最小公约数为n，也就是上述算法中x等于y。此时我们随机更换序列种子x，并进行下一轮分解。\n时间复杂度 期望运行时间正比于n的最小素数的开根，此处大约为 O(3.2e4)。\n#188 整体思路 定义 $T(a, b) = a\\uparrow\\uparrow b$。\n我们有 $T(a, b) = a^{T(a, b - 1)}$。\n由欧拉定理的扩展，如果 $T(a, b - 1) \u0026gt;= \\phi(m)$，则有 $T(a,b) \\equiv a^{T(a, b - 1) \\ %\\ \\phi(m) + \\phi(m)} \\ (\\textrm{mod} \\ m)$，否则有 $T(a,b) \\equiv a^{T(a, b - 1) \\ %\\ \\phi(m)} \\ (\\textrm{mod} \\ m)$，此时需要求解 $T(a, b - 1) \\ (\\textrm{mod}\\ \\phi(m))$。\n同理，$T(a, b - 1) \\equiv a^{T(a, b - 2) \\ %\\ \\phi(\\phi(m))\\ ?+\\ \\phi(\\phi(m))} \\ (\\textrm{mod} \\ \\phi(m))$，此时求解 $T(a, b - 2) \\ (\\textrm{mod}\\ \\phi(\\phi(m)))$，我们可以一直递归下去。\n直到求解 $T(a, 1)\\ (\\textrm{mod} \\ \\phi^{b - 1}(m))$，或者 $T(a, b') \\ (\\textrm{mod} \\ 1), \\phi^{b - b'}(m) = 1$ 为止。\n递归深度 递归深度顶多为 min(128, b - 1)，证明如下：\n递归结束条件有两个，一个是 b 等于 1，或者欧拉函数为1。\n我们证明 $m \\ge 2$ 时，$\\phi(\\phi(m)) \\le m/2$ 即可，由 $\\phi(m)$ 定义, $1 \\le \\phi(m) \\le m$ 恒成立。\n分以下两种情况：\n m 为偶数，则由定义 $\\phi(m) \\le m / 2$，$\\phi(\\phi(m)) \\le \\phi(m) \\le m / 2$ m 为奇数，则由定义$\\phi(m)$一定为偶数，$\\phi(\\phi(m)) \\le \\phi(m) / 2 \\le m / 2$  证毕。\n所以如果 $\\phi^{s}(m) = 1, m \u0026lt; 2^{64}$，那么 $s \\le 64 \\times 2 = 128$ 恒成立，所以递归深度最大不会超过 128。\n扩展欧拉定理的应用 扩展形式由于有一个大小比较的条件，所以需要估算当前栈迭代次幂的大小，而迭代次幂 (tetration) 增长十分快，所以值在 (2^63 - 1) 以内的a和b甚至可以枚举出来：\n   a b T(a, b)     1 any 1   any 1 a   2 2 4   2 3 16   2 4 65536   3 2 27   3 3 7625597484987   4 2 64   \u0026hellip; \u0026hellip; \u0026hellip;   15 2 437893890380859375    这些数我们可以轻易的计算出来，与模数进行比较，而其余的一定大于模数。\n大整数分解 由于 $m$ 的范围，我先打表 1e6 内所有的素数，并先将 m 中这样的质因子全部分解，假设剩余的数为 $m'$，那么 $m'$ 如果是合数，一定是两个质数 $p, q \\gt 1000000$ 的乘积。\n如果 $m' \\le 1e12$，那么 $m'$ 一定是素数，否则我们先对 $m'$ 进行素数测试 (Miller-Rabin)，如果为合数再使用 Polland-Rho 进行分解，一旦分解一定为两个素数。\n64位模乘trick Gcc/Clang 均提供内置类型 __int128_t，可以支持128位的运算，而如果运行在 intel x86_64 架构上，可以直接使用如下汇编\nlong mulmod(long a, long b, long m) { long res; asm(\u0026#34;mulq %2; divq %3\u0026#34; : \u0026#34;=d\u0026#34;(res), \u0026#34;+a\u0026#34;(a) : \u0026#34;S\u0026#34;(b), \u0026#34;c\u0026#34;(m)); return res; } 算法流程 $T(a, b, m)$:\n 如果 a == 1 或者 b == 1，返回 a % m 如果 m == 1，返回 0 对m进行质因数分解，并计算欧拉函数 $\\phi(m)$ 递归计算 $e = T(a, b - 1, \\phi(m))$ 估算 $T(a, b - 1)$ 并与 $\\phi(m)$ 比较大小，如果 $T(a, b - 1) \\ge \\phi(m)$，$e = e + \\phi(m)$ 计算 $a ^ e % m$  整个流程保证了所有操作数都在64位以内，递归深度最大为 min(128, b - 1)。\n算法代码托管在 https://github.com/arkbriar/hackerrank-projecteuler/blob/master/cpp/188.cc 。\nReferences [1] https://en.wikipedia.org/wiki/Pollard%27s_rho_algorithm\n","date":"2017-08-25T21:32:26+08:00","permalink":"https://blog.crazyark.xyz/p/project-euler-188/","title":"整数的超幂 (Project Euler #188 -- The Hyperexponentiation of A Number)"},{"content":"刚遇到一道可怕的题目，迭代次幂(tetration)在超级大的范围下快速求解对某个模数的幂，模数范围在 1 到 1e18 之间。\nOK，这道题其实思路很清晰，用欧拉定理降幂，但是最难的部分在于\n 对 m 进行质因数分解 使用欧拉定理在非互质情况的扩展形式  我们先把m的质因数分解放一放 (其实还没解决)\u0026hellip; 先来解决第二个问题。\nEuler Theorem \u0026amp; Fermat\u0026rsquo;s Little/Last Theorem 设m是大于1的整数，$(a, m) = 1$，则\n$$a^{\\phi(m)} \\equiv 1 \\ (\\textrm{mod}\\ m)$$\n这里 $\\phi(m)$ 是欧拉函数，如果 $m = p_1^{k_1}p_2^{k_2}\\cdots p_s^{k_s}$，$\\phi(m) = m(1 - \\frac{1}{p_1})\\cdots (1 - \\frac{1}{p_s})$。\n这里引入简化剩余系的概念，对整数m简化剩余系为一个集合 $R(m) = {r | 0 \\le r \\lt m, (r, m) = 1}$，也就是所有m的模数里与m互质的数的集合。\n显然 $|R(m)| = \\phi(m)$。\nProven for Euler Theorem 这个证明应该很常见了，我还是写一下，也很简单，对于整数a和m，$(a, m) = 1$，m的简化剩余系 $R(m) = {r_1, r_2, \u0026hellip;, r_{\\phi(m)}}$\n$(ar_1)(ar_2)\\cdots(ar_{\\phi(m)}) \\equiv a^{\\phi(m)}(r_1r_2\\cdots r_{\\phi(m)})\\ (\\textrm{mod}\\ m)$\n而因为 $(a, m) = 1$, 显然有\n $ar_i \\not\\equiv 0 \\ (\\textrm{mod} \\ m)$ $ar_i \\not\\equiv ar_j \\ (\\textrm{mod} \\ m), i \\ne j$  因此 $b_i = (ar_i\\ \\textrm{mod} \\ m)$ 同样构成了m的简化剩余系。\n所以 $\\prod_{i = 1}^{i = \\phi(m)} b_i \\equiv \\prod_{i = 1}^{i = \\phi(m)} r_i \\ (\\textrm{mod} \\ m)$\n所以 $\\prod_{i = 1}^{i = \\phi(m)} r_i \\equiv a^{\\phi(m)}\\prod_{i = 1}^{i = \\phi(m)} r_i \\ (\\textrm{mod} \\ m)$\n那么 $(a^{\\phi(m)} - 1)\\prod_{i = 1}^{i = \\phi(m)} r_i \\equiv 0 \\ (\\textrm{mod} \\ m)$\n由 $(r_i, m) = 1$，显然 $a^{\\phi(m)} - 1 \\equiv 0 \\ (\\textrm{mod} \\ m)$\n证毕。\nFermat\u0026rsquo;s Little Theorem 欧拉定理的特殊情况。\nEuler Theorem for Non-coprime 先给出结论，设a，m是正整数，我们有 $a^{k\\phi(m) + b} \\equiv a^{\\phi(m) + b} \\ (\\textrm{mod} \\ m), k \\in N^+$，显然只需要考虑 $(a, m) \\ne 1$\n我们通过证明两个弱化的结论，来证明上述结论。\n 如果p是m的质因数，即 $(p, m) = p$，那么$p^{2\\phi(m)} \\equiv p^{\\phi(m)} \\ (\\textrm{mod} \\ m)$ $a^{2\\phi(m)} \\equiv a^{\\phi(m)} \\ (\\textrm{mod} \\ m)$  令 $m = p_1^{k_1}p_2^{k_2}\\cdots p_s^{k_s}$。\n弱化1(p, m) 不妨设 $p = p_1$, 令 $m' = m \\ / \\ p_1^{k_1}, k = k_1$，显然 $(p^{k}, m') = 1$。\n易见，$p^{2\\phi(m)} - p^{\\phi(m)} \\equiv 0 \\ (\\textrm{mod} \\ m')$。\n且, $p^{2\\phi(m)} - p^{\\phi(m)} \\equiv 0 \\ (\\textrm{mod} \\ p^k)$，因为 $\\phi(m) \\ge p^{k - 1}(p - 1) \\ge k$ 永远成立。\n由中国剩余定理，$(p^{2\\phi(m)} - p^{\\phi(m)}) \\ (\\textrm{mod} \\ p^k\\cdot m')$解存在且唯一，而这里显然是0.\n所以，$p^{2\\phi(m)} \\equiv p^{\\phi(m)} \\ (\\textrm{mod} \\ m)$\n弱化2(a, m) 假设 $(a, m) \\ne 1$，并且质数p，有 $p | (a,m)$，同上不妨设$p = p_1, k = k_1$, 定义$a = pa_1, m = p^km_1$，我们有\n$a^{2\\phi(m)} - a^{\\phi(m)} = p^{2\\phi(m)}a_1^{2\\phi(m)} - p^{\\phi(m)}a_1^{\\phi(m)}$\n$= p^{2\\phi(m)}a_1^{2\\phi(m)} - p^{\\phi(m)}a_1^{2\\phi(m)} + p^{\\phi(m)}a_1^{2\\phi(m)} - p^{\\phi(m)}a_1^{\\phi(m)}$\n因此 $a^{2\\phi(m)} - a^{\\phi(m)} = (p^{2\\phi(m)} - p^{\\phi(m)})a_1^{2\\phi(m)} + p^{\\phi(m)}(a_1^{2\\phi(m)} -a_1^{\\phi(m)})$\n$\\equiv p^{\\phi(m)}(a_1^{2\\phi(m)} -a_1^{\\phi(m)}) \\ (\\textrm{mod} \\ m)$\n即要证明 $p^{\\phi(m)}(a_1^{2\\phi(m)} -a_1^{\\phi(m)}) \\equiv 0\\ (\\textrm{mod} \\ m)$，显然即 $a_1^{2\\phi(m)} -a_1^{\\phi(m)} \\equiv 0\\ (\\textrm{mod} \\ m_1)$\n如果 $(a_1, m_1) = 1$，即得证，否则 $a_1 \u0026lt; a$，我们总能找到质数 $q | (a_1, m_1)$，然后递降，而 $a_1$ 一定是整数，所以 $a_1 \\ge 1$，即存在递降下限1，而 $(1, m_t) = 1$ 恒成立。\n故得证。\n最终结论 由弱化结论2，很容易得知 $a^{k\\phi(m)} \\equiv a^{\\phi(m)} \\ (\\textrm{mod} \\ m), k \\in N^+$，以及 $a^{k\\phi(m) + b} \\equiv a^{\\phi(m) + b} \\ (\\textrm{mod} \\ m), k \\in N^+, b \\in N, b \\le m$，得证。\n总结 证明这个花了我一段时间，果然数论已经全扔了=_=。\n然而还有大整数分解让我头疼\u0026hellip;\nReference [1] 《初等数论》(第三版)，高等教育出版社\n","date":"2017-08-23T22:02:59+08:00","permalink":"https://blog.crazyark.xyz/p/euler-theorem-for-noncoprime/","title":"非质数的欧拉定理扩展 (Euler Theorem for Non-coprime)"},{"content":"前两天刷 Hackerrank 上的 Contest，给了两天时间，没想到被最后一题卡成🐶，谨记录思考和收获。\n原题目在 https://www.hackerrank.com/contests/gs-codesprint/challenges/transaction-certificates ，就不在此赘述了。\n思考过程 首先，题目其实就是将一个交易链进行hash，并要求给出hash碰撞的两个不同的交易链。\n有一些可能有用的条件，p是素数，m是2的幂。\n解的存在性就不证明了，鸽笼原理很简单。\n思路1 由两个hash相减得到，可以将题目转换为 $\\sum\\limits_{i = 0}^{n - 1}a_i \\cdot p^{n - 1 - i} \\equiv 0 \\ (mod\\ m), a_i \\in (-k, k)$, 并且不能是所有 $a_i$ 为0的 trival 解。\n在 $k \\gt p$ 时，令 $a_{n - 1} = p % m, a_{n - 2} = -1$，其余都为0，我们可以很容易的构造原来的解。\n但是在 $k \\le p$ 时，我只想到了暴力搜索（通过暴力搜索多元一次同余式的解空间）。\n思路2 由观察，$a_0a_1\u0026hellip;a_{n-1}$ 构成了一个数的p进制表示，而对于一个正整数$x$，有 $tm + x, t \\in N$ 模m的余数值与x相同。\n而对于 $x$ 的p进制表示，可以唯一的确定一条交易链 (每位+1构成交易链)，并与 $tm + x$ 构成的交易链hash相同。\n但是因为交易链编号限制，只能在 $k\\ge p$ 时快速给出解。\n对于 $k \\lt p$ 的情况，还是只想到了暴力搜索 (通过暴力搜索 tm)，但是结合上面的 $k \u0026gt; p$ 情况的快速构造，解出了44分。\n但是有两三个 TLE，这种解法肯定不行。\nEditorial 在出题者给出的答案里，使用 Thue-Morse Sequence 给出了一种非常快速的构造方案，主要是利用了m是2的幂这个性质，并且p甚至可以退化为奇数。\n下面定义两个序列 $f(N)$ 和 $g(N)$，定义为\n$f(1) = [1]$\n$g(0) = [0]$\n$f(N) = f(N - 1) \\oplus g(N - 1)$\n$g(N) = g(N - 1) \\oplus f(N - 1)$\n这里 $\\oplus$ 是指级联两个序列。\n定义计算 certificate 的函数为 $H(A, m), A = [a_0, a_1, \u0026hellip;, a_{n - 1}]$。\n显然, N 一直是2的幂，令 $N = 2^n$，那么存在一个足够大的 $N$ ，$H(f(N), m) = H(g(N), m)$，这里 m 必须是2的幂。\n最有意思的是它的证明，我们可以通过证明知道，在 $m \\le 2^{32}$ 的情况下，这样的 $N$ 大约在 $\\sqrt{m}$ 左右。\n下面是证明：\n显然，我们知道 $\\overline{f(N)} = g(N)$。\n令 $T = H(f(N), m) - H(g(N), m)$, 显然\n$T = p^{2^n - 1} - p^{2^n - 2} - p^{2^n - 3} + p^{2^n - 4} - \u0026hellip; \\pm 1$\n那么 $T = (p - 1)(p^2 - 1)\\cdots(p^{2^{n-1}} - 1)$。\nOK，还有最后一块砖: 如果 $2^s|(p - 1)$，那么 $2^{s + 1} |(p^2 - 1)$，这个证明很简单，就不证了。\n这里答案漏考虑 p = 2 的情况，但是这种情况太容易了，所以直接忽略了\u0026hellip;而且test cases里也没有😅\n所以由p为质数即奇数，$2 | (p - 1)$，那么一定有 $2^n | (p^{2^n} - 1)$。\n故而一定有 $2^{n(n - 1)/2} | T$，只要 $n(n-1)/2 \\ge x，m = 2^x$，就有 $m | T$。\n可以看到，$N$ 的收敛岂止是快。\n得到 $f, g$ 稍微处理下就出答案了，因为答案要求长度为 $n$ 的整数倍，所以复杂度为 $O(n)$。\n总结 这个题目里Certificate的计算方式就是 Polynominal Hash，它和 Thue-Morse Sequence 一样，都有一些很有意思的性质，留着之后慢慢发掘一下。\n在看到 m为2的幂的条件时，第一反应肯定是能够做什么文章，而我想来想去想将 Fermat\u0026rsquo;s Last Theorem 应用上去，始终无法如愿。这道题由于解空间庞大，所以想必是构造解，只是实在想不到如何应用p是素数以及m是2的幂，可以看到我的思路中始终无法应用这两点。\nEditorial 的解法简直精妙，更偏近于数学了。\nReferences  http://codeforces.com/blog/entry/4898 https://www.hackerrank.com/contests/gs-codesprint/challenges/transaction-certificates/editorial https://www.wikiwand.com/en/Thue%E2%80%93Morse_sequence ","date":"2017-08-21T23:58:42+08:00","permalink":"https://blog.crazyark.xyz/p/polynomial-hash-and-theu-morse-sequence/","title":"多项式哈希及Theu-Morse序列 (Polynomial Hash and Theu-Morse Sequence)"},{"content":"最近碰到一道题目，求一个图的全局最小割，可惜图论博主学的不太好，至今只记得一个求s-t最大流/最小割的 ford-fulkerson。想了想总不能做$n^2$次最大流吧，最终还是求助了维基百科 🤣\nStoer-Wagner Algorithm Stoer-Wagner 算法是一个求带非负权无向图中全局最小割的算法，它是在1995年由 Mechthild Stoer 和 Frank Wagner 提出的。\n算法的核心思想是通过不断的合并某个s-t最小割的顶点来缩小图的规模，直到图里只剩下两个合并剩下的顶点。\n算法流程 在图G中，假设G的全局最小割为C = {S, T}，那么对于任何一对G上的顶点(s, t)来说\n 要么s和t分属S和T 要么s和t同属S或T  Stoer-Wagner首先找到一个s-t最小割，然后将s和t合并：\n 删除s和t之间的边 对于任何和s、t相邻的顶点v，与新顶点链接的权重为 w(v, s) + w(v, t)（假设不相邻为0权重）  显然，对于任何s、t在同一个集合的割，合并s、t并不影响割的权重。\n上述流程（找到一个最小割、合并）为一个阶段，在进行 |V| - 1 次这样的阶段之后，G中只剩下两个点，那么全局最小割一定为每个阶段的s-t最小割以及最后两个点的割中的最小值，如此算法即完成了。\n同学们一定发现了，如何找到一个s-t最小割才是算法能够高效执行的关键，Stoer-Wagner 算法的巧妙之处就在于此。\n因为算法对于s-t最小割的起点和终点没有任何要求，所以找到任何一个最小割就行，过程如下：\n 图G上任意一个顶点$u$，放入集合$A$中 选取一个$V - A$中的点$v$，使得$w(A,v)$最大，将它放入$A$中；其中 $w(A,v) = \\sum_\\limits{u \\in A}w(u,v)$ 重复执行 步骤2，直到$V = A$ 假设最后加入$A$的两个顶点是$s$和$t$, 合并它们  上述过程中 $\\left(A - {s}, {t}\\right)$ 是一个s-t最小割。\n正确性证明 要证明算法的正确性，只需要证明阶段中找到的一定是一个s-t最小割。\n符号定义\n 图$G(V,E,W)$ $s$,$t$ 是最后加入集合$A$的两个顶点 令$C = (X, \\overline{X})$为任意一个s-t割，不失一般性，令 $t \\in X$ $A_v$为在顶点$v$加入$A$之前的集合 $C_v$是在由顶点$A_v \\cup {v}$构成的$G$的子图上，由割$C$导出的割  所以，我们有$C_v = C$，即只需要证明 $w(A_t, t) \\le w(C ) = w(C_t)$。\n下面我们将归纳证明 $\\forall u \\in X$，$w(A_u, u) \\le w(C_u)$。\n设$u_0$是$X$中第一个被放入$A$中的顶点，那么由定义，有 $w(A_{u_0}, u) = w(C_{u_0})$\n假设对于两个连续被放入$A$中、并且同属于$X$的顶点$u, v$，归纳假设有 $w(A_u,u) \\le w(C_u)$\n首先，我们有 $w(A_v, v) = w(A_u, v) + w(A_v - A_u, v)$\n因为$u$优先于$v$被选取，所以$w(A_u, u) \\ge w(A_u, v)$，所以上式有\n$w(A_v, v) \\le w(A_u, u) + w(A_v - A_u, v) \\le w(C_u) + w(A_v - A_u, v)$\n而由于$(A_v - A_u) \\cap X = {u}$，所以 $W(C_u) + w(A_v - A_u, v) = w(C_v)$\n所以有 $w(A_u, u) \\le w(C_u)$，证毕。\n故而我们知道，算法每个阶段都能找到一个s-t最小割。\n时间复杂度 每次计算最大的$w(A, u)$的时间为$O(|V|^2)$，最后合并的时间为 $O(|E|)$，所以每一个阶段时间复杂度为 $O(|E| + |V|^2)$；\n一共执行 $|V|-1$次，算法总计时间复杂度为 $O(|V||E| + |V|^3)$。\n其中每次选择最大的$w(A, u)$可以通过堆进行优化，将时间优化到$O(lg|V|)$，总计时间复杂度可以降为 $O(|V||E| + |V|^2lg|V|)$。\n这里顺便一提，Stoer-Wagner 算法适用的堆需要支持动态 update，常用的堆为斐波那契堆，在 C++ 的标准 stl 里并没有实现，替代方案可以使用 std::multiset。在 boost 中，对 Stoer-Wagner 算法有直接支持。\nReferences [1] https://en.wikipedia.org/wiki/Stoer%E2%80%93Wagner_algorithm\n[2] http://www.boost.org/doc/libs/1_64_0/libs/graph/doc/stoer_wagner_min_cut.html\n","date":"2017-08-05T22:14:01+08:00","permalink":"https://blog.crazyark.xyz/p/stoer-wagner-algorithm/","title":"无向带权图图全局最小割 Stoer-Wagner 算法 (Stoer-Wagner Algorithm -- Global Min-Cut in Undirected Weighted Graphs)"},{"content":"偶然发现 AtCoder，上去注册了准备试试，结果卡在practice contest\u0026hellip;\n问题倒是很简单：\n There are N balls labeled with the first N uppercase letters. The balls have pairwise distinct weights. You are allowed to ask at most Q queries. In each query, you can compare the weights of two balls (see Input/Output section for details). Sort the balls in the ascending order of their weights.\n  Constraints (N,Q)=(26,1000), (26,100), or (5,7).\n  Partial Score There are three testsets. Each testset is worth 100 points. In testset 1, N=26 and Q=1000. In testset 2, N=26 and Q=100. In testset 3, N=5 and Q=7.\n 通过比较排序，一共三种数据，其中 (26, 1000) 的情况用任何比较都能过，但是可能会 TLE，(26, 100) 的用 worst-case $O(nlgn)$ 的 merge sort 能过，唯一难受的是 (5, 7)。这个样例 merge sort 的 worst case 是比较8次。\n我和某网友一样，尝试用 STL 的 sort 来解决，结果发现 WA 了更多 🙄\nYou must be kidding!\nFord-Johnson 绝望之下只能求助 google，果然找到了一个算法叫 Ford-Johnson Algorithm，是 1959 年被提出来的，是针对比较排序中比较数进行最优化的算法，事实上在提出后的相当长一段时间被认为是比较数的 lower bound。\n即便后来被证明能够进一步优化，Ford-Johnson 算法距离事实 lower bound 也极其接近。\n在 Knuth 老爷子的 《 The Art of Computer Programming 》第三卷中，该算法也被称为 merge-insertion sort。\n算法流程 假设我们有n个元素需要排序，算法总共分为三步：\n  将元素分为 $\\lfloor n/2 \\rfloor$ 对，并两两进行比较，如果 n 是奇数，最后一个元素不进行配对\n  递归地排序每对中大的那一个。现在我们有一个由每对中较大值组成的排过序的序列，叫做 main chain。假设 main chain 是 $a_1, a_2, \u0026hellip; a_{\\lfloor n/2 \\rfloor}$ ，剩余的元素假设他们是 $b_1, b_2, \u0026hellip;, b_{\\lceil n/2\\rceil}$，并且有 $b_i \\le a_i$ 对 $i = 1, 2, \u0026hellip;, \\lfloor n/2 \\rfloor$ 成立。\n  现在我们将 $b_1, \u0026hellip;, b_{\\lceil n/2 \\rceil}$ 插入 main chain 中，首先是 $b_1$，我们知道他在 $a_1$ 前面，所以 main chain 变为 ${b_1, a_1, a_2, \u0026hellip; a_{\\lfloor n/2 \\rfloor}}$，然后考虑插入 $b_3$，仅需要比较三个数 ${b_1, a_1, a_2}$；假设前三个元素变为 ${b_1, a_1, b_3}$，那么插入 $b_2$ 也是在这三个数内；可以看到，不论 $b_3$ 被插入到那里了，要插入的范围总是最多为3。下一个要插入的数 $b_k$ 对应于下一个Jacobsthal Number，即我们先插入 $b_3$，然后插入 $b_5, b_{11}, b_{21} \u0026hellip;$，总结一下插入顺序为 $b_1, b_3, b_2, b_5, b_4, b_{11}, b_{10}, \u0026hellip;, b_6, b_{21}, \u0026hellip;$\n  性能 Ford-Johnson 算法需要特殊的数据结构来实现，运行速度并不算快，只是能够更少地进行比较，在实际使用中还是 merge sort 和 quick sort 来得更快一点。\n问题 (5, 7) 假设元素 A, B, C, D, E\n 比较 (A, B) 和 (C, D)，不失一般性，我们假设 A \u0026gt; B，C \u0026gt; D 比较 (A, C)，不失一般性，假设 A \u0026gt; C 将 E 插入 (D, C, A)，两次比较内完成 将 B 插入排序后的{E, C, D} 中 (因为 B \u0026lt; A，所以不需要考虑 A 的比较)，两次比较内完成  这里第三步先插入 E，是因为如果先插入 B 到 (D, C)，最多需要两次比较，而插入 E 到 {D, C, B, A} 最多要三次比较。\nReferences [1] Bui, T. D., and Mai Thanh. \u0026ldquo;Significant improvements to the Ford-Johnson algorithm for sorting.\u0026rdquo; BIT Numerical Mathematics 25.1 (1985): 70-75.\n[2] https://codereview.stackexchange.com/questions/116367/ford-johnson-merge-insertion-sort\nAppendix Interactive Sorter in C++ #include \u0026lt;iostream\u0026gt;#include \u0026lt;algorithm\u0026gt;#include \u0026lt;vector\u0026gt;using namespace std; bool less_than(char a, char b) { cout \u0026lt;\u0026lt; \u0026#34;? \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; endl; cout.flush(); char ans; cin \u0026gt;\u0026gt; ans; if (ans == \u0026#39;\u0026lt;\u0026#39;) return true; return false; } void ford_johnson(string \u0026amp;s, int n) { // assert(n == 5)  // ugly but work  if (!less_than(s[0], s[1])) { swap(s[0], s[1]); } if (!less_than(s[2], s[3])) { swap(s[2], s[3]); } if (!less_than(s[1], s[3])) { swap(s[0], s[2]); swap(s[1], s[3]); } // now we have s[0] \u0026lt; s[1], s[2] \u0026lt; s[3], s[1] \u0026lt; s[3]  vector\u0026lt;char\u0026gt; cs = {s[0], s[1], s[3]}; // insertion will be completed in two comparations  auto insert_into_first_three = [\u0026amp;](char c) { if (less_than(c, cs[1])) { if (less_than(c, cs[0])) cs.insert(cs.begin(), c); else cs.insert(cs.begin() + 1, c); } else { if (less_than(c, cs[2])) cs.insert(cs.begin() + 2, c); else cs.insert(cs.begin() + 3, c); } }; insert_into_first_three(s[4]); // always sorted {s[0], s[1], s[4]} \u0026lt; s[3] or s[0] \u0026lt; s[1] \u0026lt; s[3] \u0026lt; s[4]  // so the first three elements are enough  insert_into_first_three(s[2]); s = string(cs.begin(), cs.end()); } // at most 99 comparations for n = 26 void merge_sort(string \u0026amp;s, int n) { if (n == 1) return; else if (n == 2) { if (!less_than(s[0], s[1])) swap(s[0], s[1]); return; } auto left_half = s.substr(0, n / 2), right_half = s.substr(n / 2); merge_sort(left_half, n / 2); merge_sort(right_half, n - n / 2); // merge, at most n - 1 comparations  int i = 0, j = 0, k = 0; while (k \u0026lt; n) { if (i \u0026gt;= n / 2) s[k++] = right_half[j++]; if (j \u0026gt;= n - n / 2) s[k++] = left_half[i++]; else if (less_than(left_half[i], right_half[j])) s[k++] = left_half[i++]; else s[k++] = right_half[j++]; } } int main() { int n, q; cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; q; string s; for (int i = 0; i \u0026lt; n; ++i) { s += \u0026#39;A\u0026#39; + i; } if (n == 5) ford_johnson(s, n); else merge_sort(s, n); cout \u0026lt;\u0026lt; \u0026#34;! \u0026#34; \u0026lt;\u0026lt; s \u0026lt;\u0026lt; endl; return 0; } ","date":"2017-08-04T14:15:35+08:00","permalink":"https://blog.crazyark.xyz/p/ford-johnson-algorithm/","title":"优化比较次数的排序算法 (Ford Johnson Algorithm)"},{"content":"Leetcode 上有一道题叫 Sliding Window Maximum，虽然不是今天刷的，但是解法非常有意思，就记录一下。\n问题描述：\n Given an array nums, there is a sliding window of size k which is moving from the very left of the array to the very right. You can only see the k numbers in the window. Each time the sliding window moves right by one position.\n  For example, Given nums = [1,3,-1,-3,5,3,6,7], and k = 3.\n  Therefore, return the max sliding window as [3,3,5,5,6,7].\n 这道题可以用优先队列、自平衡BST等方法得到一个 O(nlgn) 的解法，但其实这道题有另一种 O(n) 的解法，基本思想是在过程中维持一个单调队列。\nMonotonic Queue 我们用双端队列来实现这个单调队列，保证这个队列中所有数单调非增，同时一个窗口中的最大的数就在队列的开端。\n假设我们现在有一个队列 Q 满足上述条件，当一个数 a 进入窗口时，此时数 b 滑出窗口，队列操作步骤：\n 如果队列不为空并且队列开端等于 b，remove it! 如果队列不为空并且队列尾端小于 a，remove it！ 循环 步骤2 直到队列为空或者队列尾端不小于 a 将 a 加到队列尾端  显然，在 步骤2 中，删除的数都是不可能为最大值的数，所以在操作结束后，窗口内最大值仍然在队列中，并且队列保持单调非增。\n此时最大值即为队列开端。\n因为每个数只进队一次，并出队一次，所以时间开销为 O(n)。\nAppendix C++ 实现 vector\u0026lt;int\u0026gt; maxSlidingWindow(vector\u0026lt;int\u0026gt; \u0026amp;nums, int k) { vector\u0026lt;int\u0026gt; res; // tracking index instead of value  deque\u0026lt;int\u0026gt; dq; for (int i = 0; i \u0026lt; nums.size(); ++ i) { // dequeue (i - k)th element if exists  if (!dq.empty() \u0026amp;\u0026amp; dq.front() == i - k) dq.pop_front(); // remove those less than current  while (!dq.empty() \u0026amp;\u0026amp; nums[dq.back()] \u0026lt; nums[i]) dq.pop_back(); // enqueue current  dq.push_back(i); if (i \u0026gt;= k - 1) res.push_back(nums[dq.front()]); } return res; } ","date":"2017-08-03T15:55:02+08:00","permalink":"https://blog.crazyark.xyz/p/monotonic-queue/","title":"滑动窗口中的最大值 (Sliding Window Maximum / Monotonic Queue)"},{"content":"今天在刷 leetcode 的时候遇到一道题目 Kth Smallest Element in a Sorted Matrix。\n首先用一个 Min-Heap 就可以得到 O(klgn) (n为列数)的算法，实现放在最后。\n然而在翻阅览 Discuss 区的时候发现，这玩意居然有 O(n) (n为行、列数) 的算法，来自一篇论文 Selection in X + Y and Matrices with Sorted Rows and Columns，同时适用于另一道题 Find k Pairs with Smallest Sums，在此只做介绍，因为我不认为有人能在面试的时候写的出来\u0026hellip;\n简单介绍 设 A 为一个 n * n 的矩阵，所有行和列都是非降续的。\n令 $A^*$ 是 A 取所有奇数行的子矩阵，如果 n 为偶数的话添上 A 的最后一行一列；令 $rank_{desc}(A, a)$ 为 A 中大于 a 的元素的数目，$rank_{asc}(A, a)$ 为 A 中小于 a 的元素数目。\n那么有以下不等式成立\n $rank_{asc}(A, a) \\le 4 rank_{asc}(A^*, a)$ $rank_{desc}(A, a) \\le 4 rank_{desc}(A^*, a)$  这是这个算法的基础，加上巧妙的选择，可以是使得问题变为解一个在A*上的子问题。\n算法过程参考论文，非常不直观，需要不少理论证明，实在没法详述\u0026hellip; 有兴趣的同学还是参阅原论文\nPython、C++ 算法实现 在此摘录 discuss 上大佬给出的 python 和 cpp 版实现，在面试中谁能手写出来我只能跪穿。\nclass Solution(object): def kthSmallest(self, matrix, k): # The median-of-medians selection function. def pick(a, k): if k == 1: return min(a) groups = (a[i:i+5] for i in range(0, len(a), 5)) medians = [sorted(group)[len(group) / 2] for group in groups] pivot = pick(medians, len(medians) / 2 + 1) smaller = [x for x in a if x \u0026lt; pivot] if k \u0026lt;= len(smaller): return pick(smaller, k) k -= len(smaller) + a.count(pivot) return pivot if k \u0026lt; 1 else pick([x for x in a if x \u0026gt; pivot], k) # Find the k1-th and k2th smallest entries in the submatrix. def biselect(index, k1, k2): # Provide the submatrix. n = len(index) def A(i, j): return matrix[index[i]][index[j]] # Base case. if n \u0026lt;= 2: nums = sorted(A(i, j) for i in range(n) for j in range(n)) return nums[k1-1], nums[k2-1] # Solve the subproblem. index_ = index[::2] + index[n-1+n%2:] k1_ = (k1 + 2*n) / 4 + 1 if n % 2 else n + 1 + (k1 + 3) / 4 k2_ = (k2 + 3) / 4 a, b = biselect(index_, k1_, k2_) # Prepare ra_less, rb_more and L with saddleback search variants. ra_less = rb_more = 0 L = [] jb = n # jb is the first where A(i, jb) is larger than b. ja = n # ja is the first where A(i, ja) is larger than or equal to a. for i in range(n): while jb and A(i, jb - 1) \u0026gt; b: jb -= 1 while ja and A(i, ja - 1) \u0026gt;= a: ja -= 1 ra_less += ja rb_more += n - jb L.extend(A(i, j) for j in range(jb, ja)) # Compute and return x and y. x = a if ra_less \u0026lt;= k1 - 1 else \\ b if k1 + rb_more - n*n \u0026lt;= 0 else \\ pick(L, k1 + rb_more - n*n) y = a if ra_less \u0026lt;= k2 - 1 else \\ b if k2 + rb_more - n*n \u0026lt;= 0 else \\ pick(L, k2 + rb_more - n*n) return x, y # Set up and run the search. n = len(matrix) start = max(k - n*n + n-1, 0) k -= n*n - (n - start)**2 return biselect(range(start, min(n, start+k)), k, k)[0] class Solution { public: int kthSmallest(const std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp; matrix, int k) { if (k == 1) // guard for 1x1 matrix \t{ return matrix.front().front(); } size_t n = matrix.size(); std::vector\u0026lt;size_t\u0026gt; indices(n); std::iota(indices.begin(), indices.end(), 0); std::array\u0026lt;size_t, 2\u0026gt; ks = { k - 1, k - 1 }; // use zero-based indices \tstd::array\u0026lt;int, 2\u0026gt; results = biSelect(matrix, indices, ks); return results[0]; } private: // select two elements from four elements, recursively \tstd::array\u0026lt;int, 2\u0026gt; biSelect( const std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp; matrix, const std::vector\u0026lt;size_t\u0026gt; \u0026amp; indices, const std::array\u0026lt;size_t, 2\u0026gt; \u0026amp; ks) // Select both ks[0]-th element and ks[1]-th element in the matrix, \t// where k0 = ks[0] and k1 = ks[1] and n = indices.size() satisfie \t// 0 \u0026lt;= k0 \u0026lt;= k1 \u0026lt; n*n and k1 - k0 \u0026lt;= 4n-4 = O(n) and n\u0026gt;=2 \t{ size_t n = indices.size();\tif (n == 2u) // base case of resursion \t{\treturn biSelectNative(matrix, indices, ks); } // update indices \tstd::vector\u0026lt;size_t\u0026gt; indices_; for (size_t idx = 0; idx \u0026lt; n; idx += 2) { indices_.push_back(indices[idx]); } if (n % 2 == 0) // ensure the last indice is included \t{ indices_.push_back(indices.back()); } // update ks \t// the new interval [xs_[0], xs_[1]] should contain [xs[0], xs[1]] \t// but the length of the new interval should be as small as possible \t// therefore, ks_[0] is the largest possible index to ensure xs_[0] \u0026lt;= xs[0] \t// ks_[1] is the smallest possible index to ensure xs_[1] \u0026gt;= xs[1] \tstd::array\u0026lt;size_t, 2\u0026gt; ks_ = { ks[0] / 4, 0 }; if (n % 2 == 0) // even \t{ ks_[1] = ks[1] / 4 + n + 1; } else // odd \t{ ks_[1] = (ks[1] + 2 * n + 1) / 4; } // call recursively \tstd::array\u0026lt;int, 2\u0026gt; xs_ = biSelect(matrix, indices_, ks_); // Now we partipate all elements into three parts: \t// Part 1: {e : e \u0026lt; xs_[0]}. For this part, we only record its cardinality \t// Part 2: {e : xs_[0] \u0026lt;= e \u0026lt; xs_[1]}. We store the set elementsBetween \t// Part 3: {e : x \u0026gt;= xs_[1]}. No use. Discard. \tstd::array\u0026lt;int, 2\u0026gt; numbersOfElementsLessThanX = { 0, 0 }; std::vector\u0026lt;int\u0026gt; elementsBetween; // [xs_[0], xs_[1])  std::array\u0026lt;size_t, 2\u0026gt; cols = { n, n }; // column index such that elem \u0026gt;= x \t// the first column where matrix(r, c) \u0026gt; b \t// the first column where matrix(r, c) \u0026gt;= a \tfor (size_t row = 0; row \u0026lt; n; ++row) { size_t row_indice = indices[row]; for (size_t idx : {0, 1}) { while ((cols[idx] \u0026gt; 0) \u0026amp;\u0026amp; (matrix[row_indice][indices[cols[idx] - 1]] \u0026gt;= xs_[idx])) { --cols[idx]; } numbersOfElementsLessThanX[idx] += cols[idx]; } for (size_t col = cols[0]; col \u0026lt; cols[1]; ++col) { elementsBetween.push_back(matrix[row_indice][indices[col]]); } } std::array\u0026lt;int, 2\u0026gt; xs; // the return value \tfor (size_t idx : {0, 1}) { size_t k = ks[idx]; if (k \u0026lt; numbersOfElementsLessThanX[0]) // in the Part 1 \t{ xs[idx] = xs_[0]; } else if (k \u0026lt; numbersOfElementsLessThanX[1]) // in the Part 2 \t{ size_t offset = k - numbersOfElementsLessThanX[0]; std::vector\u0026lt;int\u0026gt;::iterator nth = std::next(elementsBetween.begin(), offset); std::nth_element(elementsBetween.begin(), nth, elementsBetween.end()); xs[idx] = (*nth); } else // in the Part 3 \t{ xs[idx] = xs_[1]; } } return xs; } // select two elements from four elements, using native way \tstd::array\u0026lt;int, 2\u0026gt; biSelectNative( const std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; \u0026amp; matrix, const std::vector\u0026lt;size_t\u0026gt; \u0026amp; indices, const std::array\u0026lt;size_t, 2\u0026gt; \u0026amp; ks) { std::vector\u0026lt;int\u0026gt; allElements; for (size_t r : indices) { for (size_t c : indices) { allElements.push_back(matrix[r][c]); } } std::sort(allElements.begin(), allElements.end()); std::array\u0026lt;int, 2\u0026gt; results; for (size_t idx : {0, 1}) { results[idx] = allElements[ks[idx]]; } return results; } }; Min-Heap O(klgn) 算法 class Solution { struct Tuple { int x, y, val; Tuple(int x, int y, int val) : x(x), y(y), val(val) {} bool operator\u0026lt;(const Tuple\u0026amp; other) const { return val \u0026lt; other.val; } bool operator\u0026gt;(const Tuple\u0026amp; other) const { return val \u0026gt; other.val; } }; public: int kthSmallest(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; matrix, int k) { priority_queue\u0026lt;Tuple, vector\u0026lt;Tuple\u0026gt;, std::greater\u0026lt;Tuple\u0026gt;\u0026gt; min_heap; int n = matrix.size(); for (int i = 0; i \u0026lt; n; ++i) min_heap.push(Tuple(0, i, matrix[0][i])); for (int i = 0; i \u0026lt; k - 1; ++i) { auto t = min_heap.top(); min_heap.pop(); if (t.x == n - 1) continue; min_heap.push(Tuple(t.x + 1, t.y, matrix[t.x + 1][t.y])); } return min_heap.top().val; } }; ","date":"2017-08-02T20:58:55+08:00","permalink":"https://blog.crazyark.xyz/p/selection-in-x-y-or-sorted-matrices/","title":"有序矩阵中的第k大数 (Selection in X+Y or Sorted Matrices)"},{"content":"虽然在一年前就知道了 Proxy 模式，但是基本没有尝试使用过，仅在框架里看到一些例子。昨天翻阅《大型网站系统与Java中间件实践》时，偶然发现了 Proxy 模式在 Java 中的应用 —— 动态代理，遂记录下来，顺便复习一下 Proxy 模式。\n以下内容参考自\n 《Design Patterns》 《大型网站系统与Java中间件实践》。 http://www.baeldung.com/java-dynamic-proxies  Proxy 模式图示：\n Proxy Pattern \nProxy 模式 在《Design Patterns》书中提到\u0005了使用 Proxy 模式的四种常见情况：\n 远程代理 (Remote Proxy) 为一个对象在不同的地址空间提供局部代表。 虚代理 (Virtual Proxy) 根据需要创建开销很大的对象。 保护代理 (Protection Proxy) 控制对原始对象的访问。保护代理用于对象应该有不同的访问权限的时候。 智能指针 (Smart Reference) 引用计数、实现线程安全、延迟创建，熟悉 C++ 的同学应该知道 std::share_ptr、 std::weak_ptr、std::unique_ptr 等。  Java 中的静态代理 这里借用 《大型网站系统与Java中间件实践》中的例子：\npublic interface Calculator { int add(int a, int b); } public class CalculatorImpl implements Calculator { int add(int a, int b) { return a + b; } } public class CalculatorProxy implements Calculator { private Calculator calculator; public CalculatorProxy(Calculator calculator) { this.calculator = calculator; } int add(int a, int b) { // We can do anything before and after real add is called,  // such as logging  return calculator.add(a, b); } } 可以看到，在 CalculatorProxy 中我们可以调用了真正的add操作，并可以在add前后做很多额外的工作，这种方式看上去十分直接，但是如果我们有一批类需要代理，并且代理类中实现的功能是一致的，那么我们就需要为每一个类都实现一个 XxxProxy 类，这将会十分麻烦。\nJava 中的动态代理 Java 中提供了一种方式使我们能够动态的生成一个类的代理，我们来看一个例子\npublic class DynamicProxyPlayground { public static void main(String[] args) { Map mapProxyInstance = (Map) Proxy .newProxyInstance(DynamicProxyPlayground.class.getClassLoader(), new Class[]{Map.class}, new TimingDynamicInvocationHandler(new HashMap\u0026lt;String, String\u0026gt;())); // DynamicProxyPlayground$TimingDynamicInvocationHandler invoke  // INFO: Executing put finished in 74094 ns.  mapProxyInstance.put(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); CharSequence csProxyInstance = (CharSequence) Proxy.newProxyInstance( DynamicProxyPlayground.class.getClassLoader(), new Class[] { CharSequence.class }, new TimingDynamicInvocationHandler(\u0026#34;Hello World\u0026#34;)); // DynamicProxyPlayground$TimingDynamicInvocationHandler invoke  // INFO: Executing length finished in 11720 ns.  csProxyInstance.length(); } public static class TimingDynamicInvocationHandler implements InvocationHandler { private static Logger logger = Logger.getLogger(TimingDynamicInvocationHandler.class.getName()); private Object target; private TimingDynamicInvocationHandler(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { long start = System.nanoTime(); Object result = method.invoke(target, args); long elapsed = System.nanoTime() - start; logger.info(String.format(\u0026#34;Executing %s finished in %d ns.\u0026#34;, method.getName(), elapsed)); return result; } } } 在上述例子中，通过实现一个 InvocationHandler，我们就可以使用Proxy.newInstance为一批类创建功能相同的动态代理。\n","date":"2017-07-29T22:44:24+08:00","permalink":"https://blog.crazyark.xyz/p/dynamic-proxy-java/","title":"Dynamic Proxy in Java"},{"content":"刷leetcode时碰到的问题，本篇仅做简要描述，以及记录思考。\n参考自: https://gregable.com/2013/10/majority-vote-algorithm-find-majority.html，一篇写的非常好的博客\n问题描述：考虑你有一个长度为n的无序列表，现在你想知道列表中是否有一个值占据了列表的一半以上 (majority)，如果有的话找出这个数。\n这个问题的一个普遍的应用场景是在容错计算 (fault-tolerant computing) 中，在进行了多次冗余的计算后，输出最后多数计算得到的值。\n显而易见的做法 先排序，然后遍历列表并计数就行。耗时为 O(nlgn)，不够快！实际上我们可以在 O(n) 的时间内给出结果。\nBoyer-Moore Majority Algorithm 论文依据：Boyer-Moore Majority Algorithm\n该算法时间开销为 O(2n), 空间开销为 O(1)，总共遍历两遍列表，思想非常简单。\n我们需要以下两个值:\n candidate，初始化为任意值 count，初始化为0  第一遍遍历，以 current 代表当前值：\n IF count == 0, THEN candidate = current IF candiadate == current THEN count += 1 ELSE count -= 1  第二遍遍历，对上次结果中的 candidate 计数，超过一半则存在 majority 为 candidate，否则不存在\n来看一下Python版代码实现：\ndef boyer_moore_majority(input): candidate = 0 count = 0 for value in input: if count == 0: candidate = value if candidate == value count += 1 else: count -= 1 count = 0 for value in input: if candidate == value: count += 1 if count \u0026gt; len(input) / 2: return candidate else: return -1 # any value represents NOT FOUND 一个简单的证明 我们只需要考虑在原列表中有 majority 的情况，因为如果没有第二遍遍历会直接 reject。\n所以假设列表 L 中存在majority，记为 M。\n可以看到，上面 candidate 在 count 等于 0 的时候变更，其实将列表分成了一段一段，每一段有一个 candidate。\n每一段有一个重要的性质，即 candidate 在这一段中恰好占据了一半。\n我们归纳证明：在最后一段中 candidate == M\n那么当扫描到第一段S时，有两种情况：\n candidate == M，那么根据 M 是 majority，以及根据 count(M in S) = len(S) / 2，在子列表 L - S 中 M 还是 majority candidate != M，那么 count(M in S) \u0026lt;= len(S) / 2, 同上，L - S 中 M 还是 majority  最后一段就是最后一个子列表，所以 candidate == M。\n更快更好 😁 两遍遍历的 O(n) 已经很快，但是大家还是觉得不够快，于是\u0026hellip; O(3n / 2 -2) 的算法诞生了。\n这个算法只比较 3n/2 - 2 次，已经是理论下限。 Here is the prover.Finding a majority among N votes\n这个算法的基本想法是：将原列表重新排列，使得没有两个相邻的值是相同的。\n在这里，我们需要一个桶来存放额外的值，所以空间消耗为 O(n)，同样是两遍遍历。\n第一遍遍历，candidate 保持为列表的最后一个值，current 为当前值\n current == candidate, 把 current 放入桶中 current != candidate, 把 current 放到列表的最后，然后从桶中取出一个放到列表最后  显然列表相邻的两个绝不可能相同\n第二遍遍历中，我们需要将 candidate 不断的与列表最后一个值比较：\n 如果相同，从列表最后去除两个元素 否则，从列表最后去除一个元素，并从桶中去除一个元素  如果桶空了，那么没有 majority，否则 candidate 就是 majority。\n证明略去，有兴趣的同学可以参考论文。\n分布式 Boyer-Moore 有兴趣的同学可以阅读 Finding the Majority Element in Parallel。\n主要算法如下：\ndef distributed_boyer_moore_majority(parallel_output): candidate = 0 count = 0 for candidate_i, count_i in parallel_output: if candidate_i = candidate: count += count_i else if count_i \u0026gt; count: count = count_i - count candidate = candidate_i else: count = count - count_i ... 总结 刷 leetcode 时遇到的很有意思的题目 https://leetcode.com/problems/majority-element-ii/tabs/description，知道这个算法就非常容易了。\n","date":"2017-07-28T21:08:02+08:00","permalink":"https://blog.crazyark.xyz/p/majority-voting/","title":"Boyer-Moore 投票算法 (Boyer-Moore Majority Voting Algorithm)"},{"content":"今年阿里中间件比赛的时候不巧博主心情不好，外加要准备期末考试，并没有参加，非常遗憾。不过好在好基友 Eric Fu 参加并获得了冠军！今年的主题是分布式数据库，如果想了解详情及复赛的解题思路请读者前往 Eric 的博客。\n博主没有参加甚是遗憾，外加看到题目手痒难耐，遂问基友讨了最后的24h极客赛来玩一玩。\n24h TOPKN 题目是分布式数据库上的分页排序，对应的SQL执行为 order by id limit k，n；主要的技术挑战为\u0026quot;分布式\u0026quot;的策略，赛题中使用多个文件模拟多个数据分片。\n简称 top(k, n)。\n 给定一批数据，求解按顺序从小到大，顺序排名从第k下标序号之后连续的n个数据 (类似于数据库的order by asc + limit k,n语义)\n  top(k,3)代表获取排名序号为k+1,k+2,k+3的内容,例:top(10,3)代表序号为11、12、13的内容,top(0,3)代表序号为1、2、3的内容 需要考虑k值几种情况，k值比较小或者特别大的情况，比如k=1,000,000,000 对应k,n取值范围： 0 \u0026lt;= k \u0026lt; 2^63 和 0 \u0026lt; n \u0026lt; 100 数据全部是文本和数字结合的数据，排序的时候按照整个字符串的长度来排序。例如一个有序的排序如下(相同长度情况下ascii码小的排在前面)： 例： a ab abc def abc123\n  例如给定的k,n为(2,2)，则上面的数据需要的答案为: abc def\n 原题目在 https://code.aliyun.com/wanshao/topkn_final。\n要求在24小时内完成，然而 \u0026hellip;\nCodes 托管在 github 上 https://github.com/arkbriar/topKN，这个版本从基友的版本轻度修改而来 \u0026hellip;\n解题过程    主要条件  主要限制     3台机器: 2台worker，1台master 2. 输出在master上 | |1. Timeout: 5min 2. JVM heap size: 3G 3. 不允许使用堆外内存(FileChannel)  题目中一共有大约160000000条数据(字符串)，并分布在两台机器上，要进行 top(k, n)，首先 brute force 一定是不行的。\n如果要对160000000条数据全排序JVM，第一内存太小，第二时间不够。对 10000000 条数据进行排序大约耗时为 1min13s，参考自 https://codereview.stackexchange.com/questions/67387/quicksort-implementation-seems-too-slow-with-large-data；如果要进行外排序，磁盘读写导致耗时更长；以及还有最后要输出到master上，还有网络开销。\n稍微了解一些数据库的同学都知道，数据库里有个叫索引(Index)的东西，是用来加速查询的。那我们首先来看一下 MySQL 的索引。\nMySQL 的索引 索引对查询的速度有至关重要的影响，理解索引也是进行数据库性能调优的起点。假设数据库中一个表有100000条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10000个页面，如果这10000个页面在磁盘上随机分布，需要进行10000次I/O，假设磁盘每次I/O时间为10ms，则总共需要100s(实际远不止)。但是如果建立 B+ 树索引，则只需要进行 $log_{100}1000000 = 3$ 次页面读取，最坏情况下耗时30ms，这就是索引带来的效果。\nMySQL 有两种索引的类型：B+树和Hash索引。\n参考自 http://www.cnblogs.com/hustcat/archive/2009/10/28/1591648.html\nB/B+ 树 我们都知道RB-Tree和AVL-Tree是都是自平衡的二叉查找树，在这样的树上进行插入，删除和查询操作最坏情况都能在 $O(log_{2}n)$ 的时间内完成。\nB树是1972年由 Rudolf Bayer 和 Edward M.McCreight 提出的，它是一种泛化的自平衡树，每个节点可以有多于两个的子节点。与自平衡二叉查找树不同的是，B树为系统大块数据的读写操作做了优化。B树减少定位记录时所经历的中间过程，从而加快存取速度。所以B树常被应用于数据库和文件系统的实现。\n假设B树每个节点下有b个子节点(b个分块)，那么查找可以在约为logbN的磁盘读取开销下完成。\n关于 B/B+ 树的其他特征在维基和其他资料上有详细讲解，在此不再赘述。\n由于时间限定的缘故，实现 B/B+ 树的索引显得不太现实，所以我们将考虑一种退化的方式 —— 桶排序。\n参考自\n https://zh.wikipedia.org/wiki/B%E6%A0%91 https://zh.wikipedia.org/wiki/B%2B%E6%A0%91  桶排序 先来看一下数据的分布情况：\n 每一行字符串的长度为 [1,128] （数字在Long值范围内均匀分布）\n 显然以字符串长来标识桶是可行的，但是我们有160000000条字符串，128个桶每个桶还是有1250000条，这样的桶粒度还是太粗。\n由于我们手里没有其他分布情况，以及字符串大小在同长度下是字典序，所以以字符串长度+前2-3个字符作为桶的标识就可以了。\n在本题中，由于只进行5轮query，并且写开销非常大，故仅对桶内字符串的数目进行统计。\n基本思路  Worker 分别扫描并建立所有桶的计数 Master 汇总所有桶，并建立全局桶 Master 通过二分查找，找到(k, n)所在的几个桶 Master 向 Worker 请求获得这些桶内的所有字符串 Master 对这些字符串进行排序，并输出最终结果  实现过程 本题实现的最大难点在于 I/O 和 GC 调优，还有充分调动 CPU。\n GC 调优我们通过使用固定的 buffer 来进行操作，基本避免出现 GC。 通过多线程处理来保证 CPU 调动。 网络开销在上述算法下基本忽略了  然而，剩下的 I/O 调优让人非常头大，并伴随着可能存在的同步问题 (读和处理)\n我们来看一下硬件条件：\n CPU: 24 core, 2.2GHz 内存: 3G heap memory 数据磁盘：内存文件系统 临时、结果磁盘：SSD  Top 1 的最终结果 从基友那得知，top 1 的实现5轮总耗时(JVM启动、暂停，以及计算)共11s多，用来推算自己离最优还有多远。\n(我实在没想通第一名怎么跑到11s的，大概是我内存文件系统开的不对？)\n不稳定的I/O实现 每个文件单线程读，固定4线程处理，通过预先分配的 buffer 避免出现 GC。\n封装文件操作并行，5个文件并行总共占用 25 core。\n实现下来由于读和操作无法有效平衡，很难做到读完就操作，大量时间花费在等待读线程新产出一个buffer或是处理线程处理完一个buffer。\n在清理系统cache的情况下，大约8s处理完所有字符串，不清理大约为4s。\n这种实现无法调优，等待的时间完全看 OS 心情。\n稳定的I/O实现 其实之前提到过，单线程读并不是一个很好的思路，而且上述的处理架构存在着不稳定的等待问题。\n那么如何使得在保持多线程读的同时能够多线程处理，并且不浪费时间在等待上呢？\n答案是每一个线程在读完一份数据后，直接进行处理，然后读取下一份。\n但是如何安排每一个线程的那份数据，保证线程之间不会有重复读取，也不会有漏读的数据块？\n这里采用 Eric 在复赛里的实现: FileSegment, 通过将一个文件分割为一个一个 Segment，让每一个线程自己请求下一个 Segment，并通过 RandomAccessFile 进行读取。(由于不许用 FileChannel)\npublic class FileSegment { private File file; private long offset; private long size; ... } public abstract class BufferedFileSegmentReadProcessor implements Runnable { private final int bufferMargin = 1024; private FileSegmentLoader fileSegmentLoader; private int bufferSize; private byte[] readBuffer; // Here bufferSize must be greater than segments\u0026#39; size got from fileSegmentLoader,  // otherwise segment will not be fully read/processed!  public BufferedFileSegmentReadProcessor(FileSegmentLoader fileSegmentLoader, int bufferSize) { this.bufferSize = bufferSize; this.fileSegmentLoader = fileSegmentLoader; } private int readSegment(FileSegment segment, byte[] readBuffer) throws IOException { int limit = 0; try (RandomAccessFile randomAccessFile = new RandomAccessFile(segment.getFile(), \u0026#34;r\u0026#34;)) { randomAccessFile.seek(segment.getOffset()); limit = randomAccessFile.read(readBuffer, 0, readBuffer.length); } return limit; } protected abstract void processSegment(FileSegment segment, byte[] readBuffer, int limit); @Override public void run() { readBuffer = new byte[bufferSize + bufferMargin]; try { FileSegment segment; while ((segment = fileSegmentLoader.nextFileSegment()) != null) { int limit = readSegment(segment, readBuffer); processSegment(segment, readBuffer, limit); } } catch (IOException e) { e.printStackTrace(); } } } 这样每一个线程都在读和处理之间循环，先完成的线程会请求下一个 Segment，保证以最短的时间完成调用所有 Core 进行一遍数据处理。\n这个方法保证能够十分稳定的进行读取和处理，并且不需要等待 Reader 和 Processor 通信，在我的实验环境中基本存着cache 3-4s能扫描一遍，清理cache也在 7-8s。\n其实到这里本题已经差不多了，我记得 Eric 在这种处理下比赛的最终结果是 14s 多。\nTop 1 的优化  在建立桶的时候，同时将每个文件上第i条字符串对应的offset、桶序号存入两个数组，并将桶信息以及这两个数组一起存储到本地，这样构成了一个可以快速查询桶内所有字符串的索引 这些信息总计大约 1G 左右，通过建立这样的索引，每次仅讲索引信息读入，并进行有限次的读取，就可以获取到所有需要的字符串  对比之下，之前的方式是从内存文件系统读取5G的文件，现在是从SSD读取1G左右文件，只要并发读速度差距不在5倍以上，这样的优化是能节省不少时间！\n测试 仅测试了建 Index\n清理 cache $ sysctl -w vm.drop_caches=3 vm.drop_caches = 3 $ time java ${JAVA_OPTS} -cp /exp/topkn/topkn.jar com.alibaba.middleware.topkn.TopknWorker localhost 5527 . [2017-07-27 14:24:27.512] INFO com.alibaba.middleware.topkn.TopknWorker Connecting to master at localhost:5527 [2017-07-27 14:24:27.519] INFO com.alibaba.middleware.topkn.TopknWorker Building index ... 0.673: [GC (Allocation Failure) 0.673: [ParNew: 809135K-\u0026gt;99738K(943744K), 0.1759305 secs] 809135K-\u0026gt;542134K(3040896K), 0.1761763 secs] [Times: user=3.15 sys=0.69, real=0.17 secs] [2017-07-27 14:24:32.330] INFO com.alibaba.middleware.topkn.TopknWorker Index built. Heap par new generation total 943744K, used 903716K [0x0000000700000000, 0x0000000740000000, 0x0000000740000000) eden space 838912K, 95% used [0x0000000700000000, 0x00000007311225e0, 0x0000000733340000) from space 104832K, 95% used [0x00000007399a0000, 0x000000073fb06b38, 0x0000000740000000) to space 104832K, 0% used [0x0000000733340000, 0x0000000733340000, 0x00000007399a0000) concurrent mark-sweep generation total 2097152K, used 442395K [0x0000000740000000, 0x00000007c0000000, 0x00000007c0000000) Metaspace used 3980K, capacity 4638K, committed 4864K, reserved 1056768K class space used 434K, capacity 462K, committed 512K, reserved 1048576K real 0m5.238s user 1m11.980s sys 0m26.324s 不清理 cache $ time java ${JAVA_OPTS} -cp /exp/topkn/topkn.jar com.alibaba.middleware.topkn.TopknWorker localhost 5527 . [2017-07-27 14:25:39.986] INFO com.alibaba.middleware.topkn.TopknWorker Connecting to master at localhost:5527 [2017-07-27 14:25:39.992] INFO com.alibaba.middleware.topkn.TopknWorker Building index ... 0.590: [GC (Allocation Failure) 0.590: [ParNew: 838912K-\u0026gt;99751K(943744K), 0.1851592 secs] 838912K-\u0026gt;591302K(3040896K), 0.1855308 secs] [Times: user=3.03 sys=0.88, real=0.19 secs] [2017-07-27 14:25:44.449] INFO com.alibaba.middleware.topkn.TopknWorker Index built. Heap par new generation total 943744K, used 863664K [0x0000000700000000, 0x0000000740000000, 0x0000000740000000) eden space 838912K, 91% used [0x0000000700000000, 0x000000072ea02318, 0x0000000733340000) from space 104832K, 95% used [0x00000007399a0000, 0x000000073fb09e00, 0x0000000740000000) to space 104832K, 0% used [0x0000000733340000, 0x0000000733340000, 0x00000007399a0000) concurrent mark-sweep generation total 2097152K, used 491550K [0x0000000740000000, 0x00000007c0000000, 0x00000007c0000000) Metaspace used 3980K, capacity 4638K, committed 4864K, reserved 1056768K class space used 434K, capacity 462K, committed 512K, reserved 1048576K real 0m4.754s user 0m57.144s sys 0m30.568s UPDATE\n在我的8核小物理机上，跑一边内存文件系统里的数据不要1s\u0026hellip; 看来是机器的问题，云服务器还是不给力啊 🤣\n总结 今年的中间件比赛是在很有意思，这个题目对参赛选手有比较高的要求，解题中涉及到多线程并发读写、分布式排序、索引、Socket编程、JVM 调优等多个方面，我这种上来直接挑的果断挂了两个版本的代码，东看西看左问右问终于把最终形式给弄清楚了。\n说是我做的不如说是我在学如何做，在此过程中发现了很多有意义的技术点，正准备接下来继续学习。\n","date":"2017-07-26T16:59:26+08:00","permalink":"https://blog.crazyark.xyz/p/awrace-topkn/","title":"2017 Alibaba Middleware 24h Final (Just for Fun 😀)"}]